# 集群管理员-管理节点
管理员可以使用 CLI 管理实例中的节点。master 节点使用来自计算节点的对象信息来验证节点的运行状况。
## 监听节点
打印所有节点

	oc get nodes
仅列出有关单个节点的信息，请使用完整的节点名替换 `<node>`
	
	oc get node <node>
有些命令输出中的 `STATUS` 列表可以显示具有以下条件的节点:

- `Ready`

	节点通过从 master 节点执行的执行健康检查得到返回 `Status OK` 结果。
- `NotReady`

	节点没有通过从 master 节点执行的健康检查
- `SchedulingDisabled`

	pod 不能向这个节点调度
- `Unknown`

	如果 CLI 找不到任何节点条件，`STATUS` 状态将显示未知

获取有关特定节点的更多信息，包含当前情况:

	 oc describe node <node>

## 添加节点
添加节点需要运行安装包的 ansible 来操作。详细信息参考[高级安装](https://docs.openshift.org/3.6/install_config/adding_hosts_to_existing_cluster.html#adding-nodes-advanced)
## 删除节点
使用 CLI 删除节点时，将从 K8S 中删除节点对象，但节点本身上存在的 pod 将不会被删除。任何未被复制控制器收录的 pod 将无法控制，由复制控制器控制的 pod 将会重新安排到其他的节点，而本地的 ` local manifest` pod 则需要手动删除。
### 删除节点执行操作
- 从准备删除的节点中[疏散 pod](https://docs.openshift.org/3.6/admin_guide/manage_nodes.html#evacuating-pods-on-nodes)
- 删除节点对象

		oc delete node <node>
- 检查移除结果

		oc get nodes
- 如果想卸载节点上所有的内容，包含 pod 、容器等

		可以使用 ansible 的 uninstall.yml playbook

## 修改节点标签
- 添加或修改一个节点的标签

		oc label node <node> <key_1>=<value_1> ... <key_n>=<value_n>
- 查看更多细节

		oc label -h

## 监听节点上的 pod
- 在一个或者多个节点上列出所有选定的 pod

		oadm manage-node <node1> <node2> \
			--list-pods [--pod-selector=<pod_selector>] [-o json|yaml]
- 在选定的节点上列出所有选定的 pod

		oadm manage-node --selector=<node_selector> \
    		--list-pods [--pod-selector=<pod_selector>] [-o json|yaml]
    		
## 将节点标记成不可调度或修改成可调度
默认情况下，健康检查正常的节点都会被标记成可调度节点，这意味着新的 pod 可以调度到该节点上。手动将节点标记为不可调度可以阻止任何新的 pod 调度到该节点上来。节点上的 pod 则不会受到影响。标记方法

- 不可调度

		oadm manage-node <node1> <node2> --schedulable=false
- 可调度

		oadm manage-node <node1> <node2> --schedulable
或，也可以使用 `--selector=<node_selector>` 选项选定节点标记为可调度还是不可调度，而不是指定特定的节点，如(<node1> <node2>)										

## 迁移节点上的 pod
迁移 pod 允许从选定的节点或所有节点上迁移选择器选择的 pod 或所有 pod。执行迁移前，必须先要标记节点不可调度。

只有复制控制器支持的 pod 才可以迁移。复制控制器将在其他节点上创建新 pod ，并从指定的节点中删除现有 pod，类似升级。不受复制控制器影响的 pod 将不会受到影响。要列出将要迁移的 pod ，但实际并不执行迁移的命令时，请使用 `--dry-run`

	oadm manage-node <node1> <node2> \
    	--evacuate --dry-run [--pod-selector=<pod_selector>]
真正迁移操作

	oadm manage-node <node1> <node2> \
    	--evacuate [--pod-selector=<pod_selector>]
 可以使用 `--force` 强制删除裸 pod
 
	oadm manage-node <node1> <node2> \
    	--evacuate --force [--pod-selector=<pod_selector>]   	   	
另外，可以使用 `--selector=<node_selector>` 来选定迁移节点的 pod，而不是选择特定的节点名称。			
## 重启节点
要在不会导致平台上运行的应用程序中断的情况下重新启动节点，首先迁移 pod 是非常重要的。对于高可用的 pod 通过路由层服务，不需要做任何事情。对于需要存储的其他 pod ，通常是数据库，确保它们能够在一个 pod 暂时脱机的情况下保持工作至关重要。在为有状态的 pod 实施弹性的同时，它们每个应用程序都是不同的，将调度程序配置为使用节点[反亲和性](https://docs.openshift.org/3.6/admin_guide/scheduler.html#anti-affinity)`anti-affinity`来确保将 pod 正确分布在可用节点上非常重要。

另一个挑战是如何处理运行集群的的关键，基础设施节点(路由、registry)。使用同样的节点迁移过程是适用的，尽管理解某些边缘情况很重要。	
### 基础设施节点
基础设施节点是标记运行 ocp 环境的节点。目前，管理节点启动最简单的方法是确保至少有3个节点可用于运行基础设施。下面场景演示了当只有两个节点可用时，在 OCP 上运行的应用程序可能导致服务中断的错误。

- 节点 A 被标记为不可调度，并且所有 pod 都被迁移。
- 该节点运行的 registry 集群，现在都重新部署在节点 B 上了。这意味着节点B 运行了两个 registry pod。
- 节点 B 被标记成不可调度并执行迁移
- 节点 B 上的两个 pod 端点的服务在短时间内会丢失所有端点，直到它们在节点 A 上被重新部署。

使用三个基础设施节点的相同过程不会导致服务终端。然而由于 pod 的调度，最后一个迁移并重启的节点仍然运行0个 registry 。另外两个节点分别运行1-2个 registry。最好的解决办法是依靠 pod 的亲和性。这是现在可用于测试的 k8s 特性，不支持生产。

### 使用反亲和性
pod 的反亲和性于节点反亲和性略有不同。节点的反亲和性是如果没有其他合适的位置来部署 pod，则会被违反。 pod 反亲和性可以被设置要求或优先级。

以 regsitry pod 为例，启用此功能的第一步是在 pod 上设置 `scheduler.alpha.kubernetes.io/affinity`。由于此 pod 使用部署配置，因此添加注释的最合适的位置是 pod 模版元数据。

```
oc edit dc/docker-registry -o yaml

...
  template:
    metadata:
      annotations:
        scheduler.alpha.kubernetes.io/affinity: |
          {
            "podAntiAffinity": {
              "requiredDuringSchedulingIgnoredDuringExecution": [{
                "labelSelector": {
                  "matchExpressions": [{
                    "key": "docker-registry",
                    "operator": "In",
                    "values":["default"]
                  }]
                },
                "topologyKey": "kubernetes.io/hostname"
              }]
            }
          }
```
注意,`scheduler.alpha.kubernetes.io/affinity` 内部存储为字符串，即使内容是 json。以上显示了如何将字符串作为注释添加到 Yaml 部署配置中。

这个例子假设 registry 中心有一个 `docker-registry=default` 的标签。 Pod 反亲和性可以使用任何的 K8S 匹配表达式。

最后一部需要在 `/etc/origin/master/scheduler.json` 中启用 `MatchInterPodAffinity` 调度器。有了这个，如果只有两个基础设施节点可用，一个重新启动， registry 将不能在另一个节点上运行。 `oc get pod` 会报告 pod 没有准备好，直到一个合适的节点可用为止。一旦一个节点可用并所有 pod 返回就绪状态，下一个节点就可以重新启动。
### 运行路由节点管理
大多数情况下，运行 ocp 路由器的 pod 会暴露一个主机端口。调度器需要确保运行路由器节点没有使用与 `PodFitsPorts` 相同的端口 pod 调度到这个节点，并实现 pod 反亲和性。如果路由依赖的 [IP failover](https://docs.openshift.org/3.6/admin_guide/high_availability.html#configuring-ip-failover) 移实现高可用，就没有什么其他需要。对于依靠 AWK ELB 等外部服务实现负载均衡的高可用性路由器，这个是服务负责，需要对路由器 pod 重启进行反应。

在极少情况下，路由器 pod 可能没有配置主机端口。在这种情况下，遵循推荐的基础设施节点[重启过程](https://docs.openshift.org/3.6/admin_guide/manage_nodes.html#infrastructure-nodes)非常重要。 
## 配置节点资源
可以通过配置 `kubelet` 的参数到节点配置文件 `/etc/origin/node/node-config.yaml`,来配置节点资源。添加 `kubeletArguments` 部分并包含任何所需的选项

```
kubeletArguments:
  max-pods # 可以在这个节点上运行多大数量的 pod
    - "40"
  resolv-conf # dns 配置文件，用作容器 dns 解析配置基础
    - "/etc/resolv.conf"
  image-gc-high-threshold: #镜像垃圾回收触发机制，磁盘使用率超过的百分比，默认值 90%
    - "90"
  image-gc-low-threshold: #镜像的垃圾回收会在磁盘的 80% 的停止。 
    - "80"
```

查看 `kubelet` 参数

	kubelet -h
这也可以在安装包使用 `openshift_node_kubelet_args` 变量进行设置，如

	openshift_node_kubelet_args={'max-pods': ['40'], 'resolv-conf': ['/etc/resolv.conf'],  'image-gc-high-threshold': ['90'], 'image-gc-low-threshold': ['80']}
### 设置每个节点最大的 pod 数
在 ` /etc/origin/node/node-config.yaml` 文件中，两个参数控制可以调节节点最大的 pod 数量：`pods-per-core` 和 `max-pods`。当两个选项同时使用时，两个限制中低的参数决定一个节点上运行的 Pod 数量。超过这些值会导致:

- ocp 和 docker 都提高自己的 cpu 利用率
- pod 调度缓慢
- 潜在的内存不足
- ip 地址耗尽
- 资源过度使用，导致较差的应用性能。

在 k8s 中，pod 实际上使用两个容器。第二个容器用于实际容器启动前建立网络。因此，假如运行10个 pod 将会有20个容器。

- `pods-per-core` 

	基于节点上的处理器数量设置可运行的 pod 数量。如果，在处理器4的情况下,`pods-per-core` 设置为10.那么最大的 pod 数量为 40.而设置为零则是关闭限制。

```
kubeletArguments:
  pods-per-core:
    - "10"
```

- `max-pods`

	无论节点属性如何，都可以将节点可运行的 pod 数设置为固定值。

```
kubeletArguments:
  max-pods:
    - "250"
```

使用上面的例子，如果 `pods-per-core` 默认值为10，而 `max-pods` 默认值为 250 。则意味着除非节点cpu 核数为25或者更多，否则 `pods-per-core` 才是真正生效的策略。

### 重置 docker 存储
下载 docker 镜像、运行、删除时， docker 并不是总是释放映射磁盘的空间。因此，随着时间的推移，可能会在节点上耗尽空间，这可能会阻止 ocp 能够继续创建新的 pod 或导致 pod 创建需要话费很长时间。

例如:以下显示6分钟后仍处于 `ContainerCreating` 状态的 pod，事件日志显示 [FailedSync](https://docs.openshift.org/3.6/dev_guide/events.html#events-reference)

```
$ oc get pod
NAME                               READY     STATUS              RESTARTS   AGE
cakephp-mysql-persistent-1-build   0/1       ContainerCreating   0          6m
mysql-1-9767d                      0/1       ContainerCreating   0          2m
mysql-1-deploy                     0/1       ContainerCreating   0          6m
```

```
$ oc get events
LASTSEEN   FIRSTSEEN   COUNT     NAME                               KIND                    SUBOBJECT                     TYPE      REASON                         SOURCE                                                 MESSAGE
6m         6m          1         cakephp-mysql-persistent-1-build   Pod                                                   Normal    Scheduled                      default-scheduler                                      Successfully assigned cakephp-mysql-persistent-1-build to ip-172-31-71-195.us-east-2.compute.internal
2m         5m          4         cakephp-mysql-persistent-1-build   Pod                                                   Warning   FailedSync                     kubelet, ip-172-31-71-195.us-east-2.compute.internal   Error syncing pod
2m         4m          4         cakephp-mysql-persistent-1-build   Pod                                                   Normal    SandboxChanged                 kubelet, ip-172-31-71-195.us-east-2.compute.internal   Pod sandbox changed, it will be killed and re-created.
```

为了解决这个问题，一个办法就是重置 docker 以移除 docker 不需要的数据。

在重启 docker 存储节点前，要做以下工作：

1. 设置节点为不可调度节点

		oadm manage-node <node> --schedulable=false
- 运行以下命令关闭 docker 和 ocp 服务

		systemctl stop docker atomic-openshift-node
- 运行以下命令删除本地卷

		rm -rf /var/lib/origin/openshift.local.volumes
		
	该命令清除本地的镜像缓存。结果，镜像，包括 `ose-*` 镜像，将重新获取。这可能会导致镜像存储恢复过程中的 pod 启动时间变慢。
- 删除存储 docker 目录

		rm -rf /var/lib/docker
- 重置 docker 存储

		docker-storage-setup --reset
- 重建存储

		docker-storage-setup
- 重建 docker 目录

		mkdir /var/lib/docker
- 启动服务

		systemctl start docker atomic-openshift-node
- 恢复节点调度

		oadm manage-node <node> --schedulable=true

## 修改节点流量接口
默认情况下， DNS 路由所有节点的流量。在节点注册期间， master 节点从 DNS 配置接收节点的 IP 地址，因此通过 DNS 访问节点是大多数部署最灵活的解决方案。

如果部署正在使用云提供的程序，则该节点将从云提供程序获取 IP 信息。但是 `openshift-sdn`	尝试通过各种方法(包括 `nodeName` 为设置)进行 DNS 查找来确定 IP。

但是可能需要更改节点流量接口。如

- ocp 安装在云提供商中，哪里是内部主机名，并且没有配置所有主机可解析。
- 从 master 角度看节点的 IP 与节点自身的角度不同

设置 `openshift_set_node_ip` ansible 变量，强制通过非默认网络接口的节点流量。更改接口

- 将 `openshift_set_node_ip` 设置为 `true`
- 将 `openshift_ip` 设置为要配置的节点 IP 地址

尽管 `openshift_set_node_ip` 可以作为节点所描述的解决方法，但它通常不适用于生产环境。这是因为如果节点收到新的 IP 地址，节点将不在正常工作。 													


		
			