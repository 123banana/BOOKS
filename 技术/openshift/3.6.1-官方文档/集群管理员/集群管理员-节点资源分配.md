# 集群管理员-节点资源分配
为了提供可靠的调度和最小化节点资源的过度使用，每个节点可以保留其资源的一部分提供底层[节点组件](https://docs.openshift.org/3.6/architecture/infrastructure_components/kubernetes_infrastructure.html#node)使用，如 `kebelet`等或主机其余系统组件，如 `sshd` 等。一旦指定，调度程序就会获取到更多节点已经为节点分配资源的信息，如 cpu/mem。
## 为节点配置资源
为节点组件预留的资源基于两个设置

- `kube-reserved`

	为节点组件预留的资源，默认是 `none`
- `system-reserved`  

	为系统组件预留的资源，默认是 `none`

可以使用一组 `<resource_type>=<resource_quantity>` 如(cpu=200m,memory=30G) k/v 添加到节点配置文件 `kubeletArguments` 中。默认位置为 `/etc/origin/node/node-config.yaml`

```
kubeletArguments:
  kube-reserved:
    - "cpu=200m,memory=30G"
  system-reserved:
    - "cpu=200m,memory=30G"
```

目前支持 cpu 和内存资源类型。对于 cpu ，资源数量是以核心单位，如(200m)。对于内存，以字节为单位，如(200Ki, 100M, 50Gi)。[更多资源信息](https://docs.openshift.org/3.6/dev_guide/compute_resources.html#dev-guide-compute-resources)

如果未设置标示，则默认为0.如果没有设置任何标示，则将分配的资源设置节点容量，就像引入可分配资源之前。

## 计算分配的资源
根据以下公式计算分配的资源量

	[Allocatable] = [Node Capacity] - [kube-reserved] - [system-reserved] - [Hard-Eviction-Thresholds]
为了改善系统可靠性，从可分配的资源中扣除 `Hard-Eviction-Thresholds` 资源，在节点级别为 `end-user` pod 强制执行分配。`experimental-allocatable-ignore-eviction` 设置可用于保留传统行为，但未来的版本中将被弃用。如果 `[Allocatable]` 为负值，则设置为0。
## 查看节点可分配资源和容量
要查看节点当前的容量和可分配资源，可运行

```
oc get node/<node_name> -o yaml
...
status:
...
  allocatable: # 可分配
    cpu: "4"
    memory: 8010948Ki
    pods: "110"
  capacity: #容量
    cpu: "4"
    memory: 8010948Ki
    pods: "110"
...
```
## 节点的系统资源报告
从 1.3 版本开始每个节点会报告容器运行时间和 kubelet 使用的系统资源。为了更好地帮助配置 `--system-reserved` 和 `--kube-reserved`功能，可以使用节点摘要 API 获取节点相关的资源使用情况，可以通过 `<master>/api/v1/nodes/<node>/proxy/stats/summary`。例如要查看 `cluster.node22` 节点资源情况:

```
curl <certificate details> https://<master>/api/v1/nodes/cluster.node22/proxy/stats/summary
{
    "node": {
        "nodeName": "cluster.node22",
        "systemContainers": [
            {
                "cpu": {
                    "usageCoreNanoSeconds": 929684480915,
                    "usageNanoCores": 190998084
                },
                "memory": {
                    "rssBytes": 176726016,
                    "usageBytes": 1397895168,
                    "workingSetBytes": 1050509312
                },
                "name": "kubelet"
            },
            {
                "cpu": {
                    "usageCoreNanoSeconds": 128521955903,
                    "usageNanoCores": 5928600
                },
                "memory": {
                    "rssBytes": 35958784,
                    "usageBytes": 129671168,
                    "workingSetBytes": 102416384
                },
                "name": "runtime"
            }
        ]
    }
}
```
有关证书的细节参考 [REST API](https://docs.openshift.org/3.6/rest_api/index.html#rest-api-index)
## 节点强制
节点可以根据配置的可分配值限制 pod 可消耗的资源总量。此功能通过阻止 pod 从缺少资源节点获取资源而提高系统可靠性(如:运行时容器, `node agent`等)。强烈建议管理员根据所需的节点利用率目标预留资源，以提高系统可靠性。

该节点使用强制执行服务质量的新 `cgroup` 层次结构强制执行资源约束。所有的 pod 都是在与系统守护进程不同的专用 cgroup 层次结构中。

要配置此功能，提供以下 `kubelet` 参数

```
kubeletArguments:
  cgroups-per-qos:
    - "true" 
  cgroup-driver:
    - "systemd" 
  enforce-node-allocatable:
    - "pods" 
```

- 启用或禁用节点管理新 cgroup。任何此设置的更改都需要节点重置。该标志必须为 `true` 才能允许节点启动强制可分配节点功能，不建议用户修改。
- 管理 cgroup 层结构时，由节点使用的是 cgroup 驱动程序。该值必须匹配与容器运行时相关的驱动程序。有效值是 `systemd` 和 `cgroupfs`，默认值为 `systemd`。
- 以逗号分隔的范围列表是对节点应该在哪里实施资源约束。有效值为 `pods, system-reserved,kube-reserved`，默认为 pods。不建议修改

(可选)通过在 `enforce-node-allocatable` 标志中指定这些标记，可以使用节点执行 ` kube-reserved` 和` system-reserved`.如果指定需要提供相应的 `--kube-reserved-cgroup`或 `--system-reserved-cgroup`。在将来的发型版本中，节点和容器运行时将打包在一个与 `system.slice` 分开的公共 cgroup 中。直到那个时候，不建议更改 `enforce-node-allocatable` 标志默认值。

管理员应该将系统守护进程视为保留(Guaranteed)的 pod。系统守护进程可以在其绑定 cgroup 冲突并且这种行为为需要作为集群部署的一部分进行管理。执行系统保留的限制可能导致关键系统服务在节点上被 cpu 缺少或 OOM 杀死。建议只有在操作员详细的描述其节点以确保精确的评估和有能力恢复该组中的任何程序被 OOM 杀死的能力下来进行系统保留。

因此，强烈建议用户只在默认的情况下执行节点可分配的 pod，并为系统守护进程保留适当的预留，以保证节点整体可用性。

## 迁移(驱逐)限制
如果节点处于内存压力下，则会影响整个节点以及所有运行在上面的 pod。如果系统守护程序使用的内存超过其保留量，则可能发上 OOM 事件，这可能会影响整个节点以及所有的 pod。为了避免或降低系统 OOM 的可能性，节点提供了资源外处理。

通过 `--eviction-hard` 标志保留一些内存，当节点上的内存可用性降低到绝对值或百分比以下时，节点将尝试迁移 pod。如果系统守护进程不存在于一个节点上，则 pod 限于`capacity - eviction-hard`。由于这个原因，在达到内存不足的状态之前作为迁移缓冲区的资源不可用于 pod。

下面的例子来说明节点可分配内存的影响:

- 节点容量是 32Gi
- kube 保留是 2Gi
- 系统保留是 1Gi
- `eviction-hard` 是小于 100Mi

对于这个节点，有效节点可分配的值为 28.9 Gi。如果节点和系统组件用尽所有预留，可用于 pod 的内存为 28.9Gi,而超过此用量的 Kubelet 将被迁移。

如果我们用过顶级 cgroup 强制限制节点分配  28.9 Gi,那么 pod 将永远不会超过这个值。除非系统守护进程消耗超过 3.1 的内存，否则不会执行迁移。

如果系统守护进程没有用完所有的保留，在上面的例子中，节点迁移开始前，pod 将面临来自绑定 cgroup 的 memcg OOM 杀死。为了在这种情况下更好的执行 Qos，节点将硬迁移阈值应用于所有的 pod 的顶级 cgroup，以便成为 ` Node Allocatable + Eviction Hard Thresholds`。如果系统守护进程不使用所有的预留，节点将消耗最多到 28.9 Gi 的内存。如果迁移不及时，当 pod 消耗超过这个值时，就会被杀。

## 调度
调度程序现在使用 `node.Status.Allocatable` 值，而不是 `node.Status.Capacity` 来决定节点是否成为 pod 可调度的候选者。默认情况下，节点将报告自己的机器容量，由集群充分调度。