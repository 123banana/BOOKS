## 高级安装
服务的执行和配置可以用作安装 ocp 的高级方法，阅读这里需要 ansible 的知识。atomic 支持容器安装服务，高级安装需要在单独的 RH 系统上运行，这个主机不需要包含在 ocp 集群中。或者安装程序的容器化版本可作为系统功能容器，但这种方式只作为预览版功能。或使用快速安装。容器安装 ocp 需要[独立运行 registry](https://docs.openshift.com/container-platform/3.6/install_config/install/stand_alone_registry.html#install-config-installing-stand-alone-registry)。
### 开始前准备
安装 ocp 前，准备同快速安装准备。对于大规模使用，建议查看[扩展和性能指南](https://docs.openshift.com/container-platform/3.6/scaling_performance/install_practices.html#scaling-performance-install-best-practices)
### 镜像版本策略
镜像需要版本号策略才能维护更新，请参考[镜像版本标记策略](https://docs.openshift.com/container-platform/3.6/architecture/core_concepts/containers_and_images.html#architecture-images-tag-policy)
### 配置集群变量
在安全性较高的应用程序中，将环境变量分配给整个 ocp 集群，请在 ` /etc/ansible/hosts` 文件中的 ` [OSEv3:vars]` 设置

```
[OSEv3:vars]

openshift_master_identity_providers=[{'name': 'htpasswd_auth',
'login': 'true', 'challenge': 'true',
'kind': 'HTPasswdPasswordIdentityProvider',
'filename': '/etc/origin/master/htpasswd'}]

openshift_master_default_subdomain=apps.test.example.com
```

下表描述了可分配变量

- ansible_ssh_user

	ansible ssh 方案用户
- ansible_become

	如果上面参数不是root，需要设置这里为 true，而且必须设置 sudo 密码
- debug_level

	`systemd-journald.service` 日志等里,查看[日志等级](https://docs.openshift.com/container-platform/3.6/install_config/master_node_configuration.html#master-node-config-logging-levels)
	
	- 0 错误和警告
	- 2 普通日志(默认)
	- 4 debug
	- 6 api-level debug，请求和响应
	- 8 body-level api debug
- containerized

	如果设定为 `true` ocp 和所有目标机安装将使用容器进行。如果设定为 `false` 或者不设定，将使用 RPM 进行安装。atomic 将检测启动文件 `/run/ostree-booted`
- openshift_master_audit_config

	这个变量将实现 api 审计。相关阅读[审计配置](https://docs.openshift.com/container-platform/3.6/install_config/master_node_configuration.html#master-node-config-audit-config)
- openshift_master_cluster_hostname

	这个变量覆盖集群的主机名，默认主机名为 master
- openshift_master_cluster_public_hostname

	这个变量覆盖集群的公共主机名，默认主机名为 master
- openshift_master_cluster_method

	可选，此变量定义部署多个 master 节点时的ha方案，支持原生方案，参考[多 master](https://docs.openshift.com/container-platform/3.6/install_config/install/advanced_install.html#multiple-masters)
- openshift_rolling_restart_mode

	设定滚动重启策略，即集群在 ha 模式滚动重启。默认设置允许主机上滚动重启服务。这个操作一般在全系统重启，也适合单 master 集群，参考[滚动更新playbook](https://docs.openshift.com/container-platform/3.6/install_config/upgrading/automated_upgrades.html#running-the-upgrade-playbook-directly)
- os_sdn_network_plugin_name

	这个变量配置 pod 网络的 openshift sdn 插件，默认sdn插件为  `redhat/openshift-ovs-subnet`，变量设置为 `redhat/openshift-ovs-multitenant` 可以定义多租户模式。
- openshift_master_identity_providers

	变量设定覆盖[身份提供者](https://docs.openshift.com/container-platform/3.6/install_config/configuring_authentication.html#install-config-configuring-authentication)，默认 [Deny All](https://docs.openshift.com/container-platform/3.6/install_config/configuring_authentication.html#DenyAllPasswordIdentityProvider)
- openshift_master_named_certificates 和 openshift_master_overwrite_named_certificates

	配置安装一部分部署的[自定义证书](https://docs.openshift.com/container-platform/3.6/install_config/install/advanced_install.html#advanced-install-custom-certificates)
- openshift_hosted_registry_cert_expire_days

	自动生成 registry 证书有效期，默认 730 天(2年)
- openshift_ca_cert_expire_days

	自动生成 ca 证书，默认 1825 天(5年)
- openshift_node_cert_expire_days

	自动生成 node 证书有效期，默认 730 天(2年)
- openshift_master_cert_expire_days

	自动生成 master 证书有效期，默认 730 天(2年)
- etcd_ca_default_days

	自动生成 etcd 证书，默认 1825 天(5年)，对所有 etcd 节点有效
- os_firewall_use_firewalld

	设置为 true，使用 firewalld 而非 iptables。 atomic 不可用。详细[防火墙部分](https://docs.openshift.com/container-platform/3.6/install_config/install/advanced_install.html#advanced-install-configuring-firewalls)
- oauth 配置

	覆盖 oauth 配置，相关参考[配置会话选项](https://docs.openshift.com/container-platform/3.6/install_config/install/advanced_install.html#advanced-install-session-options)
		
	- openshift_master_session_name
	- openshift_master_session_max_seconds
	- openshift_master_session_auth_secrets
	- openshift_master_session_encryption_secrets
- openshift_portal_net

	配置 [ocp sdn](https://docs.openshift.com/container-platform/3.6/architecture/networking/sdn.html#architecture-additional-concepts-sdn) 中创建的服务子网。这块网络是私有的，并不会于基础架构中的任何网络冲突。默认是 `172.30.0.0/16`，部署后无法重新配置。如果从默认更改，避免 `172.17.0.0/16`，因为这个是 docker0网络默认使用，或者修改 docker0网络。
- openshift_master_default_subdomain

	覆盖用于暴露[路由](https://docs.openshift.com/container-platform/3.6/architecture/networking/routes.html#architecture-core-concepts-routes)，默认 	subdomain 
- openshift_node_proxy_mode

	指定[服务代理模式](https://docs.openshift.com/container-platform/3.6/architecture/core_concepts/pods_and_services.html#service-proxy-mode)，默认是 iptables，或者使用用户空间代理的用户proxy
- osm_default_node_selector

	设置节点选择器，pod 默认发送的地方。
- osm_cluster_network_cidr

	设置 sdn 集群网络 cidr，分配给 pod ip的网络，该网络是私有，其中 pod，node 和 master 节点可能需要访问。默认为 `10.128.0.0/14` 部署后无法任意重新配置，尽管可以在 [sdn master 配置](https://docs.openshift.com/container-platform/3.6/install_config/configuring_sdn.html#configuring-the-pod-network-on-masters)中进行修改。
- osm_host_subnet_length

	设置 sdn 为 pod 每台主机子网分配的大小。默认为9，这意味着每个主机分配一个大小为 23 的子网，例如默认 `10.128.0.0/14` 集群网络，这将分配 `10.128.0.0/23, 10.128.2.0/23, 10.128.4.0/23` 部署后无法更改。
- openshift_use_flannel

	设置 `flannel` 作为默认 sdn。如果启用，请使用 `openshift_use_openshift_sdn` 变量禁用 sdn。[flannel](https://docs.openshift.com/container-platform/3.6/install_config/configuring_sdn.html#using-flannel)
- openshift_docker_additional_registries

	ocp 指定附加的 registry 添加到 docker 配置。这些是要搜索的 registry 。
- openshift_docker_insecure_registries

	为 ocp 指定额外不安全的 registry 添加到 docker 设置中，对于和这些 registry 通讯将不会验证 ssl。另外还需要将这些 registry 添加到 `openshift_docker_additional_registries`
- openshift_docker_blocked_registries

	设定阻拦的 registry 列表到 docker 中，将全部阻拦其设定中的变量内容。
- openshift_hosted_metrics_public_url

	配置中的 `metricsPublicURL` 来设置与监控平台的集成。
- openshift_template_service_broker_namespaces

	设置一个模版服务代理的命名空间提供给 broker

### 设置部署类型
安装程序使用的各种 playbook 和 roles 使用的各种默认基于部署类型的配置(通常是在 Ansible `inventory` 文件中定义)，在 `inventory ` 文件 `[OSEv3:vars]`确保 `deployment_type` 参数	设置 `openshift-enterprise` 变量。

	[OSEv3:vars]
	deployment_type=openshift-enterprise
### 设置主机变量
在安装时将环境变量分配给主机，在主机输入 [master] 或 [nodes]部分后，在 `/etc/ansible/hosts` 文件中指出所需的变量。

	[masters]
	ec2-52-6-179-239.compute-1.amazonaws.com openshift_public_hostname=ose3-master.public.example.com
下表是可分配主机的变量

- openshift_hostname

	内部集群主机名，当系统功能默认ip 无法解析为主机名时使用。
- openshift_public_hostname

	公共主机名，用于云安装或使用 nat 的网络的主机。
- openshift_ip

	集群内部 ip 地址，未配置默认路由接口时使用
- openshift_public_ip

	系统共用的 ip 地址，用于云安装或使用 nat 的网络的主机。
- containerized

	如果设定为 `true` ocp 和所有目标机安装将使用容器进行。如果设定为 `false` 或者不设定，将使用 RPM 进行安装。atomic 将检测启动文件 `/run/ostree-booted`
- openshift_node_labels

	安装期间想节点增加的标签。有关信息参考[配置节点主机标签](https://docs.openshift.com/container-platform/3.6/install_config/install/advanced_install.html#configuring-node-host-labels)
- openshift_node_kubelet_args

	设置节点上的 `kubeletArguments`。例如[容器和镜像垃圾回收](https://docs.openshift.com/container-platform/3.6/admin_guide/garbage_collection.html#admin-guide-garbage-collection)中使用的参数，并指定每个节点的资源。 `kubeletArguments` 是直接传递到与 [kubelet 的命令行](http://kubernetes.io/v1.1/docs/admin/kubelet.html)参数匹配的设置键值对。 `kubeletArguments` 不被迁移或者验证，如果使用可能会变的无效。这些设置可能导致配置节点中的其他设置。如 `{'image-gc-high-threshold': ['90'],'image-gc-low-threshold': ['80']}`
- openshift_hosted_router_selector

	设置自动部署路由器 pod 的默认 node 选择器，请参考[配置节点主机标签](https://docs.openshift.com/container-platform/3.6/install_config/install/advanced_install.html#configuring-node-host-labels)
- openshift_registry_selector

	设置自动部署 registry pod 的默认 node 选择器，请参考[配置节点主机标签](https://docs.openshift.com/container-platform/3.6/install_config/install/advanced_install.html#configuring-node-host-labels)
- openshift_docker_options

	设置 `/etc/sysconfig/docker` 中 docker 的设置，例如日志。`--log-driver json-file --log-opt max-size=1M --log-opt max-file=3`
- openshift_schedulable

	设置是否可调度节点，意味着新的pod 将是否可调度到这个 node 中。参考[master 节点配置](https://docs.openshift.com/container-platform/3.6/install_config/install/advanced_install.html#marking-masters-as-unschedulable-nodes)
- openshift_template_service_broker_namespaces

	设置一个模版服务代理的命名空间提供给 broker

### 设置 master api 和控制台端口
配置 master api 和 web 控制台默认端口，请在 `/etc/ansible/hosts` 下设置变量。

- openshift_master_api_port

	访问 ocp api 端口，`openshift_master_api_port=3443`
- openshift_master_console_port

	访问 ocp web 控制台的端口，`openshift_master_console_port=8756`

### 设置集群预安装检查
预安装检查时诊断任务的一组，作为 `openshift_health_checker` ansible role 的一部分运行。它们是在安装 ocp 之前运行，确保所有所需的值已经就位和检查主机可能的阻断和干扰安装成功的问题。

- memory_availability

	确保主机内存大小，最小要求是用户定义的可通过 `inventory` 文件定义 `openshift_check_min_host_memory_gb`
- disk_availability

	仅检查 etcd,master,node 主机。确保磁盘空间。最小要求是用户定义的可通过 `inventory` 文件定义 `openshift_check_min_host_disk_gb cluster`
- docker_storage

	仅在 docker 节点主机上运行。检查 docker 使用总量是否符合用户定义。如果没有设定， docker 最大使用默认值为可用总大小的 90%。可以使用 `inventory` 设置 `max_thinpool_data_usage_percent=90` 
- docker_storage_driver

	检查 ocp 支持的 docker 存储驱动。如果使用 `devicemapper` ，则检查是否使用环路设备。
- docker_image_availability

	确保安装的 registry 在本地或配置的主机上可用。
- package_version

	基于 yum 系统运行，确定是否使用 ocp 多个版本，可以使用多个版本软件包显示启动不同版本的多个 yum repo，这可能导致安装问题。`inventory` 未定义 `openshift_release` 将会跳过此检查。
- package_availability

	ocp 非容器安装之前运行，确保当前所需 rpm 包可用。
- package_update

	检查 yum 更新或程序包安装是否成功，而不是在主机实际运行。
- openshift_disable_check

	关闭特定检查，`openshift_disable_check=memory_availability,disk_availability`

```
注意：
```
可以在基于执行的健康检查中找到类似的一组健康检查，用于运行现有集群上的诊断。检查证书到期的另一组检查可以在[重新部署证书](https://docs.openshift.com/container-platform/3.6/install_config/redeploying_certificates.html#install-config-redeploying-certificates)中找到。

	
### 配置系统容器

```
注意:
```
所有系统功能容器组件都是 ocp 3.6 中的 `技术预览版`功能。它不能在生产中使用，并且不支持 ocp 3.6。此阶段，它仅供用于非生产集群安装。系统功能容器提供了一种方法来容纳在 docker deamon运行之前所需要运行的服务。它是 docekr 格式的容器。

它使用:
 
- ostree 存储
- runc 运行时
- systemd 服务管理
- skopeo 搜索

因此，系统容器被存储并运行在传统的 docker 服务外.有关容器技术详情阅读 RHEL atomic 的管理容器文档。可以配置 ocp 的部分组件通过容器进行部署，而不是通过 rpm或标准容器方法。目前，docker 和 etcd 可以作为 ocp 的系统容器运行。

```
注意:							 
```
系统容器目前是操作系统功能特定的，因为它们需要特定版本的 atomic 和 systemd。例如 RH，Fedora,centos 创建不同的系统容器。确保您使用的系统功能容器与其他运行的主机操作系统功能相匹配。 ocp 仅支持 RHEL 和 atomic 作为主机操作系统，因此默认情况下，RHEL 创建容器的系统功能容器。	
### 在系统容器中运行docker
可以配置 ocp 安装，以便在节点主机上运行 docker 为系统功能容器。当系统容器方法，·container-engine· 容器镜像和系统服务被使用在主机而不是 docker 的包和服务。

运行 docker 作为系统容器，对于 RHEL 系统，执行以下步骤

- 安装 docker
- 配置存储

完整存储配置后，在 ` [OSEv3:vars] ` 部分的 `inventory` 集群变量中设置为 `true`

	openshift_docker_use_system_container=True
当使用系统容器时，忽略一下用于 docker 的 `inventory`变量
	
	- docker_version
	- docker_upgrade	
此外，`inventory` 不能使用以下变量:

	openshift_docker_options
当 pull 容器镜像而不是默认的 `registry.access.redhat.com/openshift3/`，还可以在系统容器中强制使用特定的 registry。在 ` [OSEv3:vars] `设置

	openshift_docker_systemcontainer_image_registry_override="registry.example.com/myrepo/"
### 作为系统容器运行的 ETCD
可以将 ocp 安装配置以系统容器运行 etcd。而 systemd 标准容器化方法使用名为 `etcd_container` 与 rpm 的方法相同。使用此方法 etcd 数据目录在  `/var/lib/etcd/etcd.etcd/etcd.etcd/member`	。etcd 容器运行设置的变量在 `[OSEv3:vars]` 的变量

	openshift_use_etcd_system_container=True
### 配置一个 registry 位置
如果使用的是除了 `registry.access.redhat.com` 上默认的镜像的 registry，需要在 ` /etc/ansible/hosts` 指定需要的 registry。

	
	openshift_examples_modify_imagestreams=true
- oreg_url

	如果镜像不在 `registry.access.redhat.com`，设定备用镜像位置 `oreg_url=example.com/openshift3/ose-${component}:${version}`
- openshift_examples_modify_imagestreams

	如果指定默认值以外的 registry，请设置为 true。将镜像流位置改为 `oreg_url`的值。
### 配置注册存储
使用高级安装，可以启用 registry 存储的几个选项:

- 选项A：NFS 主机组

	设置以下变量时，会在高级安装时，在高级安装期间创建 NFS 卷，在 `[nfs]` 主机组中使用的主机路径 ` <nfs_directory>/<volume_name>`。例如这些卷将是 `/exports/registry` 
	
```
[OSEv3:vars]

openshift_hosted_registry_storage_kind=nfs
openshift_hosted_registry_storage_access_modes=['ReadWriteMany']
openshift_hosted_registry_storage_nfs_directory=/exports
openshift_hosted_registry_storage_nfs_options='*(rw,root_squash)'
openshift_hosted_registry_storage_volume_name=registry
openshift_hosted_registry_storage_volume_size=10Gi
```		

- 选项B：外部 NFS 主机
要使用外部 NFS 卷，在存储主机上必须存在 ` <nfs_directory>/<volume_name>` 的路径。使用一下选项远程卷路径将是 `nfs.example.com:/exports/registry`

```
[OSEv3:vars]

openshift_hosted_registry_storage_kind=nfs
openshift_hosted_registry_storage_access_modes=['ReadWriteMany']
openshift_hosted_registry_storage_host=nfs.example.com
openshift_hosted_registry_storage_nfs_directory=/exports
openshift_hosted_registry_storage_volume_name=registry
openshift_hosted_registry_storage_volume_size=10Gi
```

- 选项C: openstack
一个 openstack 存储设置必须存在

```
openshift_hosted_registry_storage_kind=openstack
openshift_hosted_registry_storage_access_modes=['ReadWriteOnce']
openshift_hosted_registry_storage_openstack_filesystem=ext4
openshift_hosted_registry_storage_openstack_volumeID=3a650b4f-c8c5-4e0a-8ca5-eaee11f16c57
openshift_hosted_registry_storage_volume_size=10Gi
```

- 选项D：aws or another s3 存储
S3 块必须存在

```
#openshift_hosted_registry_storage_kind=object
#openshift_hosted_registry_storage_provider=s3
#openshift_hosted_registry_storage_s3_accesskey=access_key_id
#openshift_hosted_registry_storage_s3_secretkey=secret_access_key
#openshift_hosted_registry_storage_s3_bucket=bucket_name
#openshift_hosted_registry_storage_s3_region=bucket_region
#openshift_hosted_registry_storage_s3_chunksize=26214400
#openshift_hosted_registry_storage_s3_rootdirectory=/registry
#openshift_hosted_registry_pullthrough=true
#openshift_hosted_registry_acceptschema2=true
#openshift_hosted_registry_enforcequota=true
```

如果使用不同的s3服务，例如 minio 或者 exoscale，还可以添加区域参数

	openshift_hosted_registry_storage_s3_regionendpoint=https://myendpoint.example.com/
	
### 配置 GlusterFS持久存储
GlusterFS 配置可以为 ocp 提供一致的存储和动态配置。它可以在 ocp 中集成，也可以使用非自身节点上使用非容器化部署。

- 配置集中存储

	此选项使用 [CNS](https://access.redhat.com/documentation/en-us/red_hat_gluster_storage/3.2/html/container-native_storage_for_openshift_container_platform/) 在 ocp 中配置容器化 GlusterFS 永久存储。

请阅读[容器化 GlusterFS 注意事项](https://docs.openshift.com/container-platform/3.6/install_config/install/prerequisites.html#prereq-containerized-glusterfs-considerations)特定主机准备和先决条件

1. 在 `inventory` 文件中增加 `glusterfs` 到 `[OSEv3:children]` 打开 `[glusterfs]` 组


		[OSEv3:children]
		masters
		nodes
		glusterfs
- 可选，可能更改 `[OSEv3:vars]` 设置


		[OSEv3:vars]
		openshift_storage_glusterfs_namespace=glusterfs # 项目(命名空间)主机存储的 pod 存储。默认 glusterfs
		openshift_storage_glusterfs_name=storage #用于标示GlusterFS 集群名称，用于资源名称。默认 storage
- 添加一个 ` [glusterfs]` ，其中包含将承载GFS存储的每个存储节点并包含以下格式 ` glusterfs_ip` 和 `glusterfs_devices` 参数

		<hostname_or_ip> glusterfs_ip=<ip_address> glusterfs_devices='[ "</path/to/device1/>", "</path/to/device2>", ... ]'

	例


		[glusterfs]
		192.168.10.11 glusterfs_ip=192.168.10.11 glusterfs_devices='[ "/dev/xvdc", "/dev/xvdd" ]'
		192.168.10.12 glusterfs_ip=192.168.10.12 glusterfs_devices='[ "/dev/xvdc", "/dev/xvdd" ]'
		192.168.10.13 glusterfs_ip=192.168.10.13 glusterfs_devices='[ "/dev/xvdc", "/dev/xvdd" ]'
	将 `glusterfs_devices` 设置为原始块设备列表，将作为 GlusterFS 集群的一部分完全管理。必须有一个设备列出。每个设备必须时裸的，没有分区或者 LVM PV。将 `glusterfs_ip` 设置为 pod 将与 GlusterFS 节点通讯的 ip 地址。
- 添加 `[glusterfs]` 下列出的主机添加到 `[nodes]` 组中

		[nodes]
		192.168.10.11
		192.168.10.12
		192.168.10.13	
- 运行高级安装完成集群后，从主机运行一下命令，验证创建必要的对象已经创建
	- 验证创建 `StorageClass`

			# oc get storageclass
			NAME                  TYPE
			glusterfs-storage     kubernetes.io/glusterfs
	- 验证路由创建

			# oc get routes
			NAME            HOST/PORT                                     PATH           SERVICES   PORT   TERMINATION   WILDCARD
			heketi-glusterfs-route  heketi-glusterfs-default.cloudapps.example.com  heketi-glusterfs <all>             None
	路由名称时 `heketi-glusterfs-route`，除非在 `inventory` 文件中设置 `openshift_glusterfs_storage_name` 值覆盖。
	- 使用 `curl` 验证

			# curl http://heketi-glusterfs-default.cloudapps.example.com/hello
			Hello from Heketi.
安装成功后，请阅读 ocp 环境中的[红帽GlusterFS Storage Pod 上的操作](https://access.redhat.com/documentation/en-us/red_hat_gluster_storage/3.2/html/container-native_storage_for_openshift_container_platform/chap-documentation-red_hat_gluster_storage_container_native_with_openshift_platform-gluster_pod_operations),已检查 GlusterFS 集群状态。GlusterFS 卷的[动态配置](https://docs.openshift.com/container-platform/3.6/install_config/persistent_storage/dynamically_provisioning_pvs.html#install-config-persistent-storage-dynamically-provisioning-pvs)可以通过[创建一个 PVC 来请求存储](https://docs.openshift.com/container-platform/3.6/install_config/storage_examples/gluster_dynamic_example.html#create-a-pvc-ro-request-storage-for-your-application)。

### 配置容器 registry
配置容器 registry 时，可以使用的选项。如果没有使用 registry 存储选项，默认安装的 registry 是静态的，如果 pod 不再存在，所有的数据将丢失。ocp 还支持单点 nfs 备份的 registry,但与 GlusterFS 支持的选项相比，缺少冗余和可靠性。
### 配置一个集成 GlusterFS-BACKED registry
类似[配置容器的持久存储为 GlusterFS](https://docs.openshift.com/container-platform/3.6/install_config/install/advanced_install.html#advanced-install-containerized-glusterfs-persistent-storage)，可以配置 GlusterFS 存储和在初始安装集群期间，为 ocp registry 部署，为 registry 提供可靠存储。

	注意
	请阅读容器话GlusterFS特定主机准备和玄觉条件注意事项
设置 ocp 的存储配置与 GlusterFS 存储的配置非常相似，因为它可以容器化或者非容器化。对此容器化方法，适用以下异常和添加。

1. 编辑 `inventory`，在 ` [OSEv3:children] ` 添加 ` [glusterfs_registry] `组

		[OSEv3:children]
		masters
		nodes
		glusterfs_registry
- 在 ` [OSEv3:vars] ` 部分中添加以下角色变量以启动 GlusterFS 的 registry 支持，只要 `glusterfs_registry` 组名和 `[glusterfs_registry]` 组存在

		[OSEv3:vars]
		openshift_hosted_registry_storage_kind=glusterfs
- 建议最少三个 registry pod，因此在 `[OSEv3:vars]`中设定如下

		openshift_hosted_registry_replicas=3
- 如果要指定 GlusterFS 支持的 registry 卷大小，在 ` [OSEv3:vars]` 下设定 role 变量，如果不设置，默认5Gi

		openshift_hosted_registry_storage_volume_size=10Gi
- 安装程序将包含 `region=infra`	标签的节点上部署 ocp registry pod 和相关路由。在 ` [nodes] ` 部分，至少有一个节点条目上添加了 `infra` 标签，否则 registry 将部署失败。

		[nodes]
		192.168.10.14 openshift_schedulable=True openshift_node_labels="{'region': 'infra'}"
- 添加一个 `[glusterfs_registry]` 部分，其中包含承载 `GlusterFS` 支持的 registry 的每个存储条目

		<hostname_or_ip> glusterfs_ip=<ip_address> glusterfs_devices='[ "</path/to/device1/>", "</path/to/device2>", ... ]'
例如

		[glusterfs_registry]
		192.168.10.14 glusterfs_ip=192.168.10.14 glusterfs_devices='[ "/dev/xvdc", "/dev/xvdd" ]'
		192.168.10.15 glusterfs_ip=192.168.10.15 glusterfs_devices='[ "/dev/xvdc", "/dev/xvdd" ]'
		192.168.10.16 glusterfs_ip=192.168.10.16 glusterfs_devices='[ "/dev/xvdc", "/dev/xvdd" ]'	
将 `glusterfs_devices` 设置为原始块设备列表，将作为 GlusterFS 集群的一部分完全管理。必须有一个设备列出。每个设备必须时裸的，没有分区或者 LVM PV。将 `glusterfs_ip` 设置为 pod 将与 GlusterFS 节点通讯的 ip 地址。
- 添加 `[glusterfs]` 下列出的主机添加到 `[nodes]` 组中

		[nodes]
		192.168.10.14
		192.168.10.15
		192.168.10.16	
- 运行高级安装完成集群后，从主机运行一下命令，验证创建必要的对象已经创建
	- 验证创建 `StorageClass`

			# oc get storageclass
			NAME                  TYPE
			glusterfs-storage     kubernetes.io/glusterfs
	- 验证路由创建

			# oc get routes
			NAME            HOST/PORT                                     PATH           SERVICES   PORT   TERMINATION   WILDCARD
			heketi-glusterfs-route  heketi-glusterfs-default.cloudapps.example.com  heketi-glusterfs <all>             None
	路由名称时 `heketi-glusterfs-route`，除非在 `inventory` 文件中设置 `openshift_glusterfs_storage_name` 值覆盖。
	- 使用 `curl` 验证

			# curl http://heketi-glusterfs-default.cloudapps.example.com/hello
			Hello from Heketi.
安装成功后，请阅读 ocp 环境中的[红帽GlusterFS Storage Pod 上的操作](https://access.redhat.com/documentation/en-us/red_hat_gluster_storage/3.2/html/container-native_storage_for_openshift_container_platform/chap-documentation-red_hat_gluster_storage_container_native_with_openshift_platform-gluster_pod_operations),已检查 GlusterFS 集群状态。GlusterFS 卷的[动态配置](https://docs.openshift.com/container-platform/3.6/install_config/persistent_storage/dynamically_provisioning_pvs.html#install-config-persistent-storage-dynamically-provisioning-pvs)可以通过[创建一个 PVC 来请求存储](https://docs.openshift.com/container-platform/3.6/install_config/storage_examples/gluster_dynamic_example.html#create-a-pvc-ro-request-storage-for-your-application)。

### 配置全局 proxy
如果主机需要使用 http/https 代理才能连接到外部主机，则必须配置许多组件才能使用代理，包括 master，docker,builds。 node 服务仅连接到不需外部访问的 master api,因此不需要配置为使用代理。为了简化配置，可在集群或主机级别指定以下的复制变量，以便在整个环境中国年均匀的设置这些应用。

有关如何构建定义代理环境的更多信息，请参考[配置全局构建](https://docs.openshift.com/container-platform/3.6/install_config/build_defaults_overrides.html#install-config-build-defaults-overrides)

- openshift_http_proxy

	指定主机和 docker deamon 的程序 `HTTP_PROXY` 环境变量
- openshift_https_proxy

	指定主机和 docker deamon 的程序 `HTTPS_PROXY` 环境变量
- openshift_no_proxy

	设定 master 和 docker deamon 设置 `NO_PROXY` 环境变量，该值应设置为不应使用定义的代理主机名列表(用逗号隔开)或通配符主机。默认情况下，此列表扩展为所有定义的 ocp 主机列表
- openshift_generate_no_proxy_hosts

	这个布尔变量指定所有定义的主机和 `*.cluster` 名称。本地应该自动附加到 `NO_PROXY` 列表中，默认 `true`；
- openshift_builddefaults_http_proxy

	定义使用 `BuildDefaults` 接纳控制器插入到构建中的 `HTTP_PROXY` 环境变量。如果设置 `openshift_http_proxy` ，则该变量将继承该值，如果希望使用不同值，只需设置它。
- openshift_builddefaults_https_proxy

	定义使用 `BuildDefaults` 接纳控制器插入到构建中的 `HTTPS_PROXY` 环境变量。如果设置 `openshift_https_proxy` ，则该变量将继承该值，如果希望使用不同值，只需设置它。
- openshift_builddefaults_no_proxy

	定义使用 `NO_PROXY` 接纳控制器插入到构建中的 `HTTPS_PROXY` 环境变量。如果设置 `openshift_no_proxy ` ，则该变量将继承该值，如果希望使用不同值，只需设置它。
- openshift_builddefaults_git_http_proxy

	此变量定义了构建期间由 ` git clone` 操作使用的 http 代理，使用 `BuildDefaults ` 接纳控制器进行定义。如果设置了 `openshift_builddefaults_http_proxy `，该值将被继承，如果希望 `git clone` 操作使用不同的值，则只需要修改此选项即可。
- openshift_builddefaults_git_https_proxy

	此变量定义了构建期间由 ` git clone` 操作使用的 https 代理，使用 `BuildDefaults ` 接纳控制器进行定义。如果设置了 `openshift_builddefaults_https_proxy `，该值将被继承，如果希望 `git clone` 操作使用不同的值，则只需要修改此选项即可。

### 配置防火墙
	如果修改了防火墙设置，请确保每个主机都使用了相同的防火墙类型来防止不一致。
虽然 iptables 是默认防火墙，但是建议使用 firewalld 进行新的安装。 ocp 使用 iptables 作为默认防火墙，但可以在安装过程中将自己的集群配置为使用 firewalld。

因为 iptables 是默认防火墙，所以 ocp 设计为自动配置。但是，如果配置不正确， iptables 规则可能会破坏 ocp。防火墙的优点就包括允许多个对象安全的共享防火墙规则。

要使用 firewalld 作为 ocp 安装的防火墙，请将 `os_firewall_use_firewalld` 变量添加到可安装主机文件的配置变量列表中。

	[OSEv3:vars]
	os_firewall_use_firewalld=True 		
### 配置在 master 上的调度器
安装过程中，指定 master 设备的任何主机也应该配置为 node，以便将 master 设备配置为 ocp sdn 的一部分。必须保证这些主机添加到 `[node]` 条目中。

	[nodes]
	master.example.com				
为了确保 master 不运行业务 pod，默认将自动标注不可调度，意味着 master 将不接收新的 pod。这与设置 `openshift_schedulable=false` 相同。当然测试环境中，也可以设置为 `true`

	[nodes]
	master.example.com openshift_schedulable=true
如果要更改主机安装后的可调度性，请阅读[node 标记可调度和不可调度](https://docs.openshift.com/container-platform/3.6/admin_guide/manage_nodes.html#marking-nodes-as-unschedulable-or-schedulable)
### 配置 node 主机标签
可以通过 `/etc/ansible/hosts` 在安装 node 节点时配置标签。标签可用于使用调度程序确定 pod 在节点上的位置。除了 ` region=infra` (专门用于基础设施架构的节点)，实际的标签名称和值是任意的，可以分配，但可以根据集群的要求进行分配。

设置标签，使用 `openshift_node_labels` 变量，并将所需标签添加到 `[node]`中。例如

	[nodes]
	node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"	
### 设置专用基础设施
`openshift_router_selector ` 和 `openshift_registry_selector` 可以设定放置 registry 和路由器 pod 使用的标签选择器。默认设置为 `region=infra`

	# default selectors for router and registry services
	# openshift_router_selector='region=infra'
	# openshift_registry_selector='region=infra'
如果 `[node]` 节点与选择器相匹配，则默认路由和 registry 将会自动部署

	[nodes]
	infra-node1.example.com openshift_node_labels="{'region': 'infra','zone': 'default'}"	
路由器和 registry 只能在具有 ` region=infra` 标签的主机上运行。确保 ocp 环境中至少有一个节点主机具有 ` region=infra` 标签。

生产环境中，建议使用分开的节点来部署基础设施服务，其中 registry 和路由器端口可以与用户应用端口区分开。如果不使用 ocp 的 registry 和路由器，可以配置

	openshift_hosted_manage_registry=false
	openshift_hosted_manage_router=false	
如果默认的 `registry.access.redhat.com` 以外的 registry，则需要在 ` /etc/ansible/hosts` 指定。如配置 master 可调度所述，默认情况下， master 标记为不可调度，如果使用 ` region=infra` 标记 master 主机，并没有其他专用基础设备节点，还需要将这些 master 主机显示为可调度。除此之外，registry 和路由器端口不能使用防止冲突。

	[nodes]
	master.example.com openshift_node_labels="{'region': 'infra','zone': 'default'}" openshift_schedulable=true
### 设置会话选项
oauth 配置中会话选项在 `inventory` 文件中配置。默认情况下 ansible 会使用生成的身份验证和加密信息来填充一个 `sessionSecretsFile`，以便连接 master 服务。默认位置为 `/etc/origin/master/session-secrets.yaml` ，只有在所有主设备上删除时，才会创建新的文件。可以使用 `openshift_master_session_name` 和 `openshift_master_session_max_seconds` 来设置会话名称和最大秒数。

	openshift_master_session_name=ssn
	openshift_master_session_max_seconds=3600
如果提供 `openshift_master_session_auth_secrets`和`openshift_master_encryption_secrets` 长度必须相同。对于 `openshift_master_session_auth_secrets` 用于使用 HMAC 认证会话，建议使用 32或64字节加密。

	openshift_master_session_auth_secrets=['DONT+USE+THIS+SECRET+b4NV+pmZNSO']
### 配置自定义证书
可以在高级安装期间部署 ocp api 和 web 控制台的公共主机名的自定义服务证书，可在 `inventory` 中配置。

使用 `openshift_master_cluster_public_hostname` 设置定义的 `publicMasterURL` 关联的主机名，只能配置自定义证书。如果使用自定义服务证书异常，将导致基础组件 TLS 错误导致而连接内部的 `masterURL` 联系 master api。 `masterURL` 关联的主机名为(`openshift_master_cluster_hostname`)。

证书和密钥文件路径可以使用 `openshift_master_named_certificates` 集群变量进行配置。

	openshift_master_named_certificates=[{"certfile": "/path/to/custom1.crt", "keyfile": "/path/to/custom1.key"}]
文件路径必须位于可运行的 ansible 的系统本地。证书被复制到 master 主机并部署在 `/etc/origin/master/named_certificates/` 目录中。 ansible 可以检测证书的通用名称(` Common Name `)和主题备用名称(`Subject Alternative Names`)，设置 `openshift_master_named_certificates` 时，通过提供"名称"来覆盖检测到的名称

	openshift_master_named_certificates=[{"certfile": "/path/to/custom1.crt", "keyfile": "/path/to/custom1.key", "names": ["public-master-host.com"]}]
使用的 `openshift_master_named_certificates` 配置的证书将缓存在 master 节点上，这意味着每个额外的 ansible 运行一套不同的证书结果在 master 主机和master 配置文件中都会保留所有部署的证书。如果要使用 `openshift_master_named_certificates` 提供的值或者无值，需要指定 `openshift_master_overwrite_named_certificates` 在集群变量中。

	openshift_master_overwrite_named_certificates=true
完整例子

	openshift_master_cluster_method=native
	openshift_master_cluster_hostname=lb.openshift.com
	openshift_master_cluster_public_hostname=custom.openshift.com
随后执行覆盖证书

	openshift_master_named_certificates=[{"certfile": "/root/STAR.openshift.com.crt", "keyfile": "/root/STAR.openshift.com.key", "names": ["custom.openshift.com"]}]
	openshift_master_overwrite_named_certificates=true
### 配置证书有效性
默认证书用管理 etcd、master、kubelet 的证书的有效期是2年到5年。有效期可以用于自动生成 registry、ca、node 和主证书。配置变量在

	[OSEv3:vars]

	openshift_hosted_registry_cert_expire_days=730
	openshift_ca_cert_expire_days=1825
	openshift_node_cert_expire_days=730
	openshift_master_cert_expire_days=730
	etcd_ca_default_days=1825		
当[重新部署证书](https://docs.openshift.com/container-platform/3.6/install_config/redeploying_certificates.html#install-config-redeploying-certificates)后，也可以使用这些证书。
### 配置集群监控(metics)
默认情况下，集群不会自动部署监控。设置以下内容可以在高级安装时启动监控。

	[OSEv3:vars]

	openshift_hosted_metrics_deploy=true #打开部署
	openshift_hosted_metrics_deployer_prefix=registry.example.com:8888/openshift3/ #替换部署镜像的前缀
	openshift_hosted_metrics_deployer_version=v3.6 #替换版本
ocp web控制台使用来自 Hawkular Metrics 服务的数据图形显示。可以使用 `openshift_hosted_metrics_public_url` 设置监控url，默认值 `https://hawkular-metrics.{{openshift_master_default_subdomain}}/hawkular/metrics` 如果改变，请确保主机名可以通过路由器访问。
### 配置度量存储
必须设置 `openshift_metrics_cassandra_storage_type` 使用存储。如果没有设置，监控将数据存储在 `EmptyDir` 卷中，当 `Cassandra` pod 终止后被删除。使用高级安装时，有3个选项使用存储

- 选项A：NFS 主机组

	在 `[nfs]` 主机组中的主机上使用路径 `<nfs_directory>/<volume_name>` 进行安装时创建 nfs 卷。
	例如
	
		[OSEv3:vars]

		openshift_hosted_metrics_storage_kind=nfs
		openshift_hosted_metrics_storage_access_modes=['ReadWriteOnce']
		openshift_hosted_metrics_storage_nfs_directory=/exports
		openshift_hosted_metrics_storage_nfs_options='*(rw,root_squash)'
		openshift_hosted_metrics_storage_volume_name=metrics
		openshift_hosted_metrics_storage_volume_size=10Gi 
- 选项B：外部 NFS

	使用外部 nfs 卷，必须有一个已经准备好的 ` <nfs_directory>/<volume_name>` 存储服务
	
		[OSEv3:vars]

		openshift_hosted_metrics_storage_kind=nfs
		openshift_hosted_metrics_storage_access_modes=['ReadWriteOnce']
		openshift_hosted_metrics_storage_host=nfs.example.com
		openshift_hosted_metrics_storage_nfs_directory=/exports
		openshift_hosted_metrics_storage_volume_name=metrics
		openshift_hosted_metrics_storage_volume_size=10Gi
	远程卷路径使用 `nfs.example.com:/exports/metrics`
- 选项c：动态卷

	ocp 支持云提供商的动态卷配置，可以使用以下
	
		[OSEv3:vars]

		openshift_metrics_cassandra_storage_type=dynamic

### 配置日志集群
默认情况下，日志集群不在自动部署中。使用高级安装方法时，请设置以下功能启动日志集群。

	[OSEv3:vars]

	openshift_hosted_logging_deploy=true #启动日志集群部署
	openshift_hosted_logging_deployer_prefix=registry.example.com:8888/openshift3/ #部署镜像前缀
	openshift_hosted_logging_deployer_version=v3.6 #部署镜像版本
### 设置日志存储
使用 `openshift_hosted_logging_storage_kind`打开日志存储，如果未设置，日志数据将记录到一个 `EmptyDir` 卷中，当 ES pod 终止后，它将被删除。使用高级安装时，由三个选项用于启动集群日志存储:

- 选项A：nfs 主机组

		[OSEv3:vars]

		openshift_hosted_logging_storage_kind=nfs
		openshift_hosted_logging_storage_access_modes=['ReadWriteOnce']
		openshift_hosted_logging_storage_nfs_directory=/exports
		openshift_hosted_logging_storage_nfs_options='*(rw,root_squash)'
		openshift_hosted_logging_storage_volume_name=logging
		openshift_hosted_logging_storage_volume_size=10Gi
- 选项B：外部nfs 

		[OSEv3:vars]

		openshift_hosted_logging_storage_kind=nfs
		openshift_hosted_logging_storage_access_modes=['ReadWriteOnce']
		openshift_hosted_logging_storage_host=nfs.example.com
		openshift_hosted_logging_storage_nfs_directory=/exports
		openshift_hosted_logging_storage_volume_name=logging
		openshift_hosted_logging_storage_volume_size=10Gi
	nfs 路径为 `nfs.example.com:/exports/logging`
- 选项C 动态卷

		[OSEv3:vars]

		openshift_hosted_logging_storage_kind=dynamic

### 启动服务目录
本功能是预览版功能。RH SLA 不支持预览版功能，不建议生产使用。这些功能可以访问并得到用户反馈。有关[技术预览版更多信息](https://access.redhat.com/support/offerings/techpreview/)。启用服务目录web控制台还配置为启动更新的页面来游览服务目录。

要启动服务目录，在 `inventory` 的 `[OSEv3:vars]` 添加内容

	openshift_enable_service_catalog=true
启用服务目录后， web控制台显示更新的页面，但仍然使用正常的镜像流和模版行为。ansible service broker 可信服务更多相关查询 [Configuring the Ansible Service Broker](https://docs.openshift.com/container-platform/3.6/install_config/install/advanced_install.html#configuring-template-service-broker).默认情况下不会部署服务模版代理(TSB)，相关信息查看[配置模版服务代理](https://docs.openshift.com/container-platform/3.6/install_config/install/advanced_install.html#configuring-template-service-broker)

### 配置 Ansible Service Broker
本功能是预览版功能。RH SLA 不支持预览版功能，不建议生产使用。这些功能可以访问并得到用户反馈。有关[技术预览版更多信息](https://access.redhat.com/support/offerings/techpreview/)。启用服务目录web控制台还配置为启动更新的页面来游览服务目录。

如果已经启动了服务目录，则还可以启动 ansible service broker(ASB)。ASB 将需要部署自己 etcd 实例，要和集群使用的 etcd 分开。 ASB 的 etcd 实例需要使用持久卷(PV)的单独存储才能运行。如果没有可用 PV，etcd 会等到 PV 条件满足后才可以用。 ASB 应用程序将进入 CrashLoop 状态，直到所有 etcd 实例可用。以下显示 NFS 主机使用情况，以提供所需的 PV，但可以使用其他持久存储提供程序。一些 Ansible playbook bundles(APB)也可能需要 PV 用于自己使用。目前两个 APB 提供了 ocp :MediaWiki 和 PostgreSQL。两个都需要自己的 PV 部署。

设置 ASB

- 修改 `inventory`，添加 `nfs` 到 `[OSEv3:children]`

		[OSEv3:children]
		masters
		nodes
		nfs
- 添加主机到 `[nfs]` 组中

		[nfs]
		master1.example.com
- 启动服务目录还需要在 `[OSEv3:vars]` 添加内容

		openshift_hosted_etcd_storage_kind=nfs
		openshift_hosted_etcd_storage_nfs_options="*(rw,root_squash,sync,no_wdelay)"
		openshift_hosted_etcd_storage_nfs_directory=/opt/osev3-etcd # nfs 组主机上使用  <nfs_directory>/<volume_name>  创建nfs 卷，例如  /opt/osev3-etcd/etcd-vol2
		openshift_hosted_etcd_storage_volume_name=etcd-vol2 
		openshift_hosted_etcd_storage_access_modes=["ReadWriteOnce"]
		openshift_hosted_etcd_storage_volume_size=1G
		openshift_hosted_etcd_storage_labels={'storage': 'etcd'}
	这些设置在集群安装期间创建了一个连接到 ASB 的 etcd 实例持久卷。

### 设置模版服务 broker
本功能是预览版功能。RH SLA 不支持预览版功能，不建议生产使用。这些功能可以访问并得到用户反馈。有关[技术预览版更多信息](https://access.redhat.com/support/offerings/techpreview/)。启用服务目录web控制台还配置为启动更新的页面来游览服务目录。

如果启用了服务目录，还可以启用模版服务代理(TSB)，配置 TSB

- 必须将一个或者多个项目定义为用于模版和镜像流加载到服务目录中的代理源名称。通过修改 `inventory` 的 `[OSEv3:vars]` 中的下列项目修改

		openshift_template_service_broker_namespaces=['openshift','myproject']	
- 安装程序目前不会自动安装 TSB，因此在集群安装完毕后，必须手动运行其他步骤。继续设置 `inventory` 参考[部署 TSB 的其他步骤](https://docs.openshift.com/container-platform/3.6/install_config/install/advanced_install.html#running-the-advanced-installation)

### 配置 web 控制台自定义
以下变量设置用于自定义 web 控制台的 master 主配置选项。[更多配置](https://docs.openshift.com/container-platform/3.6/install_config/web_console_customization.html#install-config-web-console-customization)

- openshift_master_logout_url

	配置 master 的 `logoutURL` 参考 [设置注销 url](https://docs.openshift.com/container-platform/3.6/install_config/web_console_customization.html#changing-the-logout-url),例 `http://example.com`
- openshift_master_extension_scripts

	配置 master 的 `extensionScripts`，参考[加载扩展脚本和样式表](https://docs.openshift.com/container-platform/3.6/install_config/web_console_customization.html#loading-custom-scripts-and-stylesheets),例 ` ['/path/to/script1.js','/path/to/script2.js']`
- openshift_master_extension_stylesheets

	配置 master 的 `extensionStylesheets`，参考[加载扩展脚本和样式表](https://docs.openshift.com/container-platform/3.6/install_config/web_console_customization.html#loading-custom-scripts-and-stylesheets),例 `['/path/to/stylesheet1.css','/path/to/stylesheet2.css']`
- openshift_master_extensions

	配置 master 的 `extensions `,参考[提供静态文件](https://docs.openshift.com/container-platform/3.6/install_config/web_console_customization.html#serving-static-files)和[自定义页面](https://docs.openshift.com/container-platform/3.6/install_config/web_console_customization.html#customizing-the-about-page)，例 `[{'name': 'images', 'sourceDirectory': '/path/to/my_images'}]`
- openshift_master_oauth_template

	配置 master 的 ` OAuth template`，查看[自定义登录页面](https://docs.openshift.com/container-platform/3.6/install_config/web_console_customization.html#customizing-the-login-page),例 `['/path/to/login-template.html']` 
- openshift_master_metrics_public_url

	配置 master 的 `metricsPublicURL` ，参考[设置监控共用url](https://docs.openshift.com/container-platform/3.6/install_config/cluster_metrics.html#install-setting-the-metrics-public-url)，例子 `https://hawkular-metrics.example.com/hawkular/metrics`
- openshift_master_logging_public_url

	设置 master 的 `loggingPublicURL ` 参考[kibana](https://docs.openshift.com/container-platform/3.6/install_config/aggregate_logging.html#aggregate-logging-kibana)

### 例 `inventory` 文件
- 单 master 例
	
	可以使用单个或多个 master 节点，单个嵌入式 etcd 或多个外部 etcd 主机。安装后单个 master 集群不兼容多 master 集群。
	
	- 单 master 多节点
		- master & node
			- master.example.com 
		- node
			- node1.example.com
			- node1.example.com
	- `inventory`

			# 创建一个包含主节点和组节点的 OSEv3 组
			[OSEv3:children]
			masters
			nodes

			# 设置  OSEv3 通用变量 
			[OSEv3:vars]
			# ssh 用户，免密登录
			ansible_ssh_user=root

			# 如果 ansible_ssh_user 不是 root ，这个参数必须是 true
			#ansible_become=true

			openshift_deployment_type=openshift-enterprise

			# 注销以下内容以启用 htpasswd 身份验证，默认为 DenyAllPasswordIdentityProvider
			#openshift_master_identity_providers=[{'name': 'htpasswd_auth', 'login': 'true', 'challenge': 'true', 'kind': 'HTPasswdPasswordIdentityProvider', 'filename': '/etc/origin/master/htpasswd'}]

			# master 主机组
			[masters]
			master.example.com

			# node 主机组，包含区域信息
			[nodes]
			master.example.com
			node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
			node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
			infra-node1.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
			infra-node2.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
		需要将此例保存到 ` /etc/ansible/hosts`
- 单 master 多 etcd和多 node 
	- master
		- master.example.com 
	- etcd
		- etcd1.example.com
		- etcd2.example.com
		- etcd3.example.com 
	- node
		- node1.example.com
		- node2.example.com 	
	
	指定多个 etcd 主机时，安装并配置外部的 etcd， ocp 不支持嵌入式集群。
	
		[OSEv3:children]
		masters
		nodes
		etcd

		[OSEv3:vars]
		ansible_ssh_user=root
		openshift_deployment_type=openshift-enterprise

		[masters]
		master.example.com

		[etcd]
		etcd1.example.com
		etcd2.example.com
		etcd3.example.com

		[nodes]
		master.example.com
		node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
		node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
		infra-node1.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
		infra-node2.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"		
- 多 master

	可以配置多 master 站点实现高可用，多 master 和 多 node 节点环境。高级安装支持以下高可用方法
	
	- 原生

		利用 ocp 内置的原生 ha master 功能，并可与任何负载均衡解决方案相结合。如果在 `inventory` 定义了部分主机为 `[lb]`,会自动安装和配置 `haproxy` 作为负载均衡的方案，如果没有设定主机，可以使用外部的负载均衡主机，master api 端口为 8443. 
		haproxy 负载均衡宗旨在演示 api 服务器的 ha 模式，不建议生产使用。如果要部署云，RH 建议部署基于云主机的 TCP 负载均衡器，或者采取其他步骤提供的高可用性负载均衡器					
		
		对于外部负载均衡解决方案，必须具备
		
		- 为 ssl 通过配置的预先创建的负载均衡器 vip
		- 一个 vip 监听由 `openshift_master_api_port` 和 `openshift_master_console_port` 指定的端口，默认8443，并代理回该端口的所有主机。
		- dns 注册的 vip 域名
			- 该域名将成为 ocp `openshift_master_cluster_public_hostname ` 和 `openshift_master_cluster_hostname` 的值。
			- haproxy 不建议生产使用。参考[外部负载均衡集成](https://github.com/redhat-cop/openshift-playbooks/blob/master/playbooks/installation/load_balancing.adoc)
		
		关于高可用性主机架构，更多信息参考 [k8s 基础设施](https://docs.openshift.com/container-platform/3.6/architecture/infrastructure_components/kubernetes_infrastructure.html#master) 原生 ha 方案的注意事项
		
		- 高级安装方法当前不支持主动/被动设置多个 haproxy 负载均衡器。请参考[LB 管理员手册](https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Load_Balancer_Administration/ch-lvs-overview-VSA.html)
		- haproxy 设置中，`controller manager`作为独立进程运行。CM 是主要领导者，负责 etcd 的租约。默认情况下，租约在 30秒后过期。如果 master CM 发生故障，则最多需要几秒才选择另一个 master CM.可以使用 `osm_controller_lease_ttl` 调节时间。
- 多 master 多 etcd
	- master
		- master1.example.com
		- master2.example.com
		- master3.example.com
	- lb
		- lb.example.com 
	- etcd
		-  etcd1.example.com
		-  etcd2.example.com
		-  etcd3.example.com	
	- node 
		- node1.example.com
		- node2.example.com 

例子

	[OSEv3:children]
	masters
	nodes
	etcd
	lb

	[OSEv3:vars]
	ansible_ssh_user=root
	openshift_deployment_type=openshift-enterprise

	# 原生高可用集群方案选定负载均衡
	# 如果没有定义 lb 组，安装程序会假定负载均衡已经存在。
	# 对于安装，openshift_master_cluster_hostname 值必须是解析为负载均衡器，或者如果没有负载均衡器，设置其中一个或所有 master
	openshift_master_cluster_method=native
	openshift_master_cluster_hostname=openshift-cluster.example.com
	openshift_master_cluster_public_hostname=openshift-cluster.example.com

	# 应用更新节点默认值
	openshift_node_kubelet_args={'pods-per-core': ['10'], 'max-pods': ['250'], 'image-gc-high-threshold': ['90'], 'image-gc-low-threshold': ['80']}

	# 默认控制器 ttl 租约单位秒
	#osm_controller_lease_ttl=30

	# 启动 master 的ntp 服务已保证故障切换
	openshift_clock_enabled=true


	[masters]
	master1.example.com
	master2.example.com
	master3.example.com

	[etcd]
	etcd1.example.com
	etcd2.example.com
	etcd3.example.com

	# 指定负载均衡
	[lb]
	lb.example.com


	[nodes]
	master[1:3].example.com
	node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
	node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
	infra-node1.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
	infra-node2.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"				

- 多 master 和 etcd 在相同主机
	- master/etcd
		- master1.example.com
		- master2.example.com
		- master3.example.com
	- lb
		- lb.example.com 
	- node
		- node1.example.com
		- node2.example.com 	 			
		
例	
	
	[OSEv3:children]
	masters
	nodes
	etcd
	lb

	[OSEv3:vars]
	ansible_ssh_user=root
	openshift_deployment_type=openshift-enterprise
	openshift_master_cluster_method=native
	openshift_master_cluster_hostname=openshift-cluster.example.com
	openshift_master_cluster_public_hostname=openshift-cluster.example.com

	#osm_controller_lease_ttl=30

	[masters]
	master1.example.com
	master2.example.com
	master3.example.com

	[etcd]
	master1.example.com
	master2.example.com
	master3.example.com

	[lb]
	lb.example.com

	[nodes]
	master[1:3].example.com
	node1.example.com openshift_node_labels="{'region': 'primary', 'zone': 'east'}"
	node2.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"
	infra-node1.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
	infra-node2.example.com openshift_node_labels="{'region': 'infra', 'zone': 'default'}"
### 运行高级安装
通过 ` /etc/ansible/hosts` 定义配置后，可以通过 ansible 运行高级安装手册。生产 ocp 是通过 rpm 安装，而容器化安装是预览版功能。
### 运行基于 RPM 的安装程序
rpm 安装是基于 ansible 在安装机上运行 playbook 和配置文件。如果需要指定非默认的 `/etc/ansible/hosts` 目录下 `inventory`，请使用 `-i` 参数。如果使用代理服务器，则必须将 etcd 端点的 ip 地址和端口添加到 `inventory` 中的 `openshift_no_proxy` 变量中。

	# ansible-playbook  [-i /path/to/inventory] \
    /usr/share/ansible/openshift-ansible/playbooks/byo/config.yml
如果运行失败，可以重新执行进行运行。还可以查看[问题的解决办法](https://docs.openshift.com/container-platform/3.6/install_config/install/advanced_install.html#installer-known-issues)
### 容器安装
所有系统组件使用容器安装都是 ocp 的技术预览版，不能在生产中使用。镜像 `openshift3/ose-ansible` 作为系统容器运行的 ocp 安装程序的容器版本。[系统容器](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html/managing_containers/running_system_containers)存储和运行在 docker 服务之外。使用容器安装与传统的 rpm 安装程序相同，除了它在容器化环境中运行，而不是直接运行在主机上。

1. 使用 docker cli 将镜像拉到本地

		$ docker pull registry.access.redhat.com/openshift3/ose-ansible:v3.6				
- 安装系统容器必须存储在 [ostree](https://access.redhat.com/documentation/en-us/red_hat_satellite/6.2/html/content_management_guide/managing_ostree_content) 中，而不是默认的 docker deamon 存储。使用 atominc cli 将docker 镜像导入到 ostree 存储中。

		$ atomic pull --storage ostree \
    	docker:registry.access.redhat.com/openshift3/ose-ansible:v3.6
- 安装系统容器，启动的服务添加到 systemd 管理

		$ atomic install --system \
			--storage=ostree \
			--name=openshift-installer \ #设置 systemd 服务名
			--set INVENTORY_FILE=/path/to/inventory \ #在工作站指定 inventory 的文件位置
			docker:registry.access.redhat.com/openshift3/ose-ansible:v3.6
- 使用 systemctl 启动服务

		$ systemctl start openshift-installer	
	如果运行失败，可以重新执行进行运行。还可以查看[问题的解决办法](https://docs.openshift.com/container-platform/3.6/install_config/install/advanced_install.html#installer-known-issues)
- 安装完成后，还可以根据需要卸载系统容器。但是如果要安装其他服务，还需要按照之前的流程。卸载容器:

		$ atomic uninstall openshift-installer

### 运行其他的 playbook
集群安装完毕后，如果稍后运行任何其他的容器化程序，例如升级集群等，则需要使用 `PLAYBOOK_FILE` 环境变量。默认 `playbooks/byo/config.yml`，它是主要的集群安装 playbook，当然可以设置成另外一个需要的 playbook 路径。

	$ atomic install --system \
    --storage=ostree \
    --name=openshift-installer \
    --set INVENTORY_FILE=/etc/ansible/hosts \
    --set PLAYBOOK_FILE=playbooks/byo/openshift-cluster/upgrades/v3_6/upgrade.yml \ #  PLAYBOOK_FILE 设置为从 playbook/ 目录开始的相关路径。 ocp 文档中提到其他 playbook 均假定使用基于 rpm 安装，因此使用容器化安装时，请使用相对路径。
    docker:registry.access.redhat.com/openshift3/ose-ansible:v3.6
### 部署模版服务代理
如果已启用服务目录，并要部署模版服务代理(TSB),请在集群安装成功后手动运行以下步骤:
 template service broker(TSB) 限于技术预览。当前使用 TSB 需要打开未经身份验证的集群访问，在解决这个安全问题后才会退出技术预览模式。

1. 在[配置模版服务代理](https://docs.openshift.com/container-platform/3.6/install_config/install/advanced_install.html#configuring-template-service-broker)中，要确保 TSB 的一个或者多个源项目是通过 `openshift_template_service_broker_namespaces` 定义描述的。  
2. 运行以下命令启用 TSB 未认证访问

		$ oc adm policy add-cluster-role-to-group \
    	system:openshift:templateservicebroker-client \
    	system:unauthenticated system:authenticated 
3. 创建模版 `template-broker.yml`

		apiVersion: servicecatalog.k8s.io/v1alpha1
		kind: Broker
		metadata:
			name: template-broker
		spec:
			url: https://kubernetes.default.svc:443/brokers/template.openshift.io
- 使用文件注册 broker

		$ oc create -f template-broker.yml
- 启动 web 控制台中的技术预览版功能使用 TSB ，而不是标准的全局库。
	- 以下是脚本保存到文件中
	
			window.OPENSHIFT_CONSTANTS.ENABLE_TECH_PREVIEW_FEATURE.template_service_broker = true;
	-  添加 master 配置文件 `/etc/origin/master/master-config.yml`
	
			assetConfig:
				...
				extensionScripts:
					- /path/to/tech-preview.js
	- 重启 master service
	
			# systemctl restart atomic-openshift-master 
			
### 验证安装
1. 在 master 节点上验证节点状态

		# oc get nodes
		NAME                        STATUS                     AGE
		master.example.com          Ready,SchedulingDisabled   165d
		node1.example.com           Ready                      165d
		node2.example.com           Ready                      165d
- 验证 web 控制台是否正常，使用主机名和控制台端口通过游览器访问
	- 例如对 `master.openshift.com` 默认端口 8443 的 master 主机访问，应该使用 `https://master.openshift.com:8443/console`
	- 控制台端口可以通过 ` /etc/ansible/hosts` 的`openshift_master_console_port` 参数修改
- 验证多 etcd 主机
	1. 验证是否安装了 etcd 安装包
		
			# yum install etcd
	- 在 master 主机上验证 etcd 集群健康状况，替换 etcd FQDN

			# etcdctl -C \
    			https://etcd1.example.com:2379,https://etcd2.example.com:2379,https://etcd3.example.com:2379 \
    			--ca-file=/etc/origin/master/master.etcd-ca.crt \
    			--cert-file=/etc/origin/master/master.etcd-client.crt \
    			--key-file=/etc/origin/master/master.etcd-client.key cluster-health
    - 验证成员列表

    		# etcdctl -C \
    			https://etcd1.example.com:2379,https://etcd2.example.com:2379,https://etcd3.example.com:2379 \
    			--ca-file=/etc/origin/master/master.etcd-ca.crt \
    			--cert-file=/etc/origin/master/master.etcd-client.crt \
    			--key-file=/etc/origin/master/master.etcd-client.key member list
- 验证多 master 使用的 proxy

	如果安装了多个 master，将会安装 haproxy 做 LB,使用游览器查看 `[lb]` 的健康检查状态
	
		http://<lb_hostname>:9000
	安装方法 [haproxy 配置文档](https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/7/html/Load_Balancer_Administration/ch-haproxy-setup-VSA.html)

### 卸载 ocp
如果要卸载 ocp ，需要在安装机相同目录上执行 `uninstall.yml` 的 playbook,。

	# ansible-playbook [-i /path/to/file] \
	/usr/share/ansible/openshift-ansible/playbooks/adhoc/uninstall.yml
ansible 将会按照这个 playbook 删除 ocp，内容包括:

- 配置
- 容器
- 默认模版和镜像流
- 镜像
- RPM

### 删除节点
可以使用 `uninstall.yml` playbook 卸载特定主机节点。此方法在尝试删除特定指 node 节点时使用，而不能用于 master 节点和 etcd 节点，这需要在集群内部进一步配置。

- 首先按照删除节点的步骤，从集群删除删除对象节点，然后继续执行此过程中的其他步骤。
- 创建仅引用这些主机的 `inventory` 文件，例如

		[OSEv3:children]
		nodes #只包含卸载相关部分

		[OSEv3:vars]
		ansible_ssh_user=root
		openshift_deployment_type=openshift-enterprise

		[nodes]
		node3.example.com openshift_node_labels="{'region': 'primary', 'zone': 'west'}"  #只包含想要卸载的主机
- 通过 `-i` 引入删除 `inventory` 执行卸载

		# ansible-playbook -i /path/to/new/file \
    		/usr/share/ansible/openshift-ansible/playbooks/adhoc/uninstall.yml
- 删除完成后，打开 ocp 控制台检查移除结果。  