## 初步规划
对于生产环境，影响安装的因素有几个，考虑如下问题:

- 那种方式安装，快速安装还是高级安装
- 集群需要多少主机？支持单 master 节点和多 master 节点
- 集群中需要多少个 pod ？ pod 多少是通过公式计算。
- 是否是高可用部署？生产集群建议使用高可用进行容错。这种情况下，希望使用多 master 原生 ha 。
- 那种安装方式，rpm还是docker？两种方式的安装和管理更新方式不同。
- 和其他技术进行集成？[测试集成列表](https://access.redhat.com/articles/2176281)

## 安装方法
快速和高级安装方法都支持开发和生产环境。

- 如果首次部署，请使用快速安装程序，让交互的 cli 指导完成环境相关配置。
- 或者为了最大限度的控制集群配置，可以使用高级方法，如果已经熟悉 aisible 此方法特别适用。还可以直接直接提供的 playbook 进行部署和配置。
- 如果开始使用快速安装，则可以随时调整集群的配置，并使用相同的安装程序工具调整集群的主机数量。切换到高级方法，可以创建一个 inventory 文件，并继续执行操作。

## 集群注意事项
确定 openshift 容器平台需要多少节点和 pod。集群可扩展性和集群环境的pod相关。该数字影像您的设置。

### 公式
- 每节点预期有多少个 pod

	集群需求最大的 pod 数量/每个节点最大的 pod 数量 = 节点数量
- 场景

	一个集群需要 2200 个 pod，假设每个节点最大可以支持 250 个 pod
	
		2200 / 250 = 8.8
	如果节点是20个nod，那么 pod 节点数量
	
		2200 / 20 = 110

## 环境场景
	
```注意:集群不支持从 单 master 迁移到多个 master 集群```

- 单 master 多 node
	- master.example.com (master 加 node)
	- node1.example.com (node)
	- node2.example.com (node)
- 单 master 多 etcd 多 node
	- master.example.com (master 加 node)
	- etcd1.example.com (etcd)
	- etcd2.example.com (etcd)
	- etcd3.example.com (etcd)
	- node1.example.com (node)
	- node2.example.com (node)
	
	```注意:多 etcd 部署是使用外部 etcd 模式，不支持内部部署 ```
- 多 master 部署，原生高可用
	- master1.example.com (master 集群使用高可用加 node)
	- master2.example.com (master 加 node)
	- master3.example.com (master 加 node)
	- lb.example.com (只负载均衡 api server)
	- etcd1.example.com (etcd)
	- etcd2.example.com (etcd)
	- etcd3.example.com (etcd)
	- node1.example.com (node)
	- node2.example.com (node)	
	
	```注意:多 etcd 部署是使用外部 etcd 模式，不支持内部部署 ```
- 企业版部署
	- master1.example.com (master 集群使用高可用 加 node 加 etcd)
	- master2.example.com 
	- master3.example.com 
	- infra1.example.com (平台用负载均衡[ha+keepalived] 加上日志、监控、镜像、路由)
	- infra2.example.com 
	- node1.example.com (node)
	- noden.example.com (node)	
	- glusterfs1
	- glusterfs2
	- glusterfs3
	
	```注意:日志、监控、镜像和服务配置的持久话均放到 glusterfs中 ```
	![](./pic/openshift-deploy.png)
- 独立 registry 部署
- rpm vs 容器部署

	rpm安装通过安装包管理所有服务，并将服务配置为在同一用户空间内运行，而容器方式则使用容器镜像安装，并在单独的容器中运行。

## 准备工作
下面标识了 ocp 需要的主机硬件和系统功能要求，安装必须需要红帽订阅。 ocp 3.6 需要 docker 1.12 版本。
### 最小硬件配置
- master
	- OS
		- RH 7.3/7.4 版本最小安装		
	- 硬件
		- 2C/16GB 
		- disk
			- /var 最小 40 GB
			- ／usr/local/bin 最小 1 GB
			- 包含 /tmp 目录最小需要 1 GB
- node
	- OS
		- RH 7.3/7.4 版本最小安装
		- NetworkManager 1.0 或者更新
	- 硬件
		- 1C/8GB
		- disk
			- /var 最小 15 GB
			- ／usr/local/bin 最小 1 GB
			- 包含 /tmp 目录最小需要 1 GB
			- 未分配空间给 docker 使用最小 15 GB
- etcd
	
	etcd 存储最少 20 GB，如果集群较大，建议使用 ssd

## 生产级硬件要求
- master

	在上面的基础上，每管理 1000 pod 就需要增加 1C/1.5GB。因此 2000 POD 就需要 4C/19GB，最少需要3台 master 和独立的负载均衡器。 openshift 缓存资源设定是为了缓解 cpu 负载。然而小于 1000 个 pod 将浪费大量内存。所以调节，默认缓存条数 5w 条，根绝资源大小，可以增加 1-2 GB内存。可以通过 `/etc/origin/master/master-config.yaml` 设置。
	
		kubernetesMasterConfig:
			apiServerArguments:
    			deserialization-cache-size:
    			- "1000"
- node

	node 主机大小取决于预期工作负载。作为管理员，需要计算预期工作负载，然后增加 10% 开销。对于生产请分配足够的资源以便主机故障不会影响最大容量。
	
	- 单集群最大节点数 - 2000
	- 单集群最大 pod 数 - 120,000
	- 单节点最大 pod 数 - 250
	- 单核 cpu 最大 pod - 10

### cpu 使用
默认情况下， ocp 平台 master 和 node 使用系统中的所有 cpu 核数。可以通过设置 `GOMAXPROCS` 改变。

例如: 设置仅使用一个核心

	启动前设置
	# export GOMAXPROCS=1
### SELinux
启动 ocp 前，必须启动 SELinux，否则安装程序将失败。另外还需要在 `/etc/selinux/config` 配置 `SELINUXTYPE=targeted`

	# This file controls the state of SELinux on the system.
	# SELINUX= can take one of these three values:
	#     enforcing - SELinux security policy is enforced.
	#     permissive - SELinux prints warnings instead of enforcing.
	#     disabled - No SELinux policy is loaded.
	SELINUX=enforcing
	# SELINUXTYPE= can take one of these three values:
	#     targeted - Targeted processes are protected,
	#     minimum - Modification of targeted policy. Only selected processes are protected.
	#     mls - Multi Level Security protection.
	SELINUXTYPE=targeted
### 使用 overlayFS
从 RH 7.4 开始，ocp 就支持 overlayFS。overlay1 和 2 均支持。但是推荐使用 overlay2，因为它更简单和速度更快。有关设置 [overlayFS 驱动](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux_atomic_host/7/html-single/managing_containers/#overlay_graph_driver)
### NTP
必须使用 NTP 服务防止节点间时间不同步。需要在 ansible playbook 设置 `openshift_clock_enabled` 为 true。

	# openshift_clock_enabled=true
### 安全警告
ocp 在主机上运行容器，在某些情况下，如构建操作和镜像仓库服务时，需要使用特权容器。此外，这些容器访问主机 docker 守护进程，可以执行 docker 构建和推送操作。因此，可以在这些容器中运行 docker 操作和 root 权限，这些会有安全隐患。为了解决这些风险， ocp 使用上下文来约束控制 pod 可以操作和访问的能力。

相关文章

- http://opensource.com/business/14/7/docker-security-selinux
- https://docs.docker.com/engine/security/security/
### 环境要求
需要包含了对 ocp 配置环境要求，包括网络、外部服务访问，例如 git库，存储，云设施。
### DNS
ocp 需要在环境中使用全功能 dns 服务器。最理想是独立运行 dns 服务器，可以在主机和容器层提供解析功能。每个主机上设置 `/etc/hosts` 文件是不够的，因为不会复制到平台运行的容器中。 ocp 的关键组件均在容器内部运行，并使用一下过程进行名称解析"

- 默认情况下，容器从其他主机收到其 dns 配置文件
-  ocp 将植入到 pod，该值在 `/etc/origin/node/node-config.yaml` 文件中国年由 dnsIP 参数定义，该参数默认设置为主机节点地址，因为主机使用 dnsmasq。
-  如果从该文件中国年省略了 dnsIP 参数，则该默认为 kubernetes 服务IP，它是该 pod 的 `/etc/resolv.conf` 文件中的第一个 dns 服务器。

从 ocp 3.2 版本开始，dnsmasq 会自动在所有主节点和计算节点上进行配置。 pod 使用节点作为其 dns 服务。默认情况下， dnsmasq 在节点上配置 53 监听端口，因此节点不能运行其他类型的 dns 应用程序。

``` 节点上需要 NetworkManger 才能使用 DNS IP 地址填充 dnsmasq。```
```必须使用 dnsmasq (openshift_use_dnsmasq=true) ,否则会安装失败，关键功能将无法运行。```

例:  dns 解析

	master    A   10.64.33.100
	node1     A   10.64.33.101
	node2     A   10.64.33.102
 
 如果 dns 服务运行不正常，可能会在一下情况遇到故障:
 
 - 通过 ansible-base 脚本产品安装
 - 部署基础设施服务 (registry,router)
 - 访问 ocp 控制台，因为她不能通过 ip 地址单独访问
 
####  配置主机使用 dns
需要确保集群内没个主机都配置了 dns 服务器解析主机名，dns 解析配置取决于是否使用 dhcp 服务。

- 启用 dncp 服务
	- 先禁用 dhcp 服务，然后将网络接口配置为静态，并将 dns 服务器添加到 NetworkManager
	- 启用后，NetworkManager 调度脚本将自动根据 DHCP 配置设置 DNS。或者可以在 `node-config.yam` 文件中的 `dnsIP` 中添加一个值来遇险安装 pod 的 resolv.conf 文件。然后由主机的第一个 dns服务器定义第二个 dns 服务器。默认情况下，这将是节点的 ip 地址。

			注意
			对于大多数配置， ocp 的高级安装 ansible 期间不要设置 openshift_dns_ip ,因此此选项将覆盖 dnsIP 设置的默认 IP 地址。
			
			相反，允许安装程序设置每个节点使用的 dnsmasq 并将请求转发给 SkyDNS 或外部 DNS 提供程序。如果设置了 openshift_dns_ip ，则应该首先查询 SkyDNS 的 DNS IP 或者 SkyDNS 服务或者 kubernetes 服务IP。

验证主机 dns 服务器:

- 检查 `/etc/resolv.conf`

		$ cat /etc/resolv.conf
		# Generated by NetworkManager
		search example.com
		nameserver 10.64.33.1
		# nameserver updated by /etc/NetworkManager/dispatcher.d/99-origin-dns.sh 			
- 测试 dns 解析

		$ dig <node_hostname> @<IP_address> +short
	例如
	
		$ dig master.example.com @10.64.33.1 +short
		10.64.33.100
		$ dig node1.example.com @10.64.33.1 +short
		10.64.33.101
		
#### 配置 dns 通配符(可选)
为了要使用的路由器配置通配符，以便在添加新路由时不需要更新 DNS 配置。dns 区域的通配符必须最终解析为 ocp 路由器的ip地址。

例如为具有低生存生存时间值(TTL)的云应用创建通配符 dns 条目，并指向部署路由器的主机公共 ip 地址:

	*.cloudapps.example.com. 300 IN  A 192.168.133.2
几乎所有情况下，引用虚拟机时，必须使用主机名，并且使用的主机名必须于每个节点上的 `hostname -f` 命令的输出想匹配。
``` 每个节点上的  /etc/resolv.conf 文件中，必须确保由通配符条目的 dns 服务器未列为名称服务器，或者通配符域未列在搜索列表中。否则，由 ocp 管理的容器可能无法正确解析主机名。 ```
### 网络访问
master 节点和计算节点之间必须存在共享网络。如果计划使用高级安装方法为多个主机配置高可用性，则还必须在安装过程中选择要配置的 vip 的 ip 地址。选择 ip 必须在所有节点之间路由，如果使用 FQDN 进行配置，则应在所有节点进行解析。
#### NetworkManager
NetworkManager 是一种用于提供系统功能自动连接到网络的检测和配置程序。
#### 需要端口
 ocp 安装会使用 iptables 在每个主机上自动创建一组内部防火墙规则。但如果网络配置使用外部防火墙，如硬件，则必须确保基础设施组件可以通过允许特定端口通讯。
 
- node 节点之间
	- 4789/udp

		sdn 网络通讯
- node 到 master
	- 52 or 8053/tcp+udp

		集群服务 skydns 解释使用。3.2版本后默认使用8053， 53将给 dnsmasq使用
	- 4789/udp

		sdn 网络通讯
	- 443／8443

		node 节点需要与 master 节点 api 通讯，返回状态，接手任务等。
- master 到 node
	- 4789/udp

		sdn 网络通讯
	- 10250/tcp

		oc 控制 kublet
- master 到 master
	- 53(L) or 8053(L)/tcp+udp
	- 2049(L)/tcp+udp
	
		nfs 主机配置为安装的一部分需要
	- 2379/tcp

		etcd 接收使用
	- 2380/tcp

		etcd 选举使用
	- 4001/tcp (L)

		etcd 接受状态更改
	- 4789 (L)/udp

		sdn pod 之间通讯
	
	``` 注意(L) 表示标记端口也用于环回模式，使主设备能够与自身通讯。单主集群中，标记(L)的端口必须打开。没标记的(L)不需要开。多主集群所有端口必须打开。```
- 外部到负载均衡
	- 9000/tcp

		haproxy 统计页面	
- 外部到 master
	- 443/8443/tcp

		外部访问使用
- 部署需求
	- 22/tcp
	
		安装程序使用
	- 53 or 8053/tcp+udp

		域名解析使用
	- 80 or 443/tcp

		http/https 使用，所有节点对外都需要打开，特别是运行路由器节点。
	- 1936/tcp

		对于路由器统计使用。运行模版路由器以访问统计信息时需要打开，并且可以在外部或者内部打开链接。
	- 4001/tcp

		etcd 对于服务器-客户端连接，主机对内打开。
	- 2379+2380/tcp

		2379对于服务器-客户端连接，2380用于服务器-服务器连接。
	- 4789/udp

		对于 ovs vxlan 使用，仅在节点主机内部使用。
	- 8443/tcp

		ocp 控制台使用，与 api服务器共享。
	- 10250/tcp				 		
	
		kubelet 使用，需要在节点外部打开。
	- 注意
		- 用于数据报文协议。
		- 当部署 sdn 时，通过服务器代理访问 pod 网络，除非它在和 registry 的同一节点访问registry。
		- ocp 内部 dns 无法通过 sdn 接接收。根据监测到的 `openshift_facts` 值或者 `openshift_ip` 和 `openshift_public_ip` 值被覆盖，它将是 `openshift_ip` 的计算值。 对于非云部署，默认与 master 主机上的默认路由相关联的ip地址。对于云部署，它将默认为云数据定义的与第一个内部接口相关联的ip地址。
		- master 主机使用端口 10250 到达节点，不会超过 sdn。它取决于部署的目标主机，并使用计算 `openshift_hostname` 和 `openshift_public_hostname`
- 日志聚合
	- 9200/tcp

		es api 使用。需要在任何基础架构节点内部打开。以便 kibana 能够检索日志。可以通过线路直接进入 es 进行外部打开。可以使用 `oc expose` 创建。
	- 9300/tcp  

		对于 es 集群之间使用。需要在任何基础架构节点上内部打开。

### 持久化存储
kubernetes [持久化卷](https://docs.openshift.com/container-platform/3.6/architecture/additional_concepts/storage.html#architecture-additional-concepts-storage)框架允许使用环境中可用的网络存储来为 ocp 提供持久化支持。

安装和配置指南为集群管理员提供了 nfs、glusterfs、ceph rbd、openstack cinder、AWS EBS 、GCE 持久化磁盘和 ISCSI 持久化存储的指导说明。
### 云安装的思考
#### 配置安全组
在 aws 和 openstack 上安装时，请确保设置了相应的安全组。这应该包含安全组中的一些端口。可能需要更多以来于安装的集群配置。

- 所有 ocp 主机
	- tcp/22
- etcd 组
	- tcp/2379 master 主机
	- tcp/2380 etcd 主机
- master 组
	- tcp/8443 0.0.0.0
	- tcp/53 ocp 所有主机
	- udp/53 ocp 所有主机
	- tcp/8053 ocp 所有主机
	- udp/8053 ocp 所有主机 
- node 组
	- tcp/10250 master 主机
	- udp/4789 node 主机
- 基础设施节点
	- tcp/443  0.0.0.0
	- tcp/80 0.0.0.0

如果配置 ELB 用于负载均衡 master 设备或路由器，则还需要适当地为 ELB 配置 ingress 和 Egress 安全组

### 检测主机和主机名
某些部署要求用户覆盖检测到主机名和主机ip地址。要查看默认值，可以运行 openshift_facts playbook

	# ansible-playbook [-i /path/to/inventory] \
    /usr/share/ansible/openshift-ansible/playbooks/byo/config.yml
现在验证检测到常见设置。 这些参数可以覆盖，通过更改变量。

- hostname
	
	应该是实例内部 resolve 对应内部ip，通过 `openshift_hostname` 覆盖 
- ip

	应该是实例内部的ip，通过 `openshift_ip` 覆盖
- public_hostname

	应该是 resolve 的外部ip，通过 `openshift_public_hostname` 覆盖
- public_ip

	应该是与实例相关的外部可访问的 ip,通过 `openshift_public_ip` 覆盖
- use_openshift_sdn   

	应该是 true，除非是 gce，使用 `openshift_use_openshift_sdn`

如果 `openshift_hostname` 设置为除源数据提供的 `private-dns-name` 值之外的值，那么这些程序的本机集成将不再起作用。

在 AWS，需要覆盖变量的情况包括

- hostname
- ip
- public_hostname
- public_ip

如果 `openshift_hostname` 设置为除源数据提供的 `private-dns-name` 值之外的值，那么这些程序的本机集成将不再起作用。

特别是对于 EC2 主机，它们必须部署在启动 DNS主机名和 DNS 解析的vpc 中，而且 `openshift_hostname` 不能覆盖

#### 云安装后
安装后，可以在 aws、GCE、openstack配置 ocp
### 集装箱 glusterfs 注意事项
如果选择为集群配置容器化的 GlusterFS持久化存储，或者选择容器话的GlusterFS，必须考虑一下条件。
#### 存储接待你
- 使用容器化的 GlusterFS 持久化存储
	- 最少需要3个存储节点
	- 每个节点最少必须有一个原始块设备，不小于 100GB 可用
- 使用 GlusterFS 支持的 ocp 的 docker registry
	- 最少3个存储节点
	- 每个节点最少必须有一个原始块设备，不小于 10GB 可用

	虽然容器化 GlusterFS 持久化存储可以配置和部署在 ocp 集群上，但它们的存储应该彼此分开，并且还需要额外的存储节点。例如同时设置了2个，则需要共6个存储节点，3个用于 docker registry，3个用于持久化。这种限制为了避免 i/o层面和创建卷中的性能影响。

#### 存储需要组件
对于任何 RH 存储节点，必须启用 rpm 安装

	# subscription-manager repos --enable=rh-gluster-3-client-for-rhel-7-server-rpms
将这个执行在所有 GlusterFS 要安装的节点上，`mount.glusterfs` 命令必须可用。对于基于 RPM 系统，必须安装 

	# yum install glusterfs-fuse
## 主机准备
系统准备需要安装 RH 7.3 或者 7.4 。
### 主机注册
1. 每台主机注册 RHSM

		# subscription-manager register --username=<user_name> --password=<password>
2. 查询可用订阅	

		# subscription-manager list --available --matches '*OpenShift*'
3. 订阅

		# subscription-manager attach --pool=<pool_id>
4. 关闭 yun repo
	- 关闭 RHSM

			# subscription-manager repos --disable="*"
	- 列出剩余的 yum repo

			# yum repolist
	- 使用 `yum-config-manager` 禁用剩余的 yum repo

			# yum-config-manager --disable <repo_id>	
		或者禁用所有库
		
			 yum-config-manager --disable \*
5. 仅启用 ocp repo

		# subscription-manager repos \
    		--enable="rhel-7-server-rpms" \
    		--enable="rhel-7-server-extras-rpms" \
    		--enable="rhel-7-server-ose-3.6-rpms" \
    		--enable="rhel-7-fast-datapath-rpms"

### 安装基础包
1. 基础包

		# yum install wget git net-tools bind-utils iptables-services bridge-utils bash-completion kexec-tools sos psacct
2. 升级

		# yum update
3. 如果计划使用基于 rpm 的安装程来运行高级安装，则跳过。如果使用容器安装(技术预览版)
	- 安装 atomic 包

			# yum install atomic
	- 跳过 docker 安装
4. 安装以下软件包，该包提供基于 rpm 的 ocp 安装使用程序，并提供快速和高级安装方法所需的其他工具。

		# yum install atomic-openshift-utils 

#### 安装 atomic 主机系统功能		 		
1. 升级系统

		# atomic host upgrade
2. 重启

		# systemctl reboot

### 安装 docker 
应该在所有主机安装 docker

1. RH 7 系统安装，docker 1.12, Atomic 系统默认安装和配置了 docker 

		# yum install docker-1.12.6
2. 	验证安装

		# rpm -V docker-1.12.6
		# docker version				
3. 配置 docker，编辑 ` /etc/sysconfig/docker` 并添加参数 ` --insecure-registry 172.30.0.0/16`

		OPTIONS='--selinux-enabled --insecure-registry 172.30.0.0/16'			 
	如果是快速安装，可以使用 kickstart或者cloud-init 设置完成。
	
		# sed -i '/OPTIONS=.*/c\OPTIONS="--selinux-enabled --insecure-registry 172.30.0.0/16"' \
		/etc/sysconfig/docker	
	``` 注意:高级安装自动更改	```

	参数解释
	
	- `--insecure-registry` 选项表示 docker daemon 信任任何子网的 docker registry 而不要求证书。
		- 172.30.0.0/16 是 master-config.yml 文件中 `servicesSubnet` 变量的默认值。如果变化，上述步骤的值就应该被调整。注意 `openshift_master_portal_net variable` 变量可以在可选清单中设置，并在高级安装方法中使用，以修改 `servicesSubnet` 变量
	- ocp 安装完毕后，可以选择集成 docker registry，该 registry 需要调整  --insecure-registry。
#### 配置 docker 存储
容器以及创建镜像存储在 docker 存储后端，这种存储是暂时的，与分配任何持久存储分离，已满足应用程序的需要。

- 对于 atomic 主机

	docker 默认存储后端是一个池逻辑卷，在生产环境中，受支持。必须保证系统要求中提到的 docker 存储分配充足空间。
- 对于 RH 主机

	默认存储后端环路的 DM 设置不支持生产使用，仅可以概念使用。对于生产，必须建议创建一个新逻辑卷，并重新配置docker使用。
	
	可以使用 docker 附带的  docker-storage-setup 脚本创建一个池设备并配置到 docker 驱动使用。这可以在安装 docker 完成后做，并在创建镜像或容器之前完成。脚本从 ` /etc/sysconfig/docker-storage-setup` 文件读取配置选项，并支持创建逻辑卷
	
	1. 使用附加块设备
	
		最推荐使用，独立设备
	- 使用现在存在的卷组
	- 使用根文件系统所在卷中剩余的可用空间
	
		2/3 选项都需要在配置主机时留出可用空间。选择c会导致某些应用程序出现问题，如 `Red Hat Mobile Application Platform (RHMAP).`
-  创建 `docker-pool` 卷
	- 选择 1 使用附加的块设备

		 在 `/etc/sysconfig/docker-storage-setup` 中，将 DEVS 设置为要使用的块设备路径。将 VG 设置为要创建的卷组名称，docker-vg 是一个合理的选择。如:
		 
		 	# cat <<EOF > /etc/sysconfig/docker-storage-setup
			DEVS=/dev/vdc
			VG=docker-vg
			EOF			
		运行 `docker-storage-setup` 并查看运行结果

			# docker-storage-setup                                                                                                                                                                                                                               
			0
			Checking that no-one is using this disk right now ...
			OK

			Disk /dev/vdc: 31207 cylinders, 16 heads, 63 sectors/track
			sfdisk:  /dev/vdc: unrecognized partition table type

			Old situation:
			sfdisk: No partitions found

			New situation:
			Units: sectors of 512 bytes, counting from 0

			Device Boot    Start       End   #sectors  Id  System
			/dev/vdc1          2048  31457279   31455232  8e  Linux 			LVM
			/dev/vdc2             0         -          0   0  Empty
			/dev/vdc3             0         -          0   0  Empty
			/dev/vdc4             0         -          0   0  Empty
			Warning: partition 1 does not start at a cylinder boundary
			Warning: partition 1 does not end at a cylinder boundary
			Warning: no primary partition is marked bootable (active)
			This does not matter for LILO, but the DOS MBR will not boot this disk.
			Successfully wrote the new partition table

			Re-reading the partition table ...

			If you created or changed a DOS partition, /dev/foo7, say, then use dd(1)
			to zero the first 512 bytes:  dd if=/dev/zero of=/dev/foo7 bs=512 count=1
			(See fdisk(8).)
			Physical volume "/dev/vdc1" successfully created
			Volume group "docker-vg" successfully created
			 Rounding up size to full physical extent 16.00 MiB
			 Logical volume "docker-poolmeta" created.
			 Logical volume "docker-pool" created.
			 WARNING: Converting logical volume docker-vg/docker-pool and docker-vg/docker-poolmeta to pool's data and metadata volumes.
			  THIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.)
			  Converted docker-vg/docker-pool to thin pool.
			  Logical volume "docker-pool" changed.
	- 选择 2 使用现在存在的卷组
	
		设置 `/etc/sysconfig/docker-storage-setup`
			
			# cat <<EOF > /etc/sysconfig/docker-storage-setup
			VG=docker-vg
			EOF
		运行 `docker-storage-setup` 并查看运行结果
			
			# docker-storage-setup
			Rounding up size to full physical extent 16.00 MiB
			Logical volume "docker-poolmeta" created.
			Logical volume "docker-pool" created.
			WARNING: Converting logical volume docker-vg/docker-pool and docker-vg/docker-poolmeta to pool's data and metadata volumes.
			THIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.)
			Converted docker-vg/docker-pool to thin pool.
			Logical volume "docker-pool" changed.
	- 选择 3 根文件系统所在卷组中剩余可用空间。
	
		验证跟文件系统所在的卷组是否具有所需要的可用空间，然后在运行 ` docker-storage-setup`
		
			# docker-storage-setup
			Rounding up size to full physical extent 32.00 MiB
			Logical volume "docker-poolmeta" created.
			Logical volume "docker-pool" created.
			WARNING: Converting logical volume rhel/docker-pool and rhel/docker-poolmeta to pool's data and metadata volumes.			THIS WILL DESTROY CONTENT OF LOGICAL VOLUME (filesystem etc.)
			Converted rhel/docker-pool to thin pool.
			Logical volume "docker-pool" changed.		
		验证配置,应该在 `/etc/sysconfig/docker-storage` 文件和 docker-pool 逻辑卷中有一个 `dm.thinpooldev`
		
			# cat /etc/sysconfig/docker-storage
			DOCKER_STORAGE_OPTIONS=--storage-opt dm.fs=xfs --storage-opt
			dm.thinpooldev=/dev/mapper/docker--vg-docker--pool

			# lvs
			LV          VG   Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
			docker-pool rhel twi-a-t---  9.29g             0.00   0.12						
		注意验证 docker-pool 大小。
	- 检查 docker 
	
			# systemctl is-active docker
	- 设置启动

			# systemctl enable docker
			# systemctl start docker
	- 初始化

			# systemctl stop docker
			# rm -rf /var/lib/docker/*
			# systemctl restart docker	
### 重新配置 docker 存储
如果需要在创建存储后，重新分配，则首先要删除 docker-pool 逻辑卷。如果使用专用卷组，则还应该根据上述说明在重新配置 docker-storage-setup 之前删除卷组和任何关联的物理卷。
### 启用镜像签名支持
ocp 能够验证加密的来自受信来源图像。[docker 安全指南](https://docs.openshift.com/container-platform/3.6/security/deployment.html#security-deployment-from-where-images-deployed)提供了签名工作原理的高级描述。可以使用命令行界面，配置镜像签名验证(docker 版本超过1.12.5)
- 安装 atomic 包

		yum install atomic
- ` atomic trust` 子命令管理信任配置。默认配置是将所有 registry 列入白名单。这意味着没有配置签名验证。

		$ atomic trust show
		* (default)                         accept
- 合理的配置特定的registry或命名空间，黑名单是不信任的registry 列表，并在供应商 registry 表傻姑娘要求签名验证。

		$ atomic trust add --type insecureAcceptAnything 172.30.1.1:5000

		$ atomic trust add --sigstoretype atomic \
		--pubkeys pub@example.com \
		172.30.1.1:5000/production

		$ atomic trust add --sigstoretype atomic \
		--pubkeys /etc/pki/example.com.pub \
		172.30.1.1:5000/production

		$ atomic trust add --sigstoretype web \
		--sigstore https://access.redhat.com/webassets/docker/content/sigstore \
		--pubkeys /etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release \
		registry.access.redhat.com

		$ sudo atomic trust show
		* (default)                         accept
		172.30.1.1:5000                     accept
		172.30.1.1:5000/production          signed 		security@example.com
		registry.access.redhat.com          signed 		security@redhat.com,security@redhat.com
- 当所有签名来源被验证过，可以通过全局拒绝默认来进一步加固

		$ atomic trust default reject

		$ atomic trust show
		* (default)                         reject
		172.30.1.1:5000                     accept
		172.30.1.1:5000/production          signed 		security@example.com
		registry.access.redhat.com          signed 		security@redhat.com,security@redhat.com
使用 atomic 手册的例子，下文的目录包含主机信任配置

	- /etc/containers/registries.d/*
	- /etc/containers/policy.json
可以直接在每个节点上管理信任配置，或者在单独主机上管理生成文件，并使用 [ansible 进行分发](https://access.redhat.com/articles/2750891#automating-cluster-configuration)。

### 容器日志管理
容器日志会运行在节点上 ` /var/lib/docker/containers/<hash>/<hash>-json.log` 	可能会有大小问题。可以通过配置 json 文件日志驱动来限制和管理。聚合日志仅支持 journald 驱动。[更多信息](https://docs.openshift.com/container-platform/3.6/install_config/aggregate_logging.html#fluentd-upgrade-source)

- 设置 docker 日志

		OPTIONS='--insecure-registry=172.30.0.0/16 --selinux-enabled --log-opt max-size=1M --log-opt max-file=3'
- 重启

		# systemctl restart docker
### 查看日志
日志目录在 `/var/lib/docker/containers/<hash>/`

		# ls -lh /var/lib/docker/containers/f088349cceac173305d3e2c2e4790051799efe363842fdab5732f51f5b001fd8/
		total 2.6M
		-rw-r--r--. 1 root root 5.6K Nov 24 00:12 config.json
		-rw-r--r--. 1 root root 649K Nov 24 00:15 f088349cceac173305d3e2c2e4790051799efe363842fdab5732f51f5b001fd8-json.log
		-rw-r--r--. 1 root root 977K Nov 24 00:15 f088349cceac173305d3e2c2e4790051799efe363842fdab5732f51f5b001fd8-json.log.1
		-rw-r--r--. 1 root root 977K Nov 24 00:15 f088349cceac173305d3e2c2e4790051799efe363842fdab5732f51f5b001fd8-json.log.2
		-rw-r--r--. 1 root root 1.3K Nov 24 00:12 hostconfig.json
		drwx------. 2 root root    6 Nov 24 00:12 secret
### 确保主机访问
快速和高级安装方法要求有访问所有主机用户的权限，如果要非 root 用户身份安装，必须在每个目标主机上配置无密码的 sudo 权限。
## 容器安装方法
### 概述
通过 RPM 包管理安装所有服务并将服务配置为同一个用户空间内运行，而容器安装则使用容器镜像安装服务，并在单个容器中运行单独服务。 RH 默认上安装 ocp 的安装方法是 rpm。但如果系统是 atomic ，容器安装是唯一可用，并基于检测 `/run/ostree-booted` 文件自动选择它。
- rpm 和容器安装之间的差异
	- RPM
		- 安装模式 yum
		- 服务管理 systemd
		- OS RH
	- 容器
		- 安装模式 docker images
		- 服务管理 docker systemd
		- OS RH 或 atomic

### 容器安装方法
与 RPM 一样，容器化安装也可以选择快速安装和高级安装两种。

- 对于快速安装

	可以在交互式安装期间选择每个主机的安装方法是容器还是rpm，也可以在安装配置文件中手动设置。   						
- 对于高级安装

	可以选择 ansible 的 inventory 文件中设置变量 `containerized=true`
- 对于线下安装

	要安装 etcd 容器，可以将 ansible 变量 osm_etcd_image 设置为本地 registry 的镜像名。例如 `registry.example.com/rhel7/etcd`。多 master 的环境，负载均衡无法通过安装过程作为容器进行部署。

### 所需镜像
容器需使用镜像如下

- openshift3/ose
- openshift3/node
- openshift3/openvswitch
- registry.access.redhat.com/rhel7/etcd

默认情况下，所属镜像均从 `registry.access.redhat.com` 提取出来。如果需要在安装过程中使用私有 registry 部署这些镜像，则需要提前指定 registry 信息。对于高级安装方法，可以根据 ansible 的 inventory 变量进行设置。

	cli_docker_additional_registries=<registry_hostname>
	cli_docker_insecure_registries=<registry_hostname>
	cli_docker_blocked_registries=<registry_hostname>
对于快速安装方法，可以在每个主机上设置以下环境变量

	# export OO_INSTALL_ADDITIONAL_REGISTRIES=<registry_hostname>
	# export OO_INSTALL_INSECURE_REGISTRIES=<registry_hostname>
暂时快速安装方法无法指定 Blocked registry。需要配置额外不安全的 registry，以确保在镜像操作前设置完毕。

### cli 封装器
使用容器安装，在每个主机的 ` /usr/local/bin/openshift` 都部署了一个 cli 脚本包。还提供了以下符号链接来简化管理任务。

- 开发 cli

		/usr/local/bin/oc
- 管理员 cli

		/usr/local/bin/oadm
- k8s cli		

		/usr/local/bin/kubectl

封装器调用每次都会生成一个新容器，所以它比本机 cli 操作运行要慢。封装起安装了一个有限的路径子集：

- ~/.kube
- /etc/origin/
- /tmp/

传入 oc 或 oadm 命令处理文件时，可能会发现更容易重定向输入，如:

	# oc create -f - < my-file.json
封装器仅用于引导环境。在向用户授权了 `cluster-admin` 后，应该在其他主机安装 cli 工具.

### 启动和停止容器
安装过程中会创建相关的系统单元，可以使用正常的 systemctl 命令来启动、停止、和轮询服务。对于容器安装，这些单元名与 rpm 安装的名称相同，但 etcd 除外 `etcd_container`。

更改是必须的，因为目前 atomic 随机会安装一个 etcd，因此在安装容器版本 ocp 时，要禁用 etcd 服务。该 etcd 包将会从 atomic 中删除。
### 文件路径
所有 ocp 配置文件都将放在同一个位置，在容器化的安装过程中，已 rpm 为基础的安装，并且可以在 os-tree 升级过程中使用。 但默认[镜像和模版文件](https://docs.openshift.com/container-platform/3.6/install_config/imagestreams_templates.html#install-config-imagestreams-templates)在 `/etc/origin/examples/ for containerized`，而不是标准的 `/usr/share/openshift/examples/`，所以该目录在 atomic 系统是只读的。
### 存储要求
 atomic 安装通常具有非常小的根文件系统。但 etc、master、node 容器数据存在 ` /var/lib/` 目录中。安装 ocp 之前，确保根目录有足够空间。
### ovs sdn 初始化
ovs 初始化需要重新配置 docker 网桥，并重新启动 docker。当节点在容器内运行时，这会使情况复杂化。当使用 ovs 时，将看到 node 启动，重新配置 docker，重启 docker所有容器。这种情况下，node 服务可能无法启动或重启多次，因 master 服务也要和 docker 一起重启。当前实现的一种解决方案是它依赖于基于 docker 的 systemd 设备中设置 `Restart=always` 参数。
## 快速安装
快速安装方法允许使用交互的 cli 程序 `atomic-openshift-installer `在一组主机上安装 ocp。此安装程序可以通过安装 rpm 或启动容器来部署 ocp。 atomic 支持运行容器的 ocp 服务，安装程序由 rpm 提供，默认情况下 atomic 主机中不可用。因此它必须从 RH 系统中运行。启动安装的主机不包含在 ocp 集群中。提供这种安装方式通过交互式收集每个主机上所需要的数据，使安装体验变得更加容易。安装程序是一个独立的封装器，用于 RH 上使用。除了从头开始执行交互式命令安装外，还可以使用预定义的安装配置文件运行或重新运行 `atomic-openshift-installer` 命令。

- 运行无人值守安装
- 将节点增加到现有集群  
- 升级集群
- 重新安装 ocp

对于更复杂的环境，可以使用高级安装方法。安装容器话 ocp ，需要一个独立的 registry ，请参考[独立安装 registry](https://docs.openshift.com/container-platform/3.6/install_config/install/stand_alone_registry.html#install-config-installing-stand-alone-registry)	

### 安装前
安装程序允许定义主机上安装 ocp master 节点组件。默认情况下，安装过程中需要指定主设备的任何主机都将配置为节点，以便主设备配置成 ocp ovs 的一部分。然而 master  node 组件被标记为不可用，这防止将业务调度到主节点。安装完成后，可以将其标记为可调度。在安装 ocp 之前，必须满足几个条件，其中包括验证系统和环境要求，并正确安装配置 docker。还必须准备安装过程中每个目标主机提供以下验证信息:

- ssh 登录权限(root 或者 sudo 用户)
- 主机名
- 是否为 master 节点或者 node 节点安装组件
- 使用 rpm 还是容器安装
- 内部外部 ip 地址

如果正在使用容器化方法(RH 可选 atomic 必须)安装 ocp ，请参考[在容器话主机安装](https://docs.openshift.com/container-platform/3.6/install_config/install/rpm_vs_containerized.html#install-config-install-rpm-vs-containerized)，确保了解差异。

### 运行安装
运行安装程序

	$ atomic-openshift-installer install
然后按照屏幕上说明安装新的 ocp 集群。安装完成后，请备份已经创建的 `~/.config/openshift/installer.cfg.yml `	安装配置文件，如果以后重新安装运行，则将其添加到集群中，或升级集群。然后验证安装。
### 定义安装配置文件
安装程序可以使用预定义的安装配置文件，其中包含有关安装，单个主机和集群的信息。运行交互式安装时，将在 ` ~/.config/openshift/installer.cfg.yml` 中创建一个基于交互结果的安装配置文件。如果被指定退出安装已手动修改配置或者安装方式完成，则会创建该文件。当然也可以从头开始手动创建配置文件以执行无人参与的安装。
#### 配置文件样例
```
version: v2 #安装配置文件的版本，唯一有效的就是v2
variant: openshift-enterprise #安装 ocp 设置为 openshift-enterprise
variant_version: 3.6 #设置安装 ocp 版本，不填写默认安装最新版本
ansible_log_path: /tmp/ansible.log # 定义可存储日志的存储位置。默认 /tmp/ansible.log
deployment:
  ansible_ssh_user: root #定义哪个使用哪个系统用户安装，默认 root
  hosts: #定义 ocp master 和 node 节点列表
  - ip: 10.0.0.1 #安装需要设置
    hostname: master-private.example.com #安装需要设置
    public_ip: 24.222.0.1 #无人值守需要设置
    public_hostname: master.example.com #无人值守需要设置
    roles: # 安装服务类型指定
      - master
      - node
    containerized: true #如果设置为 true，则使用容器方式安装。 atomic 需要容器安装，并根据 /run/ostree 启动。
    connect_to: 24.222.0.1 #安装、升级、卸载系统时，测试连接 ip 地址。如果配置文件时自动生成的，这个是交互安装中第一个填入的。
  - ip: 10.0.0.2
    hostname: node1-private.example.com
    public_ip: 24.222.0.2
    public_hostname: node1.example.com
    node_labels: {'region': 'infra'}  # 每个 node 主机的标签
    roles:
      - node
    connect_to: 10.0.0.2
  - ip: 10.0.0.3
    hostname: node2-private.example.com
    public_ip: 24.222.0.3
    public_hostname: node2.example.com
    roles:
      - node
    connect_to: 10.0.0.3
  roles: #部署中定义的角色字典
    master:
      <variable_name1>: "<value1>" #任何 ansible 变量应用于分配角色变量。
      <variable_name2>: "<value2>"
    node:
      <variable_name1>: "<value1>" 
```

### 运行无人值守安装
无人值守安装允许在安装程序之前定义集群主机和配置，以便不需要经历所有交互。还允许恢复和完成未完成的交互安装。要使用无人值守安装，首先需要在 `~/.config/openshift/installer.cfg.yml` 定义安装配置文件。然后使用 `-u `表示运行安装程序。

	$ atomic-openshift-installer -u install
默认情况下，交互式和无人值守模式下，如果文件存在，安装程序将使用文件安装，如果不存在，无人值守安装失败。或者可以使用 `-c` 定义配置文件位置。

	$ atomic-openshift-installer -u -c </path/to/file> install
在无人值守安装完成后，确保备份使用的 `~/.config/openshift/installer.cfg.yml`   文件，因为如果重新安装或者添加主机或者升级集群均需要。
### 验证安装
- 安装完毕后，如果使用代理服务器，则必须将 etcd 端点和 ip 地址添加到清单文件中的 `openshift_no_proxy`  集群变量中。如果没有使用则跳过。当配置集群使用代理设置时[参考全局代理](https://docs.openshift.com/container-platform/3.6/install_config/install/advanced_install.html#advanced-install-configuring-global-proxy)，此更改会导致master服务器到 etcd 连接也被代理，而不是每个主机 `NO_PROXY` 设置的主机名。[noproxy 信息](https://docs.openshift.com/container-platform/3.6/install_config/http_proxies.html#install-config-http-proxies)。
	- 要解决这个问题，请设置每个主机的 `/etc/sysconfig/atomic-openshift-master-controllers` 文件中的 `NO_PROXY` 参数。

			NO_PROXY=https://<ip_address>:<port>
		主机将使用该 ip和端口同 etcd 集群进行。如果独立部署端口是 2379 或嵌入式部署端口是 4001。
	- 重启 master 服务以使修改生效

			# systemctl restart atomic-openshift-master
- 验证状态需要在 master 节点使用 root 在主机上执行

		# oc get nodes

		NAME                        STATUS                     AGE
		master.example.com          Ready,SchedulingDisabled   165d
		node1.example.com           Ready                      165d
		node2.example.com           Ready                      165d
- web 控制台验证，请使用 master 节点主机名和端口通过游览器访问。

		https://master.openshift.com:8443/console.
- 卸载 ocp

	可以使用程序卸载命令从集群中卸载 ocp .默认情况下，如果文件存在，安装程序使用位于 `~/.config/openshift/installer.cfg.yml` 配置文件。
	
		$ atomic-openshift-installer uninstall
或者通过 `-c` 指定

		$ atomic-openshift-installer -c </path/to/file> uninstall		

			
		

		    			 					
	
													

				 					  	