## 安装与配置-启动集群监控
集群监控收集的是每台机器上安装的 `kubelet` 公开的 metrics 数据，并把这些数据收集和存储到了后端的 `Heapster`。作为 ocp 管理员，可以在用户界面中查看所有容器和组件的监控信息。这些监控也是由 [pod 自动水平扩展](https://docs.openshift.org/3.6/dev_guide/pod_autoscaling.html#dev-guide-pod-autoscaling)的，这样保证扩展。后端存储使用的是 [Hawkular Metrics](https://github.com/hawkular/hawkular-metrics) 用 `Cassandra` 数据库进行指标存储。配置了功能后，就可以从 ocp web控制台查看 cpu、mem、基础网络的指标，还可以通过 pod 进行水平扩展(增加节点后)。 `Heapster` 从 master 服务器检索所有节点的列表，然后通过 `/stats` 端点联系每个节点。从那里， `Heapster`将采集监控指标然后导入 `Hawkular Metrics`。在 web控制台中游览各 pod 会显示内存和 cpu 单独的 sparkline 图表。显示的时间范围是可选的，这些图表每 30 秒将自动刷新一次。如果 pod 上有多个 pod，则可以选定一个特定容器来显示。如果为了自定义资源限制，那么还可以看到每个 pod 的甜甜圈图表。甜甜圈图表显示了资源限制情况。比如:总200MIB,145MIB可用，甜甜圈显示55MIB使用中。

### 监控项目(metrics project)
集群监控组件必须部署到 `openshift-infra` 项目中，以便进行自动缩放。 pod 水平自动器使用该项目来发现 `Heapster` 服务，并使用它来检测指标。可以通过向 `inventory` 中添加 `openshift_metrics_project` 来更改指标项目。

### 监控数据持久化
可以将度量数据[永久储存](https://docs.openshift.org/3.6/architecture/additional_concepts/storage.html#architecture-additional-concepts-storage)或放在一个临时的 [pod 卷](https://docs.openshift.org/3.6/dev_guide/volumes.html#dev-guide-volumes)中。
#### 持久化存储
运行 ocp 持久化监控数据存储，意味着将被存储到持久卷中，并能够重新启动和重新创建 pod 中的到数据保存。如果要求指标数据不受丢失影响。生产环境必须使用。

`Cassandra` 存储大小取决于 pod 的数量。管理员有责任确保足够的磁盘。持久卷生命的大小由 `openshift_metrics_cassandra_pvc_size` 变量设置，默认 10GB。如果要使用[动态配置持久卷](https://docs.openshift.com/container-platform/3.6/install_config/persistent_storage/dynamically_provisioning_pvs.html#install-config-persistent-storage-dynamically-provisioning-pvs)，请将 `openshift_metrics_cassandra_storage_type` 变量设置为 `dynamic`。
#### 集群监控规划
安装监控后，使用 `oc get pods -n openshift-infra` 得到

	NAME                                READY             STATUS      RESTARTS          AGE
	hawkular-cassandra-1-l5y4g          1/1               Running     0                 17h
	hawkular-metrics-1t9so              1/1               Running     0                 17h
	heapster-febru                      1/1               Running     0                 17h
ocp 使用了 `Cassandra` 数据库，该数据库部署通过 `openshift_metrics_cassandra_limits_memory` 设置内存，默认 2GB。可以根据启动脚本来修改这个值。此值涵盖了大多数的 ocp 安装，但在它部署前，还可以通过 docker 环境变量可以修改 `MAX_HEAP_SIZE` 和 `HEAP_NEWSIZE`。默认情况下，监控数据只保留7天，7天后将倒序清除旧数据。已经删除的 pod 和项目的指标数据不会自动清除，超过 7天才被删除。

- 规划例1 10个节点1000个 pod 累计数据

	测试场景包含10个节点和1000个 pod 测试场景汇总，24小时累积了 2.5GB 监控数据。因此规划公式为:
	
		(((2.5 × 10^9) ÷ 1000) ÷ 24) ÷ 10^6 = ~0.125 MB/小时每pod # 10^6表达10的6次方
- 规划例1 102个节点10000个 pod 累计数据			
	测试场景包含120个节点和10000个 pod，24小时累积了 25GB 数据，因此规划公式为:
	
		(((11.410 × 10^9) ÷ 1000) ÷ 24) ÷ 10^6 = 0.475 MB/小时每pod

	 | 1000 pod	| 10000 pod
-----|-----|-----
24 小时数据持久化(默认度量参数)| 2.5GB| 11.4GB

	默认设置 `openshift_metrics_duration` 是7天， `openshift_metrics_resolution` 是 10秒，则数据库每周存储要求为
	
	 | 1000 pod	| 10000 pod
-----|-----|-----
7天数据持久化(默认度量参数)| 20GB| 90GB
在上表中，添加了 10%的额外存储空间防止额外数量的 pod 缓冲。如果数据库持久化卷空间不足，则会发生数据丢失。使用集群持久化存储确保 `ReadWriteOnce` 模式访问。如果未激活，则导致数据库无法启动。请确保足够大的持久化卷可用，持久化卷声明是有 `OpenShift Ansible openshift_metrics` 角色处理。 ocp 指标还支持动态配置持久化卷。要将此功能与 ocp 指标配合使用，必须将 `openshift_metrics_cassandra_storage_type` 设置为 `dynamic`。可以使用云端的动态持久化卷，比如 EBS。

- 数据库存储要求，基于节点/pod数量

	节点数量 | pod数量	| 数据库存储增长 | 数据库每天存储增长 | 数据库每周存储增长
-----|-----|-----|-----|-----
210| 10500| 500MB/小时|15GB |75GB
990| 11000| 1GB/小时|30GB |210GB
上述计算，给出的额外空间是20%，以确保不会超过计算值。如果 `METRICS_DURATION`和 `METRICS_RESOLUTION`的值保持默认。分别是(7天和15秒)，则可以安全的参考上表的计算1周中数据库存储大小的要求。因为 ocp 使用 `Cassandra` 作为数据库存储，如果设置了 `USE_PERSISTANT_STORAGE=true ` ，则 PV 将位于网络存储顶部，NFS 是默认值。但 `Cassandra` 不推荐网络存储。

### ocp3.5 建议
在专用的基础设施中部署监控单元。配置使用持久化存储。设置 `USE_PERSISTENT_STORAGE=true`。ocp 指标部署保留了 ` METRICS_RESOLUTION=30` 参数，不推荐这个参数调节小于30.安装时使用 `openshift_metrics_resolution` 进行配置。密切监控安装监控的节点资源信息(cpu/mem)，如果缺少可能会导致 pod 数据丢失。ocp 3.5 版本测试中监控节点到 25000个 pod
### 已知限制和问题
测试发现 `heapster metrics` 组件最多处理 25000个 pod，如果超过这个数量，导致 `Heapster` 处理下降，导致绘图异常的可能性。
### 不持久化存储
运行非持久化存储的监控会意味着删除 pod后，任何数据将会丢失。但是非持久化运行会是的监控服务会容易很多，但重启容器，监控数据则不会丢失。为了使用非持久化， `inventory` 中 `openshift_metrics_cassandra_storage_type` 必须设置为 `emptydir`。当使用非持久化时，数据将被写到 pod 节点上的 `/var/lib/origin/openshift.local.volumes/pods`。请确保 `/var` 下有足够空间。
### 监控可选角色
OpenShift Ansible `openshift_metrics` 角色是可选的，可以通过配置 `inventory` 文件来设置。
### 可选变量
OpenShift Ansible 附带了 `openshift_metrics` 角色定义了部署监控的任务。以下是参数列表。

- openshift_metrics_install_metrics

	如果是 `true` 则部署监控
- openshift_metrics_start_cluster

	部署组件后启动
- openshift_metrics_image_prefix

	容器组件前缀，使用 `openshift3/ose-metrics-cassandra:v3.6`前缀是 `openshift3/ose-`
- openshift_metrics_image_version

	容器组件版本，例如 ` openshift3/ose-metrics-cassandra:v3.6.173.0.21` 设置 `v3.6.173.0.21`或者设置 `v3.6` 来保证获取最新的3.6镜像。
- openshift_metrics_startup_timeout

	启动系统超时时间，判断 `Hawkular Metrics`和`Heapster`启动失败而重启的等待时间。单位是秒
- openshift_metrics_duration

	保留数据天数
- openshift_metrics_resolution

	收集指标频率，定义数字和单位标示符 `(s)(m)(h).`
- openshift_metrics_cassandra_pvc_prefix

	为 `Cassandra` 创建持久卷声明前缀，序列号从1开始附加到前缀。
- openshift_metrics_cassandra_pvc_size

	每个 `Cassandra` 节点持久化卷设置大小
- openshift_metrics_cassandra_storage_type

	使用 `emptydir` 非持久化(测试使用)，`pv` 设置是持久化，需要安装前创建或使用动态持久化
- openshift_metrics_cassandra_replicas

	表示 `Cassandra` 节点数量，使用 `replication controllers` 控制节点数
- openshift_metrics_cassandra_limits_memory

	`Cassandra` pod 内存限制，比如限制 `2Gi` 就是限制了 2GB内存。
- openshift_metrics_cassandra_limits_cpu
		 								
	`Cassandra` pod cpu 限制，例如 `4000m` 限制使用4核cpu
- openshift_metrics_cassandra_requests_memory

	`Cassandra` pod 内存申请，比如限制 `2Gi` 就是申请了 2GB内存。
- openshift_metrics_cassandra_requests_cpu
	
	`Cassandra` pod cpu申请，例如 `4000m` 申请使用4核cpu
- openshift_metrics_cassandra_storage_group

	用于 `Cassandra` 设置存储组
- openshift_metrics_cassandra_nodeselector

	部署 `Cassandra` 节点选择器，保证放到特殊标签，比如 ` {"region":"infra"}`
- openshift_metrics_hawkular_ca

	Hawkular 的 ca 证书
- openshift_metrics_hawkular_cert

	Hawkular 的 cert 证书				
- openshift_metrics_hawkular_key

	Hawkular 的证书 key
- openshift_metrics_hawkular_limits_memory

	描述类似 `Cassandra ` 相同参数
- openshift_metrics_hawkular_limits_cpu

	描述类似 `Cassandra ` 相同参数
- openshift_metrics_hawkular_replicas

	描述类似 `Cassandra ` 相同参数
- openshift_metrics_hawkular_requests_memory

	描述类似 `Cassandra ` 相同参数
- openshift_metrics_hawkular_requests_cpu

	描述类似 `Cassandra ` 相同参数
- openshift_metrics_hawkular_nodeselector

	描述类似 `Cassandra ` 相同参数
- openshift_metrics_heapster_allowed_users

	逗号分隔的列表，接收 CN。默认允许设置 openshift 服务代理连接。添加 `system:master-proxy` 覆盖到列表中，可以让自动缩放功能正常运作。
- openshift_metrics_heapster_limits_memory

	描述类似 `Cassandra ` 相同参数
- openshift_metrics_heapster_limits_memory
	
	描述类似 `Cassandra ` 相同参数
- openshift_metrics_heapster_requests_memory

	描述类似 `Cassandra ` 相同参数
- openshift_metrics_heapster_requests_cpu

	描述类似 `Cassandra ` 相同参数
- openshift_metrics_heapster_standalone

	只部署 `Heapster`，不部署 `Hawkular Metrics` 和 `Cassandra`
- openshift_metrics_heapster_nodeselector

	描述类似 `Cassandra ` 相同参数
- openshift_metrics_install_hawkular_agent

	设置为 `true` 安装 `Hawkular OpenShift Agent (HOSA)`，HOSA 收集 pod 自定义指标。预览版功能，默认不安装。
	
关于请求和限制，可以参考 [Compute Resources](https://docs.openshift.com/container-platform/3.6/dev_guide/compute_resources.html#dev-compute-resources)

唯一需要变更的参数是 `openshift_metrics_hawkular_hostname` 执行 `openshift_metrics Ansible` 角色的时候需要它，因为 `Hawkular Metrics`使用路由的名字。此值对应的是一个完整的域名。配置控制台进行变量访问时，必须指导它。

如果使用 `Cassandra` 持久化存储，管理员有责任使用 `openshift_metrics_cassandra_pvc_size` 分配足够的磁盘空间，并运维这个大小。如果磁盘空间不足会导致数据丢失。其他可选变量允许定制更多。例如: 如果自定义安装，则 k8s主机在 `https://kubernetes.default.svc:443` 下不可用，则可以指定 `openshift_metrics_master_url` 参数指定。镜像版本使用特定的，也可以修改 `openshift_metrics_image_version` 来调节。强烈建议不使用 `openshift_metrics_image_version` 参数调节版本，版本应该对应最新的。

### 使用加密
`OpenShift Ansible openshift_metrics` 可以自生成签名证书以及其组件之间的使用，并将生成重新加密的路由公开 `Hawkular Metrics` 服务，该路由允许 web 控制台访问 `Hawkular Metrics`服务。 为了使运行的 web 控制台的游览器信任通过此路有，它必须信任该路由证书。这可以通过提供可信赖的证书来实现。 `openshift_metrics` 角色允许指定自己的证书，然后创建路由时使用它。如果不提供将使用默认证书。
### 提供自己的证书					
要提供自己重新加密的路由证书，可以在 `inventory` 中设定 `openshift_metrics_hawkular_cert` `openshift_metrics_hawkular_key` `and openshift_metrics_hawkular_ca` 三个变量，`hawkular-metrics.pem` 值需要包含 `pem` 格式证书。还可能需要 `hawkular-metrics-ca.cert` 密码为证实办法机构提供前名证书。
### 部署监控组件
因为部署和配置所有监控组件使用的是 `OpenShift Ansible` 处理，所以可以在一个步骤中完成部署。下面例子中说明如何使用默认参数部署带有或不带有持久化的监控系统。

- 持久化部署

	例子将 ` Hawkular Metrics` 路由设置为使用 `hawkular-metrics.example.com` 作为持久化部署。注意必须有足够大的持久化空间
	
		$ ansible-playbook <OPENSHIFT_ANSIBLE_DIR>/byo/openshift-cluster/openshift-metrics.yml \
		-e openshift_metrics_install_metrics=True \
		-e openshift_metrics_hawkular_hostname=hawkular-metrics.example.com \
		-e openshift_metrics_cassandra_storage_type=pv
- 非持久化部署(非持久化可能会导致数据丢失)

		$ ansible-playbook <OPENSHIFT_ANSIBLE_DIR>/byo/openshift-cluster/openshift-metrics.yml \
		-e openshift_metrics_install_metrics=True \
		-e openshift_metrics_hawkular_hostname=hawkular-metrics.example.com

### 部署 `Hawkular OpenShift Agent`	
`Hawkular OpenShift Agent` 目前是技术预览版，它可以从 ocp 集群收集 pod 的应用信息，然后在从控制台看到或者从 `Hawkular Metrics REST API` 获取这些指标。 为了收集这些指标，建议 pod 公开一个 `Prometheus` 或者 `Jolokia` 端点，并创建一个特殊的 `ConfigMap`,它定义了监控所在的端点位置和如何收集监控信息。更多参考 [Hawkular OpenShift Agent](https://github.com/hawkular/hawkular-openshift-agent)。agent 作为在集群中设置的守护进程运行并部署在 `default project` 中。部署在 `default project` 中，即使启动了 `ovs_multitenant`，也可以继续监听所有的 pod。部署代理要收集2个配置。

	$ wget https://github.com/openshift/origin-metrics/blob/enterprise/hawkular-openshift-agent/hawkular-openshift-agent-configmap.yaml
	$ wget https://github.com/openshift/origin-metrics/blob/enterprise/hawkular-openshift-agent/hawkular-openshift-agent.yaml
将 agent 部署到 ocp 中，需要运行:

	$ oc create -f hawkular-openshift-agent-configmap.yaml -n default
	$ oc process -f hawkular-openshift-agent.yaml | oc create -n default -f -
	$ oc adm policy add-cluster-role-to-user hawkular-openshift-agent system:serviceaccount:default:hawkular-openshift-agent
### 卸载 `Hawkular OpenShift Agent` 部署
取消部署，请运行

	$ oc delete all,secrets,sa,templates,configmaps,daemonsets,clusterroles --selector=metrics-infra=agent -n default
### 设置监控的公共 URL
 ocp web 控制台使用来自 `Hawkular Metrics` 服务的数据显示图形。在 master 配置文件 `/etc/origin/master/master-config.yaml` 中必须设置访问 `Hawkular Metrics` 服务的 URL。安装期间通过在 `inventory` 中设置 `openshift_metrics_hawkular_hostname`创建路径。web控制台必须可以解析 `openshift_metrics_hawkular_hostname`。如果在 `openshift_metrics_hawkular_hostname` 对应的设置是 `hawkular-metrics.example.com`，则 `master-config.yaml` 文件如下:
 
	assetConfig:
    	...
    	metricsPublicURL: "https://hawkular-metrics.example.com/hawkular/metrics"
 一旦设置完毕保存，就必须重启 ocp 实例。如果使用自签名的证书，请记住 `Hawkular Metrics` 服务发布在不同的主机名下面，并使用与控制台不同的证书。可以打开游览器选项到 `metricsPublicURL` 指定证书。为了避免这个问题，请使用认证证书。
### 直接访问 `Hawkular Metrics`			
要直接访问和管理指标，可以通过 `Hawkular Metrics API`。从 API 访问 `Hawkular Metrics`时，只能做读操作。默认情况下，写入指标是被禁止的。如果个别需求需要改，需要修改 `openshift_metrics_hawkular_user_write_access` 为 `true`。但是，建议使用默认配置，只有 `Heapster` 才可以写入。如果启动写入访问，任何用户的写入，都可能会影响性能，导致监控系统异常。 [Hawkular Metrics](http://www.hawkular.org/docs/rest/rest-metrics.html)文档介绍了如何使用 API，但是处理配置 ocp 使用 `Hawkular Metrics` 版本可能有些差异。 

- ocp 项目和租户

	`Hawkular Metrics` 是一个多租户应用，它被配置为 ocp 的项目对应 `Hawkular Metrics` 中的租户。因此当用户访问 `MyProject` 的项目指标时，就必须包含租户信息，如 [Hawkular-Tenant](http://www.hawkular.org/docs/rest/rest-metrics.html#_tenant_header) 设置为 `MyProject`.有一个特殊的名为 ` _system` 的租户，其中包含系统级别的监控数据。这样读取需要 ` cluster-reader` 或 `cluster-admin` 才有权限访问。
- 授权

	`Hawkular Metrics`服务将根据 ocp 对用户进行身份验证，以确定用户是否可以访问。`Hawkular Metrics` 接收来自客户端继承的令牌，并使用 `SubjectAccessReview ` 验证 ocp 令牌。如果用户对项目具有适当的读取权限，则可以读取改项目。对于 `_system` 租户，请求改租户读取的用户必须具有 `cluster-reader` 权限。

### 扩展 ocp pod 监控
参考 [缩放和性能指南](https://docs.openshift.com/container-platform/3.6/scaling_performance/scaling_cluster_metrics.html#cluster-metrics-scaling-openshift-metrics-pods)
### 清理
可以通过执行以下步骤删除 `openshift_metrics` 角色的所有容器

	$ ansible-playbook <OPENSHIFT_ANSIBLE_DIR>/byo/openshift-cluster/openshift-metrics.yml \
		-e openshift_metrics_install_metrics=False