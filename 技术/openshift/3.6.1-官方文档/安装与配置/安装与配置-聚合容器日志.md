## 安装与配置-聚合容器日志
作为 ocp 管理员，可以通过部署 EFK 来聚合 ocp 中的服务日志。应用人员应该可以查看它们具有访问权限的项目日志。 EFK 聚合来自主机和应用日志，无论来自多少容器，甚至删除的容器。EFK 组件包含:

- Elasticsearch (ES)

	日志对象存储
- Fluentd

	日志采集服务
- Kibana

	ES 的 web ui 展现服务
	
一旦部署在集群中，将会收集所有节点和服务日志聚合到 ES 中，并提供一个 kibana 的页面查看人和日志。管理员可以查看所有日志，而开发人员只能查看它们项目的日志。这些服务都是安全通讯。

管理 docker 日志讨论使用 json 驱动来管理日志，但聚合日志仅支持 journald 驱动。[更多 Fluentd](https://docs.openshift.com/container-platform/3.6/install_config/aggregate_logging.html#fluentd-upgrade-source)

### 部署前准备
1. 可以使用 playbook 来部署和升级日志系统。高级安装提供了这部分部署配置。配置参数需要在 `inventory` 中进行修改。
- 查看[大小规则](https://docs.openshift.com/container-platform/3.6/install_config/aggregate_logging_sizing.html#install-config-aggregate-logging-sizing)以确定如何最佳的部署。	
- 确保已经部署的集群路由器
- 确保具有 ES 所需的存储空间。注意，每个ES 都需要自己的存储卷。[更多信息](https://docs.openshift.com/container-platform/3.6/install_config/aggregate_logging.html#aggregated-elasticsearch)
- 选择一个项目，部署后，EFK 会收集所有日志。如果要指定节点选择器，则只需要创建一个项目。本章使用 默认项目为 `logging`。 否则，`openshift-logging` 这个角色将创建这个项目。

		$ oadm new-project logging --node-selector=""
		$ oc project logging
建议在项目中指定一个空[节点选择器](https://docs.openshift.com/container-platform/3.6/admin_guide/managing_projects.html#using-node-selectors)，因为 Fluentd 应该在每个节点上部署，任何选择器都将其限制其部署位置。为了控制组件部署，指定要应用于其部署配置的配个节点的节点选择器。

### 配置日志方案变量
可以在 `inventory` 文件中指定 EFK 参数来设置。默认情况下， ES 服务使用的端口是9300用于做节点之间的 tcp 通讯。

- openshift_logging_image_prefix

	用于日志组件前缀，例如 `registry.access.redhat.com/openshift3/`是 registry.access.redhat.com/openshift3/logging-fluentd:latest.`前缀
- openshift_logging_image_version

	日志镜像的版本，例如 `v3.6` 是 `registry.access.redhat.com/openshift3/logging-fluentd:v3.6.` 的版本
- openshift_logging_use_ops

	如果设置为 `true`,将会配置一个第二个日志集群(ES和Kibana)来用于独立收集操作日志。 `Fluentd` 在日志收集方面会拆分成主集群和操作集群，操作日志包括所有节点的 `/var/log/messages` 和以下项目的日志(default, openshift, openshift-infra)，意味着部署了第二个日志集群可以通过包含 `-ops` 前缀来区分。
- openshift_logging_master_url

	Kubernetes master URL，这个不需公开，而仅在集群内部访问。
- openshift_logging_master_public_url

	Kubernetes master 公开的URL，这个由 kibana proxy 进行身份重定向。
- openshift_logging_namespace

	日志系统的命名空间
- openshift_logging_install_logging

	设置为 `true` 安装日志，设置为 `false` 卸载日志
- openshift_logging_image_pull_secret

	指定用于从已经通过的身份认证的 registry 拉密钥名称。
- openshift_logging_curator_default_days

	监护用户日志保留最肖时间，默认单位是天
- openshift_logging_curator_run_hour

	当天运行几个小时
- openshift_logging_curator_run_minute

	每时运行几分钟
- openshift_logging_curator_run_timezone

	运行的时区
- openshift_logging_curator_script_log_level

	脚本日志等级
- openshift_logging_curator_log_level

	日志等级
- openshift_logging_curator_cpu_limit

	限制使用的 cpu
- openshift_logging_curator_memory_limit

	限制使用的内存
- openshift_logging_curator_nodeselector

	部署运行的节点选择器
- openshift_logging_curator_ops_cpu_limit

	ops 日志集群的 cpu 限制，仅在 `openshift_logging_use_ops` 参数打开生效
- 	openshift_logging_curator_ops_memory_limit

	ops 日志集群的内存限制 ，仅在 `openshift_logging_use_ops` 参数打开生效
- openshift_logging_kibana_hostname

	web 客户端到达 kibana 外部主机名
- openshift_logging_kibana_cpu_limit

	kibana 限制的cpu
- openshift_logging_kibana_memory_limit

	kibana 限制的内存
- openshift_logging_kibana_proxy_debug

	如果参数为 `true`,则 kibana proxy 日志等级为 debug
- openshift_logging_kibana_proxy_cpu_limit

	kibana proxy cpu 限制
- openshift_logging_kibana_proxy_memory_limit

	kibana proxy 内存限制	
- openshift_logging_kibana_replica_count

	kibana 副本数量
- openshift_logging_kibana_nodeselector

	kibana 部署节点选择器
- openshift_logging_kibana_key

	公用 key,在创建 kibana 路由时使用
- openshift_logging_kibana_cert

	匹配上面key的证书，在创建 kibana 路由时使用
- openshift_logging_kibana_ca

	可选，ca 证书,在创建 kibana 路由时使用
- openshift_logging_kibana_ops_hostname

	等效于 `openshift_logging_kibana_hostname`，仅在 `openshift_logging_use_ops` 参数打开生效	
- openshift_logging_kibana_ops_cpu_limit

	等效于 `openshift_logging_kibana_cpu_limit`，仅在 `openshift_logging_use_ops` 参数打开生效	
- openshift_logging_kibana_ops_memory_limit

	等效于 `openshift_logging_kibana_memory_limit`，仅在 `openshift_logging_use_ops` 参数打开生效
- openshift_logging_kibana_ops_proxy_debug

	等效于 `openshift_logging_kibana_proxy_debug`，仅在 `openshift_logging_use_ops` 参数打开生效
- openshift_logging_kibana_ops_proxy_cpu_limit

	等效于 `openshift_logging_kibana_proxy_cpu_limit`，仅在 `openshift_logging_use_ops` 参数打开生效
- openshift_logging_kibana_ops_proxy_memory_limit

	等效于 `openshift_logging_kibana_proxy_memory_limit`，仅在 `openshift_logging_use_ops` 参数打开生效
- openshift_logging_kibana_ops_replica_count	
	等效于 `openshift_logging_kibana_replica_count`，仅在 `openshift_logging_use_ops` 参数打开生效
- openshift_logging_fluentd_nodeselector

	用于部署 fluentd 实例的节点选择器。 fluentd 运行的节点必须符合此标签， fluentd 才能收集日志。安装聚合集群时，`openshift_logging` 的角色将标签的节点通过 `openshift_logging_fluentd_hosts` 为提供选择器。作为安装的一部分，建议将 fluentd节点标签添加到持久化[节点标签](https://docs.openshift.com/container-platform/3.6/install_config/install/advanced_install.html#configuring-node-host-labels)列表中。
- openshift_logging_fluentd_cpu_limit

	cpu 限制
- openshift_logging_fluentd_memory_limit	
	内存限制
- openshift_logging_fluentd_use_journal

	必须设定为 `true` 确定从 `journal` 采集日志,空白将由 fluentd 确定当前的日志驱动。
- openshift_logging_fluentd_journal_read_from_head

	如果设定为 `true`, fluentd 第一次启动的时候就会重头从 journal 收集日志，可能导致 es 接收当前日志延迟。
- openshift_logging_fluentd_hosts

	fluentd 要部署节点的列表
- openshift_logging_es_host

	fluentd 发送到 es 服务的主机名
- openshift_logging_es_port

	fluentd 发送到 es 服务的端口
- openshift_logging_es_ca

	ca 证书，用于与 `openshift_logging_es_host` 建立通讯的
- openshift_logging_es_client_cert

	客户端证书，用于与 `openshift_logging_es_host` 建立通讯的
- openshift_logging_es_client_key

	客户端 key，用于与 `openshift_logging_es_host` 建立通讯的
- openshift_logging_es_cluster_size

	es 副本数，高可用3个以上
- openshift_logging_es_cpu_limit

	限制 cpu
- openshift_logging_es_memory_limit

	限制内存
- openshift_logging_es_number_of_replicas			
	es 发布的副本数量
- openshift_logging_es_number_of_shards

	每个新索引的分片数量，默认是 1
- openshift_logging_es_pv_selector

	选择具体的 pv，添加到 pvc 的键值对
- openshift_logging_es_pvc_dynamic

	如果是可用集群，设置为 `true`,使 pvc 动态设置声明注释后备存储
- openshift_logging_es_pvc_size

	每个 es 创建持久卷大小，例如 100G，省略则不会创建 pvc 而是临时卷
- openshift_logging_es_pvc_prefix

	es 实例存储持久卷声明的前缀。每个实例都附加一个数字，例如 `logging-es-1`，如果不存在，则大小由 `es-pvc-size` 创建
- openshift_logging_es_recover_after_time

	es 尝试恢复前等待的时间
- openshift_logging_es_storage_group

	访问 es 存储卷的持久化组 id编号，备份卷应该允许访问此组 id
- openshift_logging_es_nodeselector

	部署 es 的节点选择器，可以用于设定保留和优化的节点上。例如 ` {"node-type":"infrastructure"}` ，在 es 部署之前，必须有一个节点是活的
- openshift_logging_es_ops_allow_cluster_reader

	设置为 true，允许 `cluster_reader`	进行读操作
- openshift_logging_es_ops_host

	等效于 `openshift_logging_es_host`，仅在 `openshift_logging_use_ops` 参数打开生效
- openshift_logging_es_ops_port

	等效于 ` openshift_logging_es_port`，仅在 `openshift_logging_use_ops` 参数打开生效
- openshift_logging_es_ops_ca

	等效于 `openshift_logging_es_ca`，仅在 `openshift_logging_use_ops` 参数打开生效
- openshift_logging_es_ops_client_cert

	等效于 `openshift_logging_es_client_cert`，仅在 `openshift_logging_use_ops` 参数打开生效
- openshift_logging_es_ops_client_key

	等效于 ` openshift_logging_es_client_key`，仅在 `openshift_logging_use_ops` 参数打开生效
- openshift_logging_es_ops_cluster_size

	等效于 `openshift_logging_es_cluster_size`，仅在 `openshift_logging_use_ops` 参数打开生效
- openshift_logging_es_ops_cpu_limit

	等效于 `openshift_logging_es_cpu_limit`，仅在 `openshift_logging_use_ops` 参数打开生效
- openshift_logging_es_ops_memory_limit

	等效于 `openshift_logging_es_memory_limit`，仅在 `openshift_logging_use_ops` 参数打开生效
- openshift_logging_es_ops_pv_selector

	等效于 `openshift_logging_es_pv_selector`，仅在 `openshift_logging_use_ops` 参数打开生效
- openshift_logging_es_ops_pvc_dynamic

	等效于 `openshift_logging_es_pvc_dynamic`，仅在 `openshift_logging_use_ops` 参数打开生效
- openshift_logging_es_ops_pvc_size

	等效于 `openshift_logging_es_pvc_size`，仅在 `openshift_logging_use_ops` 参数打开生效
- openshift_logging_es_ops_pvc_prefix

	等效于 `openshift_logging_es_pvc_prefix`，仅在 `openshift_logging_use_ops` 参数打开生效
- openshift_logging_es_ops_recover_after_time

	等效于 `openshift_logging_es_recovery_after_time`，仅在 `openshift_logging_use_ops` 参数打开生效
- openshift_logging_es_ops_storage_group

	等效于 `openshift_logging_es_storage_group`，仅在 `openshift_logging_use_ops` 参数打开生效
- openshift_logging_es_ops_nodeselector

	节点选择器，部署 ops es 实例		，例子 `node-type=infrastructure`
- openshift_logging_kibana_ops_nodeselector

	节点选择器，部署 ops kibana 实例
- openshift_logging_curator_ops_nodeselector	
	节点选择器，部署 ops curator 实例	

### 自定义证书
可以使用以下证书设置添加到 `inventory` 中，而不依赖于部署生成的那些。这些证书用于保护用户游览器与 kibana 之间的通讯。不提供则会自动生成。

- openshift_logging_kibana_cert

	证书
- openshift_logging_kibana_key

	key
- openshift_logging_kibana_ca

	ca 证书
- openshift_logging_kibana_ops_cert

	证书
- openshift_logging_kibana_ops_key

 	key
- openshift_logging_kibana_ops_ca		

	ca证书

### 部署 EFK 
使用 `iventory` 文件部署

	$ ansible-playbook [-i </path/to/inventory>] \
    /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/openshift-logging.yml
部署包括Secrets, ServiceAccounts, DeploymentConfigs。playbook 会等待部署直到服务开始运行，如果等待失败，部署仍然成功。因为它可能从 registry 检索镜像，需要通过以下命令来观察

	$ oc get pods -w
最终进入运行状态，可以通过命令显示详细信息

	$ oc describe pods/<pod_name>
检查日志

	$ oc logs -f <pod_name>
### 了解调整部署
默认的 ` default, openshift, openshift-infra` 会自动聚合，并在 kibana 中分组到 `.operations` 的项目。部署的日志系统并没有聚合日志，操作并找到id。如果在 `inventory` 中将 `openshift_logging_use_ops` 设置为 `true`,则 fluentd 为主 es 集群和操作日志导入到另外一个集群。
### ES 集群
高可用部署最少需要 3个 ES 集群，每个载不同主机上。ES需要自己的存储，但 ocp 配置载所有 pod 之间共享存储卷。因此在扩展时，EFK 部署要确保每个副本都要部署配置。通过修改 `inventory` 中的 `openshift_logging_es_cluser_size` 并重新运行 playbook，可以在创建后扩展集群。可以修改参数。阅读[ES 文档](https://www.elastic.co/guide/en/elasticsearch/guide/current/hardware.html)来了解存储和网络。	

- 查看部署

		$ oc get dc --selector logging-infra=elasticsearch
- 节点选择器

	由于 ES 使用大量资源，所以集群所有成员均应该有低延迟网络连接网络存储。通过使用节点选择器将实例指向专用的节点或集群中的专用区域来确保。要配置节点选择器，请在 inventory 中指定 `openshift_logging_es_nodeselector`,这适用于所有 ES 部署。如果需要个性化选择器，则必须在部署后手动配置。节点选择器被指定为 python 兼容的字典。例如 `{"node-type":"infra", "region":"east"}.`

### 持久化存储
默认情况下，ansible 的角色 `openshift_logging` 会创建的是一个临时存储部署，其所有的 pod 数据重启后都会消失。对于生产，会为每个 es 部署配置指定持久卷。可以在部署前创建必要的持久卷声明和创建它。必须命名 PVC 才能匹配 `openshift_logging_es_pvc_prefix` 设置，默认为 `logging-es-`，每个 PVC 将添加一个序列号，如`logging-es-1、logging-es-2`。如果已经存在PVC，则直接使用。如果没有，并制定了 `openshift_logging_es_pvc_size`，则会使用该大小的请求创建它。 


使用 NFS 最为持久卷(或通过诸如 Gluster 的 NAS) 都不支持 ES 存储，因为 Lucene 依赖于 NFS 不提供的文件系统。

- NFS 文件卷方法

	如果需要 NFS ，则可以在卷上分配一个大文件作为存储，并将其安装在一个主机上，例如 NFS存储卷安装在 ` /nfs/storage`

		$ truncate -s 1T /nfs/storage/elasticsearch-1
		$ mkfs.xfs /nfs/storage/elasticsearch-1
		$ mount -o loop /nfs/storage/elasticsearch-1 /usr/local/es-storage
		$ chown 1000:1000 /usr/local/es-storage
	然后，使用 `usr/local/es-storage` 作为主机安装。使用不同的后备文件作为每个es的副本存储。该环路必须在节点上的 ocp 之外手动维护。
- 本地磁盘卷

	可以将每个节点主机的本地磁盘卷用于做 ES 存储。需要如下准备
	
	1. 必须向服务账户授权装载和编辑本地卷权限。

			$ oc adm policy add-scc-to-user privileged  \
				system:serviceaccount:logging:aggregated-logging-elasticsearch #使用之前创建的项目运行 playbook ,如 logging
	- 每个 es 副本定义都必须补充声明权限

			$ for dc in $(oc get deploymentconfig --selector logging-infra=elasticsearch -o name); do
    			oc scale $dc --replicas=0
    			oc patch $dc \
					-p '{"spec":{"template":{"spec":{"containers":[{"name":"elasticsearch","securityContext":{"privileged": true}}]}}}}'
			done			
	- ES 副本必须位于正确的节点上才能使用本地存储，即使节点关闭也不会被迁移。这需要给每个 ES 副本一个节点选择器，该节点选择器对于管理员分配了存储的节点是唯一的。要配置节点选择器，编辑每个 ES 部署配置和添加或编辑节点选择器部分指定每个所属的节点应用唯一标签

			apiVersion: v1
			kind: DeploymentConfig
			spec:
				template:
					spec:
						nodeSelector:
							logging-es-node: "1" #此标签使用低啊有该标签的单个节点唯一标记副本，在本例中，为  logging-es-node=1。使用 oc label 命令根据需要将标签应用于节点。
- 自动应用节点选择器，可以使用命令 `oc patch`

		$ oc patch dc/logging-es-<suffix> \
			-p '{"spec":{"template":{"spec":{"nodeSelector":{"logging-es-node":"1"}}}}}'
- 设定完以上步骤就可以在本地主机部署副本，将持久化卷挂载，如本例中

		$ for dc in $(oc get deploymentconfig --selector logging-infra=elasticsearch -o name); do
			oc set volume $dc \
          	--add --overwrite --name=elasticsearch-storage \
          	--type=hostPath --path=/usr/local/es-storage
			oc rollout latest $dc
			oc scale $dc --replicas=1
		done				

### 扩展 ES 大小							
如果需要扩展 ES 集群实例数量，它不像扩展一个 ES 集群部署配置(dc)一样简单。这是由于持久化卷的性质以及 ES 如何配置来存储和恢复其数据。扩展需要每个 ES 集群节点创建 dc。 

更改规模的简单办法是修改 `inventory`，重新运行日志的 playbook。假设之前部署的 es 提供了持久存储，这个操作不应该是破坏性的。

如果不想重装，例如因为已经进行了保留的自定义，那么可能需要使用一个模版添加一个新的 ES dc 到集群中，这需要一个更复杂的过程。
### 允许 `cluster-reader` 查看操作日志
默认情况下，只有 `cluster-admin` 用户在 ES 和 Kibana 中被授予权访问权限才能查看操作日志。要 `cluster-reader` 用户也可以查询这些日志，请将 `Elasticsearch configmap`的 `openshift.operations.allow_cluster_reader` 设置为 `true`

	$ oc edit configmap/logging-elasticsearch
注意，在重新部署 pod 之后，可能不会显示对 `configmap` 的更改。通过在 `inventry` 中设置 `openshift_logging_es_allows_cluster_reader` 设置为 `true` 实现跨持久化部署。
### Fluentd
Fluentd 部署为 DaemonSet,根据节点标签选择器部署副本，可以使用 `inventory` `openshift_logging_fluentd_nodeselector`指定副本选择器，默认 `logging-infra-fluentd`。作为 ocp 一部分，建议将 Fluentd 节点选择器添加到持久化节点标签列表中。

- 使用 systemd journal 作为数据源

	默认情况下， Fluentd 分别从 `/var/log/messages` 和 `/var/log/containers/<container>.log ` 读取系统日志和容器日志。可以使用 systemd 日志作为数据源。有三个 `inventory` 参数参考
	
	- openshift_logging_use_journal

		默认设置为空，配置 Fluentd 检查 Docker 当前的日志驱动。如果使用 `--log-driver=journald`，Fluentd 从系统日志读取。设置 `openshift_logging_use_journal` 要明确日志驱动。 docker 必须使用 `--log-driver=journald` 才支持聚合。
		
		强烈建议使用默认值，升级 ocp 更改 docker 日志驱动情况下，如果明确指定 `false`，fluentd 依然读取 json 日志驱动和文件。这导致可能会丢失日志。
	- openshift_logging_journal_read_from_head

		如果设置为 `false`,Fluentd 将从尾部开始收集日志。如果使用 `true`,可能延迟几分钟到几小时才能收到日志，取决于日志大小。
		
		ocp 3.3 开始，fluentd 使用 josn 文件日志驱动时就不再读取历史日志文件。在集群具有大量日志文件并且比 EFK 部署更早的情况下，这样可以避免推送到 ES 的日志发生延迟。	
		在使用 `openshift_logging_journal_read_from_head=true` 时，可能会延迟几分钟或几个小时，取决于日志大小。
				
#### Fluentd 发送日志到另一个 ES
使用 `ES_COPY` 已经被弃用。要配置将其日志副本发送到另一个 ES，请改用 [Fluentd Secure Forward](https://docs.openshift.com/container-platform/3.6/install_config/aggregate_logging.html#fluentd-external-log-aggregator)。		
可以配置 fluentd 将每个日志信息的副本发送到 ocp 聚合日志 es 和一个外部的 es 实例。例如已经部署了一套 es 来做审计，可以将每个日志信息的副本发送到该 es。

该功能通过 fluentd 的环境变量进行控制，可以按照以下说明进行修改。如果环境变量 `ES_COPY` 设置为 `true`， fluentd 将会把日志副本发送到另一个 es。复制变量名称就像当前的 `ES_HOST` 一样， `OPS_HOST` 和其他变量，除了它们添加 `_COPY:ES_COPY_HOST`、`OPS_COPY_HOST`等等，还添加了一些附加参数:

- `ES_COPY_SCHEME`, `OPS_COPY_SCHEME` 可以使用 http/https，默认是 https
- `ES_COPY_USERNAME`, `OPS_COPY_USERNAME` 用于向 es 进行身份验证的用户名
- `ES_COPY_PASSWORD`, `OPS_COPY_PASSWORD` 用于向 es 进行身份验证的密码

直接将日志发送到 aws es 实例将不支持，使用 `Fluentd Secure Forward` 将日志引导到可控的 fluentd 实例，并配置 `fluent-plugin-aws-elasticsearch-service plug-in` 插件。

设置参数：

- 编辑 fluentd 的 DaemonSet

		$ oc edit -n logging ds logging-fluentd

	添加或编辑环境变量 `ES_COPY` 以使其值为 `true`(带引号)，并添加活着编辑上面列出的 `COPY` 变量。这些更改将不会在多次运行 logging playbook 中持久化。每次都需要编辑 `DaemonSet` 来设置。
	
#### 设置 fluentd 将日志发送到外部的日志聚合器
可以将日志副本发送到外部的日志聚合器，而不是默认的 es,使用 `secure-forward` 插件，可以在本地托管的 fluentd 处理之后进一步处理日志。日志记录部署在 `Fluentd configmap` 中提供了一个用于配置外部聚合器的 `secure-forward.conf`

	<store>
	@type secure_forward
	self_hostname pod-${HOSTNAME}
	shared_key thisisasharedkey
	secure yes
	enable_strict_verification yes
	ca_cert_path /etc/fluent/keys/your_ca_cert
	ca_private_key_path /etc/fluent/keys/your_private_key
	ca_private_key_passphrase passphrase
	<server>
		host ose1.example.com
		port 24284
	</server>
	<server>
		host ose2.example.com
		port 24284
		standby
	</server>
	<server>
		host ose3.example.com
		port 24284
		standby
	</server>
	</store>		
这可以使用 oc 编辑命令更新

	$ oc edit configmap/logging-fluentd
要在 ` secure-forward.conf` 中使用的证书可以添加到安装在 fluentd pod 上的 `secret`中。 `your_ca_cert` 和 `your_private_key` 值必须与 `configmap/logging-fluentd` 中的 `secure-forward.conf` 匹配

	$ oc patch secrets/logging-fluentd --type=json \
		--patch "[{'op':'add','path':'/data/your_ca_cert','value':'$(base64 /path/to/your_ca_cert.pem)'}]"
	$ oc patch secrets/logging-fluentd --type=json \
		--patch "[{'op':'add','path':'/data/your_private_key','value':'$(base64 /path/to/your_private_key.pem)'}]"	
配置外部聚合器时，必须更够从 fluentd 安全的接收信息。如果外部的聚合器是另外一个 fluentd，则必须安装 ` fluent-plugin-secure-forward` 插件，并向其提供以下数据到插件:

	<source>
		@type secure_forward

		self_hostname ${HOSTNAME}
		bind 0.0.0.0
		port 24284

		shared_key thisisasharedkey

		secure yes
		cert_path        /path/for/certificate/cert.pem
		private_key_path /path/for/certificate/key.pem
		private_key_passphrase secret_foo_bar_baz
	</source>	
这里可以找到 `fluent-plugin-secure-forward` 插件的[进一步解释](https://github.com/tagomoris/fluent-plugin-secure-forward)
#### 减少从 fluentd 到 api server的连接数
使用 `mux`,可以部署 n 个 `mux` 服务，其中 n 少于节点数。每个 fluentd 都配置有 ` USE_MUX_CLIENT=1`。这告诉 fluentd 将原始日志发送到 `mux`，不进行过滤，没有 k8s 元数据过滤，其中涉及到 api 服务器的连接。可以使用所有的处理器和 k8s 元数据过滤 `mux`。

注意`mux` 是技术预览版功能。

- openshift_logging_use_mux

	默认是 `false`。如果设置为 `true`,则会部署一个 `mux` 服务。此服务将充当 `secure_forward` 聚合器，用于在集群中运行的节点代理 fluentd 的守护程序。使用 ` openshift_logging_use_mux` 减少与 ocp api 服务器的连接数，并设置 fluentd 中的每个节点，将原始日志发送到 mux 并关闭 k8s 元数据插件。
- openshift_logging_mux_allow_external

	默认是 `false`。如果设置为 `true`,则会部署 `mux` 服务。并将其配置允许在集群外部运行的 fluentd 客户端使用 `secure_forward` 发送日志。允许将 ocp 日志记录用作中央日志记录服务提供给外部的集群或者客户端。
- openshift_logging_use_mux_client

	默认是 `false`。如果设置为 `true`,则节点代理 fluentd 服务配置为将日志发送到 mux，而不是 es。并将 k8s 元数据过滤被禁用，减少 api 服务连接数。
- openshift_logging_mux_hostname

	默认是 `mux` 添加 `openshift_master_default_subdomain`。这是 `external_clients` 的 hostname 用于连接到 mux，并作为 tls 服务器认证主体。
- openshift_logging_mux_port
		
	24284
- openshift_logging_mux_cpu_limit

	500M
- openshift_logging_mux_memory_limit

	1Gi
- openshift_logging_mux_default_namespaces

	默认是 `mux-undefined` ，列表中第一个值用于未定义项目的命名空间，后跟默认创建的任何其他命名空间。通常不需要设置。
- openshift_logging_mux_namespaces

	默认值为 `empty`，允许创建用于外部的 mux 客户端与其日志关联的其他命名空间。将需要设置此值。
	
#### fluentd 中的日志节流器
对于特别大的项目，在处理之前，管理员可以减少 fluentd 读取日志的速率。限制可以帮助配置项目落后的日志聚合。如果在 fluentd 赶上之前删除一个 pod，则日志条目可能丢失。使用 systemd journal 作为日志源时，调节不起作用。节流器实现取决于是否能够限制每个项目的单个日志文件的读取。从日志记录中读取时，只有一个日志源，没有日志文件则没有基于文件的限制可用。没有一种限制读入 fluentd 进程日志条目的方法。告诉 fluentd 那个项目应该限制，部署后在其 `ConfigMap` 中编辑配置：

	$ oc edit configmap/logging-fluentd
`throttle-config.yaml` 格式时一个 yaml 文件，其中包含项目名称和每个节点读取日志所需的速率。每个节点默认为 1000 行。例如

	logging:
		read_lines_limit: 500

	test-project:
	 	read_lines_limit: 10

	.operations:
		read_lines_limit: 100
### kibana
要从 ocp web控制台访问 kibana 控制台，在 ` /etc/origin/master/master-config.yaml` 文件中添加 `loggingPublicURL` 参数，与 kibana 控制台(kibana-hostname 参数)的 URL。该值必须是 `HTTPS URL`

	...
	assetConfig:
		...
		loggingPublicURL: "https://kibana.example.com"
	...			
设置 `loggingPublicURL` 参数将在 ocp web 控制台下创建一个可见的 `Archive` 按钮。点击它连接到 kibana 控制台。

可以很简单的扩容 kibana 

	$ oc scale dc/logging-kibana --replicas=2
为了确保多次执行 logging playbook 可以保持设置持久化，可以在 `inventry` 中设置 `openshift_logging_kibana_replica_count`。可以通过访问 `openshift_logging_kibana_hostname` 变量来指定用户界面。 [kibana 具体信息](https://www.elastic.co/guide/en/kibana/4.5/discover.html)
#### kibana 可视化
kibana 可视化能使用户能够创建可视化和仪表盘来监控容器和 pod，从而允许管理员用户(cluster-admin 或 cluster-reader) 通过部署，命名空间，pod和容器来查看日志。 kibana 可视化存在于 es和  ES-OPS pod 中，必须在这些 pod 中运行。要加载仪表盘和其他的 kibana ui 对象，必须首先要添加仪表盘的用户身份登录 kibana，然后注销。这将创建下一步依赖的必要的每个用户配置，然后运行

	$ oc exec <$espod> -- es_load_kibana_ui_objects <user-name>
其中 `$espod` 是 es 中的任何一个 pods

### Curator
Curator 允许管理员配置预定的es操作，以便在每个项目的基础上自动执行。它计划每天根据其配置执行操作。每个 es 集群只能有一个 Curator pod。 Curator 通过 yaml 配置文件进行配置，具体结构如下:

```
	$PROJECT_NAME:
		$ACTION:
    		$UNIT: $VALUE

	$PROJECT_NAME:
  		$ACTION:
  			$UNIT: $VALUE
	...
```

可用参数
 
- `$PROJECT_NAME`

 	项目的实际名称如:`myapp-devel`。对于 ocp `operations`日志，使用名称 `.operations` 作为项目名称。

- `$ACTION`

	采取的行动，目前只有允许 `delete`
- `$UNIT`

	单位，`days`, `weeks`,  `months`
- `$VALUE`

	单位是整数	
- `.defaults`

	使用 `.defaults` 作为 `$PROJECT_NAME` 设置未指定的项目默认值
- `runhour`

	当日的(数字)小时以 24 小时格式运行 Curator 任务。用于 `.defaults`	
- `runminute `												
	小时中(数字)分钟运行 Curator 任务。用于 `.defaults`

例如，要配置 Curator 为：

- 删除 `myapp-dev` 项目中超过 1天 `1 day` 的索引
- 删除 `myapp-qe` 项目超过1周 `1 week` 的索引
- 删除8周 `8 weeks` 以上所有 `operations` 日志
- 删除所有超过30天 `30 days` 的其他项目数据
- 每天午夜运行 	Curator

例

	myapp-dev:
		delete:
			days: 1

	myapp-qe:
		delete:
			weeks: 1

	.operations:
		delete:
			weeks: 8

	.defaults:
		delete:
			days: 30
		runhour: 0
		runminute: 0

当使用月份作为 `operation` 的 `$UNIT` 时，Curator 认为的第一天是当月第一天，而不是运行日。例如，如果今天是4月15号，想要删除比今天大2个月的索引，Curator 不会删除大于 2月15号的索引，而会删除2月1号之前的索引。最好设置单位为天。(比如 delete: days: 30)

#### 创建 Curator 配置
`openshift_logging` ansible 角色提供了一个 `ConfigMap`，Curator 从中可以看到它的配置。可以编辑或者替换 `ConfigMap ` 来重新配置 Curator。当前 `logging-curator ConfigMap` 用于配置操作和非操作 Curator 实例。任何 ` .operations ` 配置为于与应用程序日志相同的位置。

1. 编辑 `ConfigMap` 配置 Curator 实例

		$ oc edit configmap/logging-curator
- 替换配置

		$ create /path/to/mycuratorconfig.yaml
		$ oc create configmap logging-curator -o yaml \
		--from-file=config.yaml=/path/to/mycuratorconfig.yaml | \
		oc replace -f -
- 更改后重新部署

		$ oc rollout latest dc/logging-curator
		$ oc rollout latest dc/logging-curator-ops

### 卸载 EFK

	$ ansible-playbook [-i </path/to/inventory>] \ /usr/share/ansible/openshift-ansible/playbooks/common/openshift-cluster/openshift_logging.yml \
    	-e openshift_logging_install_logging=False
### 故障检查 Kibana
在 ocp 平台上使用 kibana 控制台可能会导致的问题。可以使用以下方案排查

- 登录循环

	kibana 控制台的 oauth2 代理必须与主机的 oauth2 服务器共享密码。如果两个服务器密码不一致，则可能导致登录循环，不断重定向到 kibana 登录页面。要解决此问题，需要删除当前的 oauth 客户端，并使用 openshift-ansible 重新运行 openshift_logging 角色。
	
		$ oc delete oauthclient/kibana-proxy
		$ ansible-playbook [-i </path/to/inventory>] \
    		/usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/openshift-logging.yml
- 控制台出现加密错误

	尝试访问 kibana 控制台时，可能会收到游览器的错误
	
		{"error":"invalid_request","error_description":"The request is missing a required parameter,includes an invalid parameter value, includes a parameter more than once, or is otherwise malformed."} 
	这个可能由 oauth2 客户端端与服务器之间的不匹配引起的。客户端的返回地址必须在白名单中，因此服务器可以在登录后重新安全的重定向。可以通过替换 oauth 客户端条目来解决。
	
		$ oc delete oauthclient/kibana-proxy
		$ ansible-playbook [-i </path/to/inventory>] \
    		/usr/share/ansible/openshift-ansible/playbooks/byo/openshift-cluster/openshift-logging.yml
    如果问题依然存在，请检查是否在 oauth 客户端中列出的 url 访问 kibana。这个问题可能时通过访问转发端口的 url ，如 1443 而不是标准的 443 https 端口引起的。可以通过编辑 oauth 客户端来调整服务器白名单
    
    	$ oc edit oauthclient/kibana-proxy
- 查看控制台时出现 503 错误

	如果在查看 kibana 控制台时接收到代理错误，可能由两个问题之一引起。
	
	- 首先，kibana 可能不识别 pod。如果 es 启动很慢，kibana 可能呢暂停尝试到达它。检查相关服务端点。
	
			$ oc describe service logging-kibana
			Name:                   logging-kibana
			[...]
			Endpoints:              <none> 		
		如果任何 kibana  pod 时活的，则列出端点。如果没有，请检查 kibana pod 的状态并进行部署。肯能需要备份和重新部署。
	- 其次，如果访问 kibana 服务路由被屏蔽，可能会导致第二个可能问题。如果在一个项目中执行测试部署，然后在不完全删除第一个部署的情况下部署其他项目，则可能会发生这种情况，当多个路由发送到同一个目的地时，默认的路由只会路由到第一个创建的路由。检查路由问题，看是否多次定义。

			$ oc get route  --all-namespaces --selector logging-infra=support
- F5 负载均衡和 `X-Forwarded-For` 启动

	如果尝试在启动 `X-Forwarded-For` 的 kibana 前使用 f5 负载均衡，则可能会导致 es Searchguard 插件无法正确接受来自 kinaba 连接问题， kinaba 错误信息:
	
		Kibana: Unknown error while connecting to Elasticsearch

		Error: Unknown error while connecting to Elasticsearch
		Error: UnknownHostException[No trusted proxies]
	配置 `Searchguard` 以忽略额外的头
	
	- 缩减所有的 fluentd pod
	- fluentd pod 全部终止后，缩减 es pod
	- 添加 `searchguard.http.xforwardedfor.header: DUMMY`到 es 配置部分

			$ oc edit configmap/logging-elasticsearch 
	- 扩展恢复 es pod
	- 扩展恢复 fluentd pod

### 发送日志到外部 es 实例
fluentd 将日志发送到 es 部署配置 `ES_HOST, ES_PORT, OPS_HOST,  OPS_PORT` 环境变量的值。应用日志重定向到 `ES_HOST`，操作日志重定向到 `OPS_HOST`。要将日志指向特定的 es 实例，请编辑部署配置，并使用实例变量替换。

	$ oc edit dc/<deployment_configuration>
对于外部的 es 实例来包含应用程序和操作的日志，可以设置 `ES_HOST` 和 `OPS_HOST`设置为同一个目标，同时确保 `ES_PORT`和 `OPS_PORT` 值相同。如果外部 es 不使用 tls，请将 `_CLIENT_CERT, _CLIENT_KEY, _CA` 设置为空。如果它使用 tls 而不是相互的 tls ，则将 `_CLIENT_CERT`和`_CLIENT_KEY`设置为空，并使用合适的 `_CA`修改或者重新创建 `logging-fluentd` secret。如果时相互 tls ，则可以使用客户端密钥，客户端证书和 ca 来修复或重新创建 `logging-fluentd` secret。

如果没有使用提供的 kibana 和 es 镜像，则不具有相同的多租户功能，并且数据不受用户访问特定项目的限制。
### es 运维管理
在 logging 3.2.0 中，可以在 ` logging-elasticsearch` secret 内提供用于与 es 通讯并执行管理操作的管理员证书、密钥、ca。要确认 efk是否安装这些，可以运行：

	$ oc describe secret logging-elasticsearch
如果不可用，[参考手册](https://docs.openshift.com/container-platform/3.6/install_config/upgrading/manual_upgrades.html#manual-upgrading-efk-logging-stack)，并确保处于最新版本。

- 连接到正在尝试维护的集群中的 es pod
- 要在集群查找可用 pod 						

		$ oc get pods -l component=es -o name | head -1
		$ oc get pods -l component=es-ops -o name | head -1
- 连接一个 pod

		$ oc rsh <your_Elasticsearch_pod>
- 一但连接到 es 容器，可以使用已安装的证书，从 secret 到与 es 根据其提供的 api 进行交流。

	fluentd 使用索引格式 `project.{project_name}` 项目其日志发送到 es。 `{project_uuid}.YYYY.MM.DD` 其中 `YYYY.MM.DD`是日志记录的日期。 例如，要从2016年6月15号使用 `uuid 3b3594fa-2ccd-11e6-acb7-0eb6b35eaee3` 删除所有日志记录，可以用
	
		$ curl --key /etc/elasticsearch/secret/admin-key \
			--cert /etc/elasticsearch/secret/admin-cert \
			--cacert /etc/elasticsearch/secret/admin-ca -XDELETE \
			"https://localhost:9200/project.logging.3b3594fa-2ccd-11e6-acb7-0eb6b35eaee3.2016.06.15"

### 在 docker 日志驱动程序更新后更新 fluentd 的日志源
如果 docker 日志驱动从 Json 修改成 journald，并之前配置 `USE_JOURNAL=False`，那么需要修改默认值。要启动时更新 fluentd 检测正确的源。

1. 从部署 fluentd 节点删除标签

		$ oc label node --all logging-infra-fluentd- #假定是默认的节点选择器并部署在所有节点上
- 将 `daemonset/logging-fluentd USE_JOURNA`的值更新

		$ oc patch daemonset/logging-fluentd \
		-p '{"spec":{"template":{"spec":{"containers":[{"name":"fluentd-elasticsearch","env":[{"name": "USE_JOURNAL", "value":""}]}]}}}}'
- 重新标记节点部署 fluentd

		$ oc label node --all logging-infra-fluentd=true 																	