# 环境健康检查
## 检查完整的环境健康
要验证群集的端到端功能，请构建和部署示例应用程序。

步骤

1. 创建一个名为的新项目 `validate`，以及 `cakephp-mysql-example` 模板中的示例应用程序：

		$ oc new-project validate
		$ oc new-app cakephp-mysql-example
	可以检查日志
	
		$ oc logs -f bc/cakephp-mysql-example
- 构建完成后，应该运行两个pod,数据库和应用程序	

		$ oc get pods
		NAME                            READY     STATUS      RESTARTS   AGE
		cakephp-mysql-example-1-build   0/1       Completed   0          1m
		cakephp-mysql-example-2-247xm   1/1       Running     0          39s
		mysql-1-hbk46                   1/1       Running     0          1m
- 访问应用程序URL。Cake PHP 框架欢迎页面应该是可见的。URL应具有以下格式 `cakephp-mysql-example-validate.<app_domain>`
- 验证功能后，`validate` 可以删除项目，项目中的所有资源也被删除

		$ oc delete project validate

## 使用 Prometheus 创建警报
可以将集群与 Prometheus 集成，以创建监控和警报，以帮助诊断任何环境问题。这些问题可能包括节点是否关闭，pod 是否占用过多 CPU 或内存等等。

有关详细信息，请参阅 [安装和配置指南中的 prometheus 部分](https://docs.okd.io/3.10/install_config/cluster_metrics.html#openshift-prometheus)上的 Prometheus”部分。
## 宿主机健康
要验证群集主机是否已启动并正在运行，请连接到 master 服务，运行以下命令

	# oc get nodes
	NAME                     STATUS    ROLES           AGE       VERSION
	master237.dmos.dataman   Ready     master          5d        v1.10.0+b81c8f8
	node186.dmos.dataman     Ready     compute         5d        v1.10.0+b81c8f8
	node238.dmos.dataman     Ready     compute,infra   5d        v1.10.0+b81c8f8


上述群集示例包含1个 master 节点、1个基础结构节点和1个计算节点。所有这些都在运行。群集中的所有主机都应在此输出中可见。

该 `Ready` 状态表示 master 节点可以与计算节点通信，并且所述节点准备运行 pod（不包括标记调度禁止和没有compute角色的节点)

在运行 `etcd` 命令之前，请获取 `etcd.conf` 文件：

	#source /etc/etcd/etcd.conf
可以使用以下 `etcdctl` 命令从任何 master 节点检查基本的 etcd 运行状况

	# etcdctl --cert-file=$ETCD_PEER_CERT_FILE --key-file=$ETCD_PEER_KEY_FILE \
	  --ca-file=/etc/etcd/ca.crt --endpoints=$ETCD_LISTEN_CLIENT_URLS cluster-health
	member 59df5107484b84df is healthy: got healthy result from https://10.156.0.5:2379
	member 6df7221a03f65299 is healthy: got healthy result from https://10.156.0.6:2379
	member fea6dfedf3eecfa3 is healthy: got healthy result from https://10.156.0.9:2379
	cluster is healthy
要获取更多信息，如主控节点

	etcdctl --cert-file=$ETCD_PEER_CERT_FILE --key-file=$ETCD_PEER_KEY_FILE \
	  --ca-file=/etc/etcd/ca.crt --endpoints=$ETCD_LISTEN_CLIENT_URLS member list
	295750b7103123e0: name=ocp-master-zh8d peerURLs=https://10.156.0.7:2380 clientURLs=https://10.156.0.7:2379 isLeader=true
	b097a72f2610aea5: name=ocp-master-qcg3 peerURLs=https://10.156.0.11:2380 clientURLs=https://10.156.0.11:2379 isLeader=false
	fea6dfedf3eecfa3: name=ocp-master-j338 peerURLs=https://10.156.0.9:2380 clientURLs=https://10.156.0.9:2379 isLeader=false
如果 etcd 集群与 master 服务位于同一服务器，则所有 etcd 主机应包含 master 主机名，或如果 etcd 单独运行，则所有 etcd 实例应该可见。

`etcdctl2` 是该 `etcdctl` 工具的别名，该工具包含用于在v2数据模型中查询 `etcd` 集群的正确标志，以及 `etcdctl3` 用于v3数据模型。

	注意 3.10 因为 etcd 部署在容器中，所以以上命令均需要执行在容器里。
## 路由器和镜像仓库健康
要检查路由器服务是否正在运行

	$ oc -n default get deploymentconfigs/router
	NAME      REVISION   DESIRED   CURRENT   TRIGGERED BY
	router    1          3         3         config
`DESIRED` 和 `CURRENT` 列中的值应与节点主机的数量相匹配

使用相同的命令检查注册表状态：

	$ oc -n default get deploymentconfigs/docker-registry
	NAME              REVISION   DESIRED   CURRENT   TRIGGERED BY
	docker-registry   1          3         3         config
	
镜像仓库多个运行实例需要支持多个进程写入的后端存储。如果所选的基础结构提供程序不包含此功能，则可以运行单个实例。

要验证所有 pod 是否正在运行以及在哪些主机上运行：

	$ oc -n default get pods -o wide
	NAME                       READY     STATUS    RESTARTS   AGE       IP            NODE
	docker-registry-1-54nhl    1/1       Running   0          2d        172.16.2.3    ocp-infra-node-tl47
	docker-registry-1-jsm2t    1/1       Running   0          2d        172.16.8.2    ocp-infra-node-62rc
	docker-registry-1-qbt4g    1/1       Running   0          2d        172.16.14.3   ocp-infra-node-xrtz
	registry-console-2-gbhcz   1/1       Running   0          2d        172.16.8.4    ocp-infra-node-62rc
	router-1-6zhf8             1/1       Running   0          2d        10.156.0.4    ocp-infra-node-62rc
	router-1-ffq4g             1/1       Running   0          2d        10.156.0.10   ocp-infra-node-tl47
	router-1-zqxbl             1/1       Running   0          2d        10.156.0.8    ocp-infra-node-xrtz

如果使用外部镜像仓库，则不需要运行内部镜像仓库。
## 网络连接
网络连接有两个主要的网络层:用于节点交互的集群网络以及用于 pod 交互的软件定义网络SDN。OKD 支持多种网络配置，通常针对特定基础架构提供商进行优化。

由于网络的复杂性，本节不涉及所有验证方案。
## 主控主机上的连接
- etcd 和 master主机

	master 的服务使用 etcd 键值存储保持其状态同步。master 和 etcd 服务之间的通信很重要，无论这些 etcd 服务在哪里运行。此通信发生在 TCP 端口 `2379`和`2380`。有关检查此通信的方法，请参阅[master 运行状况部分](https://docs.okd.io/3.10/day_two_guide/environment_health_checks.html#day-two-guide-host-health)。
- SkyDNS

	SkyDNS 提供在OKD中运行的本地服务的名称解析。此服务使用`TCP`和`UDP`端口`8053`。

要验证名称解析

	$ dig +short docker-registry.default.svc.cluster.local
	172.30.150.7
如果答案与以下输出匹配，则 SkyDNS 服务正常运行

	$ oc get svc/docker-registry -n default
	NAME              CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
	docker-registry   172.30.150.7   <none>        5000/TCP   3d
- API service 和 web console

	API 服务和 Web 控制台共享同一端口，通常 TCP `8443` 还是 `443` 取决于设置。此端口需要在群集中以及需要使用已部署环境的每个人可用。对于内部群集和外部客户端，可以访问此端口的URL可能不同。

	在以下示例中，`https://internal-master.example.com:443` URL由内部群集使用，`https://master.example.com:443` URL由外部客户端使用。
	
	- 在任何节点主机上

			$ curl https://internal-master.example.com:443/version
			{
			  "major": "1",
			  "minor": "6",
			  "gitVersion": "v1.6.1+5115d708d7",
			  "gitCommit": "fff65cf",
			  "gitTreeState": "clean",
			  "buildDate": "2017-10-11T22:44:25Z",
			  "goVersion": "go1.7.6",
			  "compiler": "gc",
			  "platform": "linux/amd64"
			}
	- 从客户的网络访问

			$ curl -k https://master.example.com:443/healthz
			ok

## 计算节点实例上的连接
计算节点上的 SDN 连接 pod 通信默认使用 UDP 端口 `4789`。

要验证计算节点主机功能，请创建一个新应用程序。以下示例确保节点到达在基础结构节点上运行的 docker 仓库：

步骤

1. 创建一个新项目

		$ oc new-project sdn-test
- 部署httpd应用程序

		$ oc new-app centos/httpd-24-centos7~https://github.com/sclorg/httpd-ex
	等到构建完成
	
		$ oc get pods
		NAME               READY     STATUS      RESTARTS   AGE
		httpd-ex-1-205hz   1/1       Running     0          34s
		httpd-ex-1-build   0/1       Completed   0          1m
- 连接到正在运行的 pod

		$ oc rsh po/<pod-name>
	例子
		
		
		$ oc rsh po/httpd-ex-1-205hz
- 检查 `healthz` 内部镜像仓库的路径

		$ curl -kv https://docker-registry.default.svc.cluster.local:5000/healthz
		* About to connect() to docker-registry.default.svc.cluster.locl port 5000 (#0)
		*   Trying 172.30.150.7...
		* Connected to docker-registry.default.svc.cluster.local (172.30.150.7) port 5000 (#0)
		* Initializing NSS with certpath: sql:/etc/pki/nssdb
		* skipping SSL peer certificate verification
		* SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
		* Server certificate:
		* 	subject: CN=172.30.150.7
		* 	start date: Nov 30 17:21:51 2017 GMT
		* 	expire date: Nov 30 17:21:52 2019 GMT
		* 	common name: 172.30.150.7
		* 	issuer: CN=openshift-signer@1512059618
		> GET /healthz HTTP/1.1
		> User-Agent: curl/7.29.0
		> Host: docker-registry.default.svc.cluster.local:5000
		> Accept: */*
		>
		< HTTP/1.1 200 OK
		< Cache-Control: no-cache
		< Date: Mon, 04 Dec 2017 16:26:49 GMT
		< Content-Length: 0
		< Content-Type: text/plain; charset=utf-8
		<
		* Connection #0 to host docker-registry.default.svc.cluster.local left intact
		
		sh-4.2$ *exit*
	
	返回200算正常
- 清理测试项目

		$ oc delete project sdn-test
		project "sdn-test" deleted
- 计算节点主机侦听 `TCP` 端口 `10250`。任何节点上的所有主机都需要可以访问此端口，如果在群集中部署了监视，则基础结构节点也必须能够访问所有实例上的此端口。可以使用以下命令检测此端口

		$ oc get nodes
		NAME                  STATUS                     AGE       VERSION
		ocp-infra-node-1clj   Ready                      4d        v1.6.1+5115d708d7
		ocp-infra-node-86qr   Ready                      4d        v1.6.1+5115d708d7
		ocp-infra-node-g8qw   Ready                      4d        v1.6.1+5115d708d7
		ocp-master-94zd       Ready,SchedulingDisabled   4d        v1.6.1+5115d708d7
		ocp-master-gjkm       Ready,SchedulingDisabled   4d        v1.6.1+5115d708d7
		ocp-master-wc8w       Ready,SchedulingDisabled   4d        v1.6.1+5115d708d7
		ocp-node-c5dg         Ready                      4d        v1.6.1+5115d708d7
		ocp-node-ghxn         Ready                      4d        v1.6.1+5115d708d7
		ocp-node-w135         NotReady                   4d        v1.6.1+5115d708d7

	在上面的输出中，`ocp-node-w135` 主服务无法访问节点上的节点服务，这由其`NotReady` 状态表示。
- 最后一项服务是路由器，它负责将连接路由到在集群中运行的正确服务。路由器侦听TCP端口80和443基础结构节点上的入口流量。在路由器开始工作之前，必须配置客户端或外网的 DNS

		$ dig *.apps.example.com
		
		; <<>> DiG 9.11.1-P3-RedHat-9.11.1-8.P3.fc27 <<>> *.apps.example.com
		;; global options: +cmd
		;; Got answer:
		;; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 45790
		;; flags: qr rd ra; QUERY: 1, ANSWER: 2, AUTHORITY: 0, ADDITIONAL: 1
		
		;; OPT PSEUDOSECTION:
		; EDNS: version: 0, flags:; udp: 4096
		;; QUESTION SECTION:
		;*.apps.example.com.	IN	A
		
		;; ANSWER SECTION:
		*.apps.example.com. 3571	IN	CNAME	apps.example.com.
		apps.example.com.	3561	IN	A	35.xx.xx.92
		
		;; Query time: 0 msec
		;; SERVER: 127.0.0.1#53(127.0.0.1)
		;; WHEN: Tue Dec 05 16:03:52 CET 2017
		;; MSG SIZE  rcvd: 105					
	在这种情况下 `35.xx.xx.92`，IP 地址应指向负载均衡器，将入口流量分配到所有基础结构节点。要验证路由器的功能，请再次检查镜像仓库服务，但这次是从群集外部进行的

		$ curl -kv https://docker-registry-default.apps.example.com/healthz
		*   Trying 35.xx.xx.92...
		* TCP_NODELAY set
		* Connected to docker-registry-default.apps.example.com (35.xx.xx.92) port 443 (#0)
		...
		< HTTP/2 200
		< cache-control: no-cache
		< content-type: text/plain; charset=utf-8
		< content-length: 0
		< date: Tue, 05 Dec 2017 15:13:27 GMT
		<
		* Connection #0 to host docker-registry-default.apps.example.com left intact
## 存储
master 节点需要至少40 GB的硬盘空间用于 /var 目录。使用以下 df 命令检查 master 主机的磁盘使用情况：		

		$ df -hT
		Filesystem     Type      Size  Used Avail Use% Mounted on
		/dev/sda1      xfs        45G  2.8G   43G   7% /
		devtmpfs       devtmpfs  3.6G     0  3.6G   0% /dev
		tmpfs          tmpfs     3.6G     0  3.6G   0% /dev/shm
		tmpfs          tmpfs     3.6G   63M  3.6G   2% /run
		tmpfs          tmpfs     3.6G     0  3.6G   0% /sys/fs/cgroup
		tmpfs          tmpfs     732M     0  732M   0% /run/user/1000
		tmpfs          tmpfs     732M     0  732M   0% /run/user/0****
节点实例需要至少 15 GB 的空间用于 `/var` 目录，至少另外 15 GB 用于 Docker 存储（/var/lib/docker在本例中）。根据群集的大小和 pod 所需的临时存储量，应 `/var/lib/origin/openshift.local.volumes` 在节点上创建单独的分区

		$ df -hT
		Filesystem     Type      Size  Used Avail Use% Mounted on
		/dev/sda1      xfs        25G  2.4G   23G  10% /
		devtmpfs       devtmpfs  3.6G     0  3.6G   0% /dev
		tmpfs          tmpfs     3.6G     0  3.6G   0% /dev/shm
		tmpfs          tmpfs     3.6G  147M  3.5G   4% /run
		tmpfs          tmpfs     3.6G     0  3.6G   0% /sys/fs/cgroup
		/dev/sdb       xfs        25G  2.7G   23G  11% /var/lib/docker
		/dev/sdc       xfs        50G   33M   50G   1% /var/lib/origin/openshift.local.volumes
		tmpfs          tmpfs     732M     0  732M   0% /run/user/1000

应该在运行群集的实例之外处理 pod 的持久存储。pod 的持久卷可以由基础结构提供程序提供，也可以使用容器本机存储或容器就绪存储。
## Docker存储
Docker Storage 可以通过两个选项之一进行备份。第一个是带有设备映射器的精简池逻辑卷，第二个是自 Red Hat Enterprise Linux 7.4版以来的 overlay2 文件系统。由于易于设置和提高性能，通常建议使用overlay2文件系统。

Docker存储磁盘/var/lib/docker以xfs 文件系统的形式安装和格式化。Docker存储配置为使用overlay2文件系统

	$ cat /etc/sysconfig/docker-storage
	DOCKER_STORAGE_OPTIONS='--storage-driver overlay2'
要验证Docker是否使用了此存储驱动程序

	# docker info
	Containers: 4
	 Running: 4
	 Paused: 0
	 Stopped: 0
	Images: 4
	Server Version: 1.12.6
	Storage Driver: overlay2
	 Backing Filesystem: xfs
	....
## API服务状态
API 服务 `atomic-openshift-master-api.service` 在所有主实例上运行。要查看服务的状态：

- 企业版

		$ systemctl status atomic-openshift-master-api.service
		● atomic-openshift-master-api.service - Atomic OpenShift Master API
		   Loaded: loaded (/usr/lib/systemd/system/atomic-openshift-master-api.service; enabled; vendor preset: disabled)
		   Active: active (running) since Thu 2017-11-30 11:40:19 EST; 5 days ago
		     Docs: https://github.com/openshift/origin
		 Main PID: 30047 (openshift)
		   Memory: 65.0M
		   CGroup: /system.slice/atomic-openshift-master-api.service
		           └─30047 /usr/bin/openshift start master api --config=/etc/origin/master/ma...
		
		Dec 06 09:15:49 ocp-master-94zd atomic-openshift-master-api[30047]: I1206 09:15:49.85...
		Dec 06 09:15:50 ocp-master-94zd atomic-openshift-master-api[30047]: I1206 09:15:50.96...
		Dec 06 09:15:52 ocp-master-94zd atomic-openshift-master-api[30047]: I1206 09:15:52.34...
- 社区版

		# service origin-master-api status
		Redirecting to /bin/systemctl status origin-master-api.service
		● origin-master-api.service - Atomic OpenShift Master API
		   Loaded: loaded (/usr/lib/systemd/system/origin-master-api.service; enabled; vendor preset: disabled)
		   Active: active (running) since 四 2018-08-30 18:42:32 CST; 3 weeks 6 days ago
		     Docs: https://github.com/openshift/origin
		 Main PID: 41030 (openshift)
		   Memory: 1.1G
		   CGroup: /system.slice/origin-master-api.service
		           └─41030 /usr/bin/openshift start master api --config=/etc/origin/master/master-config.yaml --loglevel=8 --listen=https://0.0.0.0:8443 --master=https://master227.dmos.dataman:8443
- 3.10 版

		[root@master237 ~]# oc project kube-system
		Now using project "kube-system" on server "https://master237.dmos.dataman:8443".
		[root@master237 ~]# oc  get pods
		NAME                                        READY     STATUS    RESTARTS   AGE
		master-api-master237.dmos.dataman           1/1       Running   7          6d
		master-controllers-master237.dmos.dataman   1/1       Running   7          6d
		master-etcd-master237.dmos.dataman          1/1       Running   3          6d

	API服务公开健康检查，可以使用以下方式在外部查询
	
		$ curl -k https://master.example.com/healthz
		ok
		
## 控制器角色验证
控制器服务 `atomic-openshift-master-controllers.service` 可在所有 master 主机上使用。该服务以主动/被动模式运行，这意味着它应该只能在一个 master 服务器上真正运行。

控制器执行一个过程来选择运行该服务的主机。当前运行值 `configmap` 存储在 `kube-system` 项目中存储的特殊注释中。

验证 `atomic-openshift-master-controllers` 以 `cluster-admin` 用户身份运行服务的 master 主机

	$ oc get -n kube-system cm openshift-master-controllers -o yaml
	apiVersion: v1
	kind: ConfigMap
	metadata:
	  annotations:
	    control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"master-ose-master-0.example.com-10.19.115.212-dnwrtcl4","leaseDurationSeconds":15,"acquireTime":"2018-02-17T18:16:54Z","renewTime":"2018-02-19T13:50:33Z","leaderTransitions":16}'
	  creationTimestamp: 2018-02-02T10:30:04Z
	  name: openshift-master-controllers
	  namespace: kube-system
	  resourceVersion: "17349662"
	  selfLink: /api/v1/namespaces/kube-system/configmaps/openshift-master-controllers
	  uid: 08636843-0804-11e8-8580-fa163eb934f0
该命令在 `control-plane.alpha.kubernetes.io/leader` 注释中输出当前主控制器 ，在 `holderIdentity` 属性中：

	master-<hostname>-<ip>-<8_random_characters>
通过使用以下内容过滤输出来查找 master 主机的主机名

	$ oc get -n kube-system cm openshift-master-controllers -o json | jq -r'.metadata.annotations [] | fromjson.holderIdentity | 匹配（“^ master  - （。*） -  [0-9。] *  -  [0-9a-z] {8} $”）| .captures [0] .string”
	ose-master-0.example.com
## 验证正确的最大传输单元（MTU）大小
验证最大传输单元（MTU）可防止可能伪装成SSL证书问题的网络配置错误。

当数据包大于通过 HTTP 传输的 MTU 大小时，物理网络路由器能够将数据包分成多个数据包以传输数据。但是，当数据包大于通过 HTTPS 传输的 MTU 大小时，路由器将被强制丢弃数据包。

安装生成证书，提供与多个组件的安全连接，包括：

- master
- node
- infra node
- registry
- router

可以 `/etc/origin/master` 在主节点的 `/etc/origin/node` 目录和 infra 和 app 节点的目录中找到这些证书。

安装后，可以 `REGISTRY_OPENSHIFT_SERVER_ADDR` 使用[网络连接](https://docs.okd.io/3.10/environment_health.html#day-two-guide-network-connectivity)部分中概述的过程验证与[该链接连接](https://docs.okd.io/3.10/environment_health.html#day-two-guide-network-connectivity)。

先觉条件

1. 从 master 主机获取 https 地址

		$ oc -n default get dc docker-registry -o jsonpath='{.spec.template.spec.containers[].env[?(@.name=="REGISTRY_OPENSHIFT_SERVER_ADDR")].value}{"\n"}'
		docker-registry.default.svc:5000

	以上给出了输出 `docker-registry.default.svc:5000`
- 附加 `/healthz` 到上面给出的值，使用它来检查所有主机(主服务器，基础结构，节点)

		$ curl -v https://docker-registry.default.svc:5000/healthz
		* About to connect() to docker-registry.default.svc port 5000 (#0)
		*   Trying 172.30.11.171...
		* Connected to docker-registry.default.svc (172.30.11.171) port 5000 (#0)
		* Initializing NSS with certpath: sql:/etc/pki/nssdb
		*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
		  CApath: none
		* SSL connection using TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
		* Server certificate:
		* 	subject: CN=172.30.11.171
		* 	start date: Oct 18 05:30:10 2017 GMT
		* 	expire date: Oct 18 05:30:11 2019 GMT
		* 	common name: 172.30.11.171
		* 	issuer: CN=openshift-signer@1508303629
		> GET /healthz HTTP/1.1
		> User-Agent: curl/7.29.0
		> Host: docker-registry.default.svc:5000
		> Accept: */*
		>
		< HTTP/1.1 200 OK
		< Cache-Control: no-cache
		< Date: Tue, 24 Oct 2017 19:42:35 GMT
		< Content-Length: 0
		< Content-Type: text/plain; charset=utf-8
		<
		* Connection #0 to host docker-registry.default.svc left intact
	上面的示例输出显示了用于确保 SSL 连接正确的 MTU 大小。尝试连接成功，然后建立连接，并使用 certpath 和有关 docker -registry 的所有服务器证书信息初始化 NSS 。

	MTU 大小不合适会导致超时
	
		$ curl -v https://docker-registry.default.svc:5000/healthz
		* About to connect() to docker-registry.default.svc port 5000 (#0)
		*   Trying 172.30.11.171...
		* Connected to docker-registry.default.svc (172.30.11.171) port 5000 (#0)
		* Initializing NSS with certpath: sql:/etc/pki/nssdb
	
	上面的示例显示已建立连接，但无法使用 certpath 完成 NSS 初始化。该问题涉及在适当的[节点配置映射中](https://docs.okd.io/3.10/admin_guide/manage_nodes.html#modifying-nodes)设置的不正确的MTU大小。

	要解决此问题，请将节点配置映射中的 MTU 大小调整为比 OpenShift SDN 以太网设备使用的 MTU 大小小 50 个字节。
- 查看所需以太网设备的 MTU 大小(即eth0)

		$ ip link show eth0
		2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
		    link/ether fa:16:3e:92:6a:86 brd ff:ff:ff:ff:ff:ff
	以上显示MTU设置为1500
- 要更改 MTU 大小，请修改相应的[节点配置映射](https://docs.okd.io/3.10/admin_guide/manage_nodes.html#modifying-nodes)， 并设置一个比`ip`命令提供的输出小50个字节的值。例，如果MTU大小设置为1500，则在节点配置映射中将MTU大小调整为1450：

		networkConfig:
		   mtu: 1450
- 保存更改并重新启动节点：

	必须在属于 OKD SDN 的所有主站和节点上更改MTU大小。此外，tun0 接口的MTU大小必须与作为群集一部分的所有节点相同。
- 节点重新联机后，通过重新运行原始 curl 命令确认问题不再存在

		$ curl -v https://docker-registry.default.svc:5000/healthz
