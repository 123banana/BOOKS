# 主机任务
## 将主机添加到群集
有关将 master 节点或计算节点添加到群集的信息，请参阅“安装和配置指南”中的“ [将主机添加到现有群集](https://docs.okd.io/3.10/install_config/adding_hosts_to_existing_cluster.html#install-config-adding-hosts-to-cluster)”部分。

## master 节点主机任务
### 弃用 master	节点
弃用 master 节点会将其从OKD环境中删除。弃用或缩小原因包括硬件重新调整大小或替换底层基础架构等。高可用环境至少需要3个 master 节点和3个 etcd 节点。通常，它们服务是共存的。如果弃用，则还必须弃用该主机上的 etcd 服务。

	由于在 master 服务之间发生的投票机制，确保始终以奇数部署 master 和 etcd 服务。
主控主机运行重要的服务，例如 API 和控制器服务等。要弃用，必须停止这些服务。

API 服务是主主模式，因此只要将请求发送到单独的 master 服务器，停止服务就不会影响环境。但是，控制器服务是主备模式，其中服务利用 etcd 来决定活动 leader 服务器。

在多主机架构中弃用 master 节点包括从负载平衡器池中删除主机，以避免将新连接转发到下线服务。此过程在很大程度上取决于所使用的负载平衡器。以下步骤显示了从 haproxy 中删除 master 节点的详细信息。如果 OKD 在云提供商或使用F5设备上运行，请参阅特定产品文档操作。

步骤

1. 删除配置文件中的 backend 部分 `/etc/haproxy/haproxy.cfg`。例如，如果弃用名为`master-0.example.comusing` 的 master 节点

		backend mgmt8443
		    balance source
		    mode tcp
		    # MASTERS 8443
		    server master-1.example.com 192.168.55.12:8443 check
		    server master-2.example.com 192.168.55.13:8443 check
- 重新启动 haproxy 服务

		sudo systemctl restart haproxy
- 禁用去除 master 节点上的 API 和控制器服务	

		$ sudo systemctl disable --now atomic-openshift-master-api
		$ sudo systemctl disable --now atomic-openshift-master-controllers
- 由 master 节点是可调度节点，请按照 [弃用节点主机](https://docs.okd.io/3.10/day_two_guide/host_level_tasks.html#deprecating-node_deprecating-etcd)部分中的步骤操作
- 从 Ansible 清单文件 `/etc/ansible/hosts` 中的 [masters] 和 [nodes] 组中删除 master 节点， 以避免在使用该清单文件运行任何 Ansible 任务时出现问题。
	- 弃用 Ansible 清单文件中列出的第一个 master 节点时需要额外的预防措施。
	- 该 `/etc/origin/master/ca.serial.txt` 文件仅在 Ansible 主机清单中列出的第一个 master 节点上生成。如果弃用，备份该文件到其余主主机。 
- 该 kubernetes 服务包括 master 主机 IP 作为端点。要验证是否已正确弃用主 kubernetes 服务器，请检查服务输出并查看是否已删除已弃用，成功弃用后，可以安全地删除主机

		$ oc describe svc kubernetes -n default
		Name:			kubernetes
		Namespace:		default
		Labels:			component=apiserver
					provider=kubernetes
		Annotations:		<none>
		Selector:		<none>
		Type:			ClusterIP
		IP:			10.111.0.1
		Port:			https	443/TCP
		Endpoints:		192.168.55.12:8443,192.168.55.13:8443
		Port:			dns	53/UDP
		Endpoints:		192.168.55.12:8053,192.168.55.13:8053
		Port:			dns-tcp	53/TCP
		Endpoints:		192.168.55.12:8053,192.168.55.13:8053
		Session Affinity:	ClientIP
		Events:			<none>	

### 删除 ETCD 节点
如果 etcd 主机无法恢复，请将其从群集中删除。

需要在所有 master 主机上执行

1. 从 etcd 集群中删除异常的 etcd 主机。对每个 etcd 节点运行以下命令

		# etcdctl -C https://<surviving host IP address>:2379 \
		  --ca-file=/etc/etcd/ca.crt     \
		  --cert-file=/etc/etcd/peer.crt     \
		  --key-file=/etc/etcd/peer.key member remove <failed member ID>
- 重启所有 api 服务器

		# master-restart api restart-master controller

在 etcd 节点执行步骤

1. 从群集中删除失败的主机，注意 `remove` 命令需要的是 etcdid 不是主机名

		# etcdctl2 cluster-health
		member 5ee217d19001 is healthy: got healthy result from https://192.168.55.12:2379
		member 2a529ba1840722c0 is healthy: got healthy result from https://192.168.55.8:2379
		failed to check the health of member 8372784203e11288 on https://192.168.55.21:2379: Get https://192.168.55.21:2379/health: dial tcp 192.168.55.21:2379: getsockopt: connection refused
		member 8372784203e11288 is unreachable: [https://192.168.55.21:2379] are all unreachable
		member ed4f0efd277d7599 is healthy: got healthy result from https://192.168.55.13:2379
		cluster is healthy
		
		# etcdctl2 member remove 8372784203e11288 
		Removed member 8372784203e11288 from cluster
		
		# etcdctl2 cluster-health
		member 5ee217d19001 is healthy: got healthy result from https://192.168.55.12:2379
		member 2a529ba1840722c0 is healthy: got healthy result from https://192.168.55.8:2379
		member ed4f0efd277d7599 is healthy: got healthy result from https://192.168.55.13:2379
		cluster is healthy
2. 要确保 etcd 配置在重新启动 etcd 服务时不使用失败的主机，请修改 `/etc/etcd/etcd.conf`所有剩余的 etcd 主机上的文件，并删除 `ETCD_INITIAL_CLUSTER` 变量值中的失败主机，不需要重新启动 etcd 服务，因为使用了删除失败的主机 `etcdctl`

		#vi /etc/etcd/etcd.conf
	如
	
		ETCD_INITIAL_CLUSTER=master-0.example.com=https://192.168.55.8:2380,master-1.example.com=https://192.168.55.12:2380,master-2.example.com=https://192.168.55.13:2380
	变为
	
		ETCD_INITIAL_CLUSTER=master-0.example.com=https://192.168.55.8:2380,master-1.example.com=https://192.168.55.12:2380
- 修改 Ansible 清单文件以反映群集的当前状态，并避免在重新运行 Playbook 时出现问题

		[OSEv3:children]
		masters
		nodes
		etcd
		
		... [OUTPUT ABBREVIATED] ...
		
		[etcd]
		master-0.example.com
		master-1.example.com
- 如果使用 `Flannel`，请修改 flanneld 位于 `/etc/sysconfig/flanneld` 每个主机上的服务配置并删除 `etcd` 主机
- 重启 `flanneld `

		#systemctl restart flanneld.service

### 还原 master 备份
创建重要 master 节点文件的备份后，如果它们被损坏或意外删除，可以通过将文件复制回 master 节点来恢复文件，确保它们包含正确的内容，然后重新启动受影响的服务。

步骤

1. 恢复 `/etc/origin/master/master-config.yaml` 文件

		# MYBACKUPDIR=*/backup/$(hostname)/$(date +%Y%m%d)*
		# cp /etc/origin/master/master-config.yaml /etc/origin/master/master-config.yaml.old
		# cp /backup/$(hostname)/$(date +%Y%m%d)/origin/master/master-config.yaml /etc/origin/master/master-config.yaml
		# master-restart api
		# master-restart controllers
		
	重新启动 master 服务可能会导致停机。但是，可以从负载平衡器中删除 master 服务，然后执行还原操作。正确还原服务后，再将添加回负载平衡器。

	执行受影响实例的完全重新引导以恢复 iptables 配置
- 如果由于缺少软件包而无法重新启动，请重新安装软件包。
	1. 获取当前已安装的软件包列表

			$ rpm -qa | sort > /tmp/current_packages.txt
	- 查看包列表之间的差异

			$ diff /tmp/current_packages.txt ${MYBACKUPDIR}/packages.txt
			
			> ansible-2.4.0.0-5.el7.noarch
	- 重新安装缺少的包

			# yum reinstall -y <packages>
- 通过将证书复制到 `/etc/pki/ca-trust/source/anchors/` 目录来还原系统证书并执行 `update-ca-trust`,请确保在复制文件时还原用户ID和组ID以及SELinux上下文

		$ MYBACKUPDIR=*/backup/$(hostname)/$(date +%Y%m%d)*
		$ sudo cp ${MYBACKUPDIR}/external_certificates/my_company.crt /etc/pki/ca-trust/source/anchors/
		$ sudo update-ca-trust

## 计算节点主机任务
### 弃用节点主机
弃用基础结构节点或应用程序节点的过程是相同的。
### 先决条件
确保集群有足够的容量可用于从要删除的节点集迁移现有Pod。仅当在删除基础结构节点后至少还有两个节点保持联机时，才建议删除基础结构节点。

步骤

1. 列出所有可用节点以查找要弃用的节点

		$ oc get nodes
		NAME                  STATUS                     AGE       VERSION
		ocp-infra-node-b7pl   Ready                      23h       v1.6.1+5115d708d7
		ocp-infra-node-p5zj   Ready                      23h       v1.6.1+5115d708d7
		ocp-infra-node-rghb   Ready                      23h       v1.6.1+5115d708d7
		ocp-master-dgf8       Ready,SchedulingDisabled   23h       v1.6.1+5115d708d7
		ocp-master-q1v2       Ready,SchedulingDisabled   23h       v1.6.1+5115d708d7
		ocp-master-vq70       Ready,SchedulingDisabled   23h       v1.6.1+5115d708d7
		ocp-node-020m         Ready                      23h       v1.6.1+5115d708d7
		ocp-node-7t5p         Ready                      23h       v1.6.1+5115d708d7
		ocp-node-n0dd         Ready                      23h       v1.6.1+5115d708d7

	例如，本主题弃用了 `ocp-infra-node-b7pl` 基础结构节点
- 描述节点及其运行服务

		$ oc describe node ocp-infra-node-b7pl
		Name:			ocp-infra-node-b7pl
		Role:
		Labels:			beta.kubernetes.io/arch=amd64
					beta.kubernetes.io/instance-type=n1-standard-2
					beta.kubernetes.io/os=linux
					failure-domain.beta.kubernetes.io/region=europe-west3
					failure-domain.beta.kubernetes.io/zone=europe-west3-c
					kubernetes.io/hostname=ocp-infra-node-b7pl
					role=infra
		Annotations:		volumes.kubernetes.io/controller-managed-attach-detach=true
		Taints:			<none>
		CreationTimestamp:	Wed, 22 Nov 2017 09:36:36 -0500
		Phase:
		Conditions:
		  ...
		Addresses:		10.156.0.11,ocp-infra-node-b7pl
		Capacity:
		 cpu:		2
		 memory:	7494480Ki
		 pods:		20
		Allocatable:
		 cpu:		2
		 memory:	7392080Ki
		 pods:		20
		System Info:
		 Machine ID:			bc95ccf67d047f2ae42c67862c202e44
		 System UUID:			9762CC3D-E23C-AB13-B8C5-FA16F0BCCE4C
		 Boot ID:			ca8bf088-905d-4ec0-beec-8f89f4527ce4
		 Kernel Version:		3.10.0-693.5.2.el7.x86_64
		 OS Image:			Employee SKU
		 Operating System:		linux
		 Architecture:			amd64
		 Container Runtime Version:	docker://1.12.6
		 Kubelet Version:		v1.6.1+5115d708d7
		 Kube-Proxy Version:		v1.6.1+5115d708d7
		ExternalID:			437740049672994824
		Non-terminated Pods:		(2 in total)
		  Namespace			Name				CPU Requests	CPU Limits	Memory Requests	Memory Limits
		  ---------			----				------------	----------	---------------	-------------
		  default			docker-registry-1-5szjs		100m (5%)	0 (0%)		256Mi (3%)0 (0%)
		  default			router-1-vzlzq			100m (5%)	0 (0%)		256Mi (3%)0 (0%)
		Allocated resources:
		  (Total limits may be over 100 percent, i.e., overcommitted.)
		  CPU Requests	CPU Limits	Memory Requests	Memory Limits
		  ------------	----------	---------------	-------------
		  200m (10%)	0 (0%)		512Mi (7%)	0 (0%)
		Events:		<none>
	上面的输出显示节点正在运行两个pod `router-1-vzlzq` 和 `docker-registry-1-5szjs`。另外两个基础结构节点可用于迁移这两个pod。

	上述群集是一个高度可用的群集，这意味着服务 `router` 和 `docker-registry` 服务都在所有基础结构节点上运行。
- 将节点标记为不可调度并撤离其所有pod

		$ oc adm drain ocp-infra-node-b7pl --delete-local-data
		node "ocp-infra-node-b7pl" cordoned
		WARNING: Deleting pods with local storage: docker-registry-1-5szjs
		pod "docker-registry-1-5szjs" evicted
		pod "router-1-vzlzq" evicted
		node "ocp-infra-node-b7pl" drained
	如果 pod 已附加本地存储（例如，EmptyDir），则必须提供`--delete-local-data`选项。通常，在生产中运行的 pod 应该仅将本地存储用于临时文件或缓存文件，但不能用于任何重要或持久的文件。对于常规存储，应用程序应使用对象存储或持久卷。在这种情况下，  `docker-registry` pod 的本地存储是空的，因为使用对象存储来存储镜像。

	上述操作删除节点上运行的现有 pod。然后，根据复制控制器创建新的 pod。

	通常，应使用部署配置部署每个应用程序，该部署配置使用复制控制器创建 pod。

	`oc adm drain` 不会删除任何裸露 pod (既不是镜面 pod 也不是由管理ReplicationController、ReplicaSet、DaemonSet、StatefulSet 或 job 管理的 pod)。为此，`--force` 需要该选项。请注意，裸机 pod 将不会在其他节点上重新创建，并且在此操作期间可能会丢失数据。

	下面的示例显示了镜像仓库的复制控制器的输出
	
		$ oc describe rc/docker-registry-1
		Name:		docker-registry-1
		Namespace:	default
		Selector:	deployment=docker-registry-1,deploymentconfig=docker-registry,docker-registry=default
		Labels:		docker-registry=default
				openshift.io/deployment-config.name=docker-registry
		Annotations: ...
		Replicas:	3 current / 3 desired
		Pods Status:	3 Running / 0 Waiting / 0 Succeeded / 0 Failed
		Pod Template:
		  Labels:		deployment=docker-registry-1
					deploymentconfig=docker-registry
					docker-registry=default
		  Annotations:		openshift.io/deployment-config.latest-version=1
					openshift.io/deployment-config.name=docker-registry
					openshift.io/deployment.name=docker-registry-1
		  Service Account:	registry
		  Containers:
		   registry:
		    Image:	openshift3/ose-docker-registry:v3.6.173.0.49
		    Port:	5000/TCP
		    Requests:
		      cpu:	100m
		      memory:	256Mi
		    Liveness:	http-get https://:5000/healthz delay=10s timeout=5s period=10s #success=1 #failure=3
		    Readiness:	http-get https://:5000/healthz delay=0s timeout=5s period=10s #success=1 #failure=3
		    Environment:
		      REGISTRY_HTTP_ADDR:					:5000
		      REGISTRY_HTTP_NET:					tcp
		      REGISTRY_HTTP_SECRET:					tyGEnDZmc8dQfioP3WkNd5z+Xbdfy/JVXf/NLo3s/zE=
		      REGISTRY_MIDDLEWARE_REPOSITORY_OPENSHIFT_ENFORCEQUOTA:	false
		      REGISTRY_HTTP_TLS_KEY:					/etc/secrets/registry.key
		      OPENSHIFT_DEFAULT_REGISTRY:				docker-registry.default.svc:5000
		      REGISTRY_CONFIGURATION_PATH:				/etc/registry/config.yml
		      REGISTRY_HTTP_TLS_CERTIFICATE:				/etc/secrets/registry.crt
		    Mounts:
		      /etc/registry from docker-config (rw)
		      /etc/secrets from registry-certificates (rw)
		      /registry from registry-storage (rw)
		  Volumes:
		   registry-storage:
		    Type:	EmptyDir (a temporary directory that shares a pod's lifetime)
		    Medium:
		   registry-certificates:
		    Type:	Secret (a volume populated by a Secret)
		    SecretName:	registry-certificates
		    Optional:	false
		   docker-config:
		    Type:	Secret (a volume populated by a Secret)
		    SecretName:	registry-config
		    Optional:	false
		Events:
		  FirstSeen	LastSeen	Count	From			SubObjectPath	Type		Reason		Message
		  ---------	--------	-----	----			-------------	--------	------		-------
		  49m		49m		1	replication-controller			Normal		SuccessfulCreate	Created pod: docker-registry-1-dprp5
	输出底部的事件显示有关新pod创建的信息。因此，列出所有pod时

		$ oc get pods
		NAME                       READY     STATUS    RESTARTS   AGE
		docker-registry-1-dprp5    1/1       Running   0          52m
		docker-registry-1-kr8jq    1/1       Running   0          1d
		docker-registry-1-ncpl2    1/1       Running   0          1d
		registry-console-1-g4nqg   1/1       Running   0          1d
		router-1-2gshr             0/1       Pending   0          52m
		router-1-85qm4             1/1       Running   0          1d
		router-1-q5sr8             1/1       Running   0          1d
- 现在已弃用的节点上运行的 `docker-registry-1-5szjs` 和 `router-1-vzlzq` pod 已不再可用。相反，已经创建了两个新的 pod `docker-registry-1-dprp5` 和 `router-1-2gshr`。如上所示，新路由器 pod 是 `router-1-2gshr`，但处于该 Pending 状态。这是因为每个节点只能在一个路由器上运行，并且绑定到主机的端口80和443。
- 在观察新创建的镜像仓库时，下面的示例显示已在 `ocp-infra-node-rghb` 节点上创建了 pod ，这与弃用节点不同

		$ oc describe pod docker-registry-1-dprp5
		Name:			docker-registry-1-dprp5
		Namespace:		default
		Security Policy:	hostnetwork
		Node:			ocp-infra-node-rghb/10.156.0.10
		...
	弃用基础结构和应用程序节点之间的唯一区别是，一旦基础结构节点被撤离，并且如果没有计划替换该节点，则可以缩小基础结构节点上运行的服务

		$ oc scale dc/router --replicas 2
		deploymentconfig "router" scaled
		
		$ oc scale dc/docker-registry --replicas 2
		deploymentconfig "docker-registry" scaled
- 现在，每个基础结构节点只运行一种每个 pod

		$ oc get pods
		NAME                       READY     STATUS    RESTARTS   AGE
		docker-registry-1-kr8jq    1/1       Running   0          1d
		docker-registry-1-ncpl2    1/1       Running   0          1d
		registry-console-1-g4nqg   1/1       Running   0          1d
		router-1-85qm4             1/1       Running   0          1d
		router-1-q5sr8             1/1       Running   0          1d
		
		$ oc describe po/docker-registry-1-kr8jq | grep Node:
		Node:			ocp-infra-node-p5zj/10.156.0.9
		
		$ oc describe po/docker-registry-1-ncpl2 | grep Node:
		Node:			ocp-infra-node-rghb/10.156.0.10
- 要提供完全高可用性的群集，应始终至少有三个基础结构节点可用
- 要验证节点上的调度是否已禁用

		$ oc get nodes
		NAME                  STATUS                     AGE       VERSION
		ocp-infra-node-b7pl   Ready,SchedulingDisabled   1d        v1.6.1+5115d708d7
		ocp-infra-node-p5zj   Ready                      1d        v1.6.1+5115d708d7
		ocp-infra-node-rghb   Ready                      1d        v1.6.1+5115d708d7
		ocp-master-dgf8       Ready,SchedulingDisabled   1d        v1.6.1+5115d708d7
		ocp-master-q1v2       Ready,SchedulingDisabled   1d        v1.6.1+5115d708d7
		ocp-master-vq70       Ready,SchedulingDisabled   1d        v1.6.1+5115d708d7
		ocp-node-020m         Ready                      1d        v1.6.1+5115d708d7
		ocp-node-7t5p         Ready                      1d        v1.6.1+5115d708d7
		ocp-node-n0dd         Ready                      1d        v1.6.1+5115d708d7		  
- 并且该节点不包含任何pod

		$ oc describe node ocp-infra-node-b7pl
		Name:			ocp-infra-node-b7pl
		Role:
		Labels:			beta.kubernetes.io/arch=amd64
					beta.kubernetes.io/instance-type=n1-standard-2
					beta.kubernetes.io/os=linux
					failure-domain.beta.kubernetes.io/region=europe-west3
					failure-domain.beta.kubernetes.io/zone=europe-west3-c
					kubernetes.io/hostname=ocp-infra-node-b7pl
					role=infra
		Annotations:		volumes.kubernetes.io/controller-managed-attach-detach=true
		Taints:			<none>
		CreationTimestamp:	Wed, 22 Nov 2017 09:36:36 -0500
		Phase:
		Conditions:
		  ...
		Addresses:		10.156.0.11,ocp-infra-node-b7pl
		Capacity:
		 cpu:		2
		 memory:	7494480Ki
		 pods:		20
		Allocatable:
		 cpu:		2
		 memory:	7392080Ki
		 pods:		20
		System Info:
		 Machine ID:			bc95ccf67d047f2ae42c67862c202e44
		 System UUID:			9762CC3D-E23C-AB13-B8C5-FA16F0BCCE4C
		 Boot ID:			ca8bf088-905d-4ec0-beec-8f89f4527ce4
		 Kernel Version:		3.10.0-693.5.2.el7.x86_64
		 OS Image:			Employee SKU
		 Operating System:		linux
		 Architecture:			amd64
		 Container Runtime Version:	docker://1.12.6
		 Kubelet Version:		v1.6.1+5115d708d7
		 Kube-Proxy Version:		v1.6.1+5115d708d7
		ExternalID:			437740049672994824
		Non-terminated Pods:		(0 in total)
		  Namespace			Name		CPU Requests	CPU Limits	Memory Requests	Memory Limits
		  ---------			----		------------	----------	---------------	-------------
		Allocated resources:
		  (Total limits may be over 100 percent, i.e., overcommitted.)
		  CPU Requests	CPU Limits	Memory Requests	Memory Limits
		  ------------	----------	---------------	-------------
		  0 (0%)	0 (0%)		0 (0%)		0 (0%)
		Events:		<none>
- 从配置文件中的 `backend` 部分中删除基础结构实例 `/etc/haproxy/haproxy.cfg`

		backend router80
		    balance source
		    mode tcp
		    server infra-1.example.com 192.168.55.12:80 check
		    server infra-2.example.com 192.168.55.13:80 check
		
		backend router443
		    balance source
		    mode tcp
		    server infra-1.example.com 192.168.55.12:443 check
		    server infra-2.example.com 192.168.55.13:443 check
- 然后，重新启动该haproxy服务

		$ sudo systemctl restart haproxy
- 使用命令驱逐所有pod后，从群集中删除节点,有关疏散和排空pod或节点的更多信息，请参阅[节点维护部分](https://docs.okd.io/3.10/day_two_guide/host_level_tasks.adic#day-two-guide-node-maintenance)

		$ oc delete node ocp-infra-node-b7pl
		node "ocp-infra-node-b7pl" deleted

		$ oc get nodes
		NAME                  STATUS                     AGE       VERSION
		ocp-infra-node-p5zj   Ready                      1d        v1.6.1+5115d708d7
		ocp-infra-node-rghb   Ready                      1d        v1.6.1+5115d708d7
		ocp-master-dgf8       Ready,SchedulingDisabled   1d        v1.6.1+5115d708d7
		ocp-master-q1v2       Ready,SchedulingDisabled   1d        v1.6.1+5115d708d7
		ocp-master-vq70       Ready,SchedulingDisabled   1d        v1.6.1+5115d708d7
		ocp-node-020m         Ready                      1d        v1.6.1+5115d708d7
		ocp-node-7t5p         Ready                      1d        v1.6.1+5115d708d7
		ocp-node-n0dd         Ready                      1d        v1.6.1+5115d708d7

### 还原节点主机备份
在创建重要节点主机文件的备份后，如果它们被损坏或意外删除，可以通过复制文件来恢复文件，确保它包含正确的内容并重新启动受影响的服务。

步骤

1. 恢复 `/etc/origin/node/node-config.yaml` 文件

		# MYBACKUPDIR=/backup/$(hostname)/$(date +%Y%m%d)
		# cp /etc/origin/node/node-config.yaml /etc/origin/node/node-config.yaml.old
		# cp /backup/$(hostname)/$(date +%Y%m%d)/etc/origin/node/node-config.yaml /etc/origin/node/node-config.yaml
		# systemctl restart atomic-openshift-node
	重新启动服务可能会导致停机。有关如何简化过程的提示，请参阅[节点维护](https://docs.okd.io/3.10/day_two_guide/host_level_tasks.html#day-two-guide-node-maintenance)
	
	执行受影响实例的完全重新引导以恢复 iptables 配置
- 如果由于缺少软件包而无法重新启动OKD，请重新安装软件包。
	1. 获取当前已安装的软件包列表

			$ rpm -qa | sort> /tmp/current_packages.txt
	2. 查看包列表之间的差异

			$ diff /tmp/current_packages.txt ${MYBACKUPDIR}/packages.txt

			> ansible-2.4.0.0-5.el7.noarch
	- 重新安装缺少的包
	
			# yum reinstall -y <packages>
- 通过将证书复制到 `/etc/pki/ca-trust/source/anchors/` 目录来还原系统证书并执行 `update-ca-trust`,始终确保在复制文件时恢复正确的用户ID和组ID以及SELinux上下文

### 节点维护和后续步骤
有关各种节点管理选项，请参[阅管理节点](https://docs.okd.io/3.10/admin_guide/manage_nodes.html#admin-guide-manage-nodes)或[管理 pod](https://docs.okd.io/3.10/admin_guide/managing_pods.html#admin-guide-manage-pods)主题。这些包括：

- [将节点标记为不可调度或可调度](https://docs.okd.io/3.10/admin_guide/manage_nodes.html#marking-nodes-as-unschedulable-or-schedulable)
- [在节点上疏散Pod](https://docs.okd.io/3.10/admin_guide/manage_nodes.html#evacuating-pods-on-nodes)
- [设置Pod中断预算](https://docs.okd.io/3.10/admin_guide/managing_pods.html#managing-pods-poddisruptionbudget)

节点可以保留其部分资源以供特定组件使用。这些包括 kubelet，kube-proxy，Docker或其他系统组件，如sshd和NetworkManager。有关详细信息，请参见[群集管理器指南中的分配节点资源](https://docs.okd.io/3.10/admin_guide/allocating_node_resources.html#admin-guide-allocating-node-resources)部分。

## ETCD 任务
### 修复 etcd
etcd 配置文件的还原过程将替换相应的文件，然后重新启动该服务。
如果 etcd 主机已损坏且 `/etc/etcd/etcd.conf` 文件丢失，请使用以下命令恢复它

	$ ssh master-0
	# cp /backup/yesterday/master-0-files/etcd.conf /etc/etcd/etcd.conf
	# restorecon -Rv /etc/etcd/etcd.conf
	# systemctl restart etcd.service
在此示例中，备份文件存储在 `/backup/yesterday/master-0-files/etcd.conf` 可用作外部 NFS共享，S3 或其他存储解决方案
### 恢复ETCD V2和V3数据
以下过程恢复数据文件并将 etcd 集群作为单个节点启动，然后在需要 etcd 集群时添加其余节点。

步骤

1. 停止所有etcd服务

		# systemctl stop etcd.service
- 要确保恢复正确的备份，请删除 etcd 目录
	- 要在删除目录之前备份当前的 etcd 数据，请运行以下命令

			# mv /var/lib/etcd /var/lib/etcd.old
			# mkdir /var/lib/etcd
			# chown -R etcd.etcd /var/lib/etcd/
			# restorecon -Rv /var/lib/etcd/
	- 或删除目录和 etcd，数据，请运行以下命令

			# rm -Rf /var/lib/etcd/*
		在一体化集群中，etcd 数据目录位于 `/var/lib/origin/openshift.local.etcd` 目录中
- 将备份数据文件还原到每个 etcd 节点。在所有 etcd 主机上执行此步骤，包括与 etcd 部署在 master 节点的主机

		# cp -R /backup/etcd-xxx/* /var/lib/etcd/
		# mv /var/lib/etcd/db /var/lib/etcd/member/snap/db
		# chcon -R --reference /backup/etcd-xxx/* /var/lib/etcd/
		# chown -R etcd:etcd /var/lib/etcd/R
- 在每台主机上运行 etcd 服务，强制新的集群。

	这将为 etcd 服务创建一个自定义文件，该文件将覆盖添加 `--force-new-cluster` 选项的执行命令
	
		# mkdir -p /etc/systemd/system/etcd.service.d/
		# echo "[Service]" > /etc/systemd/system/etcd.service.d/temp.conf
		# echo "ExecStart=" >> /etc/systemd/system/etcd.service.d/temp.conf
		# sed -n '/ExecStart/s/"$/ --force-new-cluster"/p' \
		    /usr/lib/systemd/system/etcd.service \
		    >> /etc/systemd/system/etcd.service.d/temp.conf
		
		# systemctl daemon-reload
		# master-restart etcd
- 检查错误消息

		$ journalctl -fu etcd.service
- 检查健康状况

		# etcdctl2 cluster-health
		member 5ee217d17301 is healthy: got healthy result from https://192.168.55.8:2379
		cluster is healthy
- 以集群模式重新启动 etcd 服务

		# rm -f /etc/systemd/system/etcd.service.d/temp.conf
		# systemctl daemon-reload
		# master-restart etcd
- 检查健康状况和成员列表

		# etcdctl2 cluster-health
		member 5ee217d17301 is healthy: got healthy result from https://192.168.55.8:2379
		cluster is healthy
		
		# etcdctl2 member list
		5ee217d17301: name=master-0.example.com peerURLs=http://localhost:2380 clientURLs=https://192.168.55.8:2379 isLeader=true
- 第一个实例运行后，可以还原其余的 etcd 服务器

#### 修复 peerURLS 参数
恢复数据并创建新群集后，peerURLs 参数显示 localhost 而不是 etcd 正在侦听对等通信的IP

	# etcdctl2 member list
	5ee217d17301: name=master-0.example.com peerURLs=http://*localhost*:2380 clientURLs=https://192.168.55.8:2379 isLeader=true
步骤

1. 使用 `etcdctl member list` 以下方式获取会员ID

		etcdctl member list
- 获取etcd侦听对等通信的IP

		$ ss -l4n | grep 2380
- 使用该IP更新成员信息

		# etcdctl2 member update 5ee217d17301 https://192.168.55.8:2380
		Updated member with ID 5ee217d17301 in cluster
- 要验证，请检查IP是否在成员列表中

		$ etcdctl2 member list
		5ee217d17301: name=master-0.example.com peerURLs=https://*192.168.55.8*:2380 clientURLs=https://192.168.55.8:2379 isLeader=true
		
### 恢复 ETCD V3 快照
v3 数据的还原过程类似于v2数据的还原过程。

可以在恢复时选择性地验证快照完整性。如果使用快照 `etcdctl snapshot save`，则它将具有检查的完整性哈希 `etcdctl snapshot restore`。如果从数据目录复制快照，则没有完整性哈希，它只能通过使用来恢复 `--skip-hash-check`。

	必须在单个etcd主机上执行仅还原v3数据的过程。然后，可以将其余节点添加到群集。
步骤

1. 停止所有etcd服务

		#systemctl stop etcd.service
- 清除所有旧数据，因为 `etcdctl` 要在将要执行还原过程的节点中重新创建它

		# rm -Rf /var/lib/etcd
- 运行 `snapshot restore` 命令，替换 `/etc/etcd/etcd.conf` 文件中的值

		# etcdctl3 snapshot restore /backup/etcd-xxxxxx/backup.db \
		  --data-dir /var/lib/etcd \
		  --name master-0.example.com \
		  --initial-cluster "master-0.example.com=https://192.168.55.8:2380" \
		  --initial-cluster-token "etcd-cluster-1" \
		  --initial-advertise-peer-urls https://192.168.55.8:2380 \
		  --skip-hash-check=true
		
		2017-10-03 08:55:32.440779 I | mvcc: restore compact to 1041269
		2017-10-03 08:55:32.468244 I | etcdserver/membership: added member 40bef1f6c79b3163 [https://192.168.55.8:2380] to cluster 26841ebcf610583c
- 恢复selinux已还原文件的权限和上下文

		# chown -R etcd.etcd /var/lib/etcd/
		# restorecon -Rv /var/lib/etcd
- 启动etcd服

		# systemctl start etcd
- 检查是否有任何错误消息

		# master-logs etcd etcd

### 替换 etcd 主机
要替换 etcd 主机，请扩展 etcd 群集，然后删除主机。如果在更换过程中丢失了 etcd 主机，此过程可确保仲裁。

在替换操作期间，etcd 集群必须保持仲裁。这意味着至少有一台主机必须始终处于运行状态。

如果在 etcd 集群维持仲裁时发生主机替换操作，则集群操作通常不会受到影响。如果必须复制大量的 etcd 数据，某些操作可能会变慢。

在开始涉及 etcd 集群的任何过程之前，必须备份 etcd 数据和配置文件，以便在过程失败时还原集群
### 缩放 etcd
可以通过向 etcd 主机添加更多资源或通过添加更多 etcd 来水平扩展 etcd 集群。

由于 etcd 使用投票系统 ，集群必须始终包含奇数个成员。

具有奇数个 etcd 主机的群集可以考虑容错。拥有奇数个 etcd 主机不会更改仲裁所需的数量，但会增加对故障的容忍度。例如，对于由三个成员组成的群集，仲裁为两个，其容错为一。这确保了如果两个成员健康，群集将继续运行。

建议使用三个 etcd 在生产中集群。

新主机需要一个全新的操作系统。etcd 存储应该位于SSD磁盘上以实现最高性能并且安装在专用磁盘上 `/var/lib/etcd`
#### 先决条件
1. 在添加新的 etcd 主机之前，请执行 etcd 配置和数据的备份以防止数据丢失。
2. 检查当前的 etcd 集群状态，以避免将新主机添加到不健康的集群。
	-  v2 etcd api，请运行以下命令

			# etcdctl --cert-file=/etc/etcd/peer.crt \
			          --key-file=/etc/etcd/peer.key \
			          --ca-file=/etc/etcd/ca.crt \
			          --peers="https://*master-0.example.com*:2379,\
			          https://*master-1.example.com*:2379,\
			          https://*master-2.example.com*:2379"\
			          cluster-health
			member 5ee217d19001 is healthy: got healthy result from https://192.168.55.12:2379
			member 2a529ba1840722c0 is healthy: got healthy result from https://192.168.55.8:2379
			member ed4f0efd277d7599 is healthy: got healthy result from https://192.168.55.13:2379
			cluster is healthy
	- v3 etcd api，请运行以下命令	
	
			# ETCDCTL_API=3 etcdctl --cert="/etc/etcd/peer.crt" \
			          --key=/etc/etcd/peer.key \
			          --cacert="/etc/etcd/ca.crt" \
			          --endpoints="https://*master-0.example.com*:2379,\
			            https://*master-1.example.com*:2379,\
			            https://*master-2.example.com*:2379"
			            endpoint health
			https://master-0.example.com:2379 is healthy: successfully committed proposal: took = 5.011358ms
			https://master-1.example.com:2379 is healthy: successfully committed proposal: took = 1.305173ms
			https://master-2.example.com:2379 is healthy: successfully committed proposal: took = 1.388772ms
- 在运行 `scaleupplaybook` 之前，请确保新主机已注册到正确的Red Hat软件通道

		# subscription-manager register \
		    --username=*<username>* --password=*<password>*
		# subscription-manager attach --pool=*<poolid>*
		# subscription-manager repos --disable="*"
		# subscription-manager repos \
		    --enable=rhel-7-server-rpms \
		    --enable=rhel-7-server-extras-rpms

	etcd 托管在 `rhel-7-server-extras-rpms` 软件频道中
- 升级 etcd 和 iptables

		# yum update etcd iptables-services
- 备份 etcd 主机的 `/etc/etcd` 配置。
- 如果新的 etcd 成员也将是OKD节点， [请将所需数量的主机添加到群集](https://docs.okd.io/3.10/install_config/adding_hosts_to_existing_cluster.html#install-config-adding-hosts-to-cluster)。
- 此过程的其余部分假定添加了一个主机，但如果添加多个主机，请在每个主机上执行以上所有步骤。

### 使用 ANSIBLE 添加新的 ETCD 主机
步骤

1. 在 Ansible 清单文件中，创建一个名为的新组 `[new_etcd]` 并添加新主机。然后，将该 `new_etcd` 组添加为该组的子级 `[OSEv3]`

		[OSEv3:children]
		masters
		nodes
		etcd
		new_etcd 
		
		... [OUTPUT ABBREVIATED] ...
		
		[etcd]
		master-0.example.com
		master-1.example.com
		master-2.example.com
		
		[new_etcd] 
		etcd0.example.com 
- 从安装了 OKD 的主机并托管 Ansible 清单文件，运行 etcd `scaleup` playbook

		$ ansible-playbook  /usr/share/ansible/openshift-ansible/playbooks/byo/openshift-etcd/scaleup.yml
- 在 Playbook 运行后，通过将新的 etcd 主机从 `[new_etcd]` 组移动到组来修改库存文件以反映当前状态 `[etcd]`

		[OSEv3:children]
		masters
		nodes
		etcd
		new_etcd
		
		... [OUTPUT ABBREVIATED] ...
		
		[etcd]
		master-0.example.com
		master-1.example.com
		master-2.example.com
		etcd0.example.com
- 如果使用 Flannel，请修改 `flanneld` 位于的每个OKD主机上的服务配置 `/etc/sysconfig/flanneld`，以包含新的 etcd 主机

		FLANNEL_ETCD_ENDPOINTS=https://master-0.example.com:2379,https://master-1.example.com:2379,https://master-2.example.com:2379,https://etcd0.example.com:2379
- 重启 `flanneld` 服务

		# systemctl restart flanneld.service

### 手动添加新的ETCD主机
步骤

- 修改当前的 etcd 集群

	要创建 etcd 证书，请运行该 openssl 命令，将值替换为环境中的值。
	
	1. 创建一些环境变量
	
			export NEW_ETCD_HOSTNAME="*etcd0.example.com*"
			export NEW_ETCD_IP="192.168.55.21"
			
			export CN=$NEW_ETCD_HOSTNAME
			export SAN="IP:${NEW_ETCD_IP}"
			export PREFIX="/etc/etcd/generated_certs/etcd-$CN/"
			export OPENSSLCFG="/etc/etcd/ca/openssl.cnf"
		使用的自定义 `openssl` 扩展 `etcd_v3_ca_*`,包括 `$SAN` 环境变量 `subjectAltName`。请参阅有关 `/etc/etcd/ca/openssl.cnf` 更多信息
	- 创建证书目录

			# mkdir -p ${PREFIX}
	- 创建服务器证书请求并对其进行签名:(server.csr和server.crt)

			# openssl req -new -config ${OPENSSLCFG} \
			    -keyout ${PREFIX}server.key  \
			    -out ${PREFIX}server.csr \
			    -reqexts etcd_v3_req -batch -nodes \
			    -subj /CN=$CN
			
			# openssl ca -name etcd_ca -config ${OPENSSLCFG} \
			    -out ${PREFIX}server.crt \
			    -in ${PREFIX}server.csr \
			    -extensions etcd_v3_ca_server -batch
	- 创建对等证书请求并对其进行签名:(peer.csr和peer.crt)

			# openssl req -new -config ${OPENSSLCFG} \
			    -keyout ${PREFIX}peer.key \
			    -out ${PREFIX}peer.csr \
			    -reqexts etcd_v3_req -batch -nodes \
			    -subj /CN=$CN
			
			# openssl ca -name etcd_ca -config ${OPENSSLCFG} \
			  -out ${PREFIX}peer.crt \
			  -in ${PREFIX}peer.csr \
			  -extensions etcd_v3_ca_peer -batch
	- 从运行中节点复制 ca.crt 到新建 etcd 的配置和文件作为示例以便稍后修改

			# cp /etc/etcd/etcd.conf ${PREFIX}
			# cp /etc/etcd/ca.crt ${PREFIX}
	- 仍然在运行中的 etcd 主机上，将新主机添加到群集，必须首先在第一个成员的值中调整默认的 `localhost` peer `peerURLs`：
		-  使用以下 `member list` 命令获取第一个成员的成员标识,确保在 `--peers` 参数值中仅指定活动的 etcd 成员的 URL

				# etcdctl --cert-file=/etc/etcd/peer.crt \
				    --key-file=/etc/etcd/peer.key \
				    --ca-file=/etc/etcd/ca.crt \
				    --peers="https://172.18.1.18:2379,https://172.18.9.202:2379,https://172.18.0.75:2379" \ 
				    member list
		- 获取 etcd 侦听集群对等体的IP地址

				$ ss -l4n | grep 2380
		- 通过传递从前面步骤获得的成员ID和IP地址 `peerURLs` 来使用 `etcdctl member update` 命令更新值

				# etcdctl --cert-file=/etc/etcd/peer.crt \
				    --key-file=/etc/etcd/peer.key \
				    --ca-file=/etc/etcd/ca.crt \
				    --peers="https://172.18.1.18:2379,https://172.18.9.202:2379,https://172.18.0.75:2379" \
				    member update 511b7fb6cc0001 https://172.18.1.18:2380
		- 重新运行该 `member list` 命令,并确保对等 URL 不再包含 `localhost`
	- 将新主机添加到etcd集群。请注意，尚未配置新主机，因此状态将保持不变， `unstarted` 直到配置新主机。

		必须一次一个添加每个成员。将每个其他成员添加到群集时，必须调整 `peerURLs` 对等项列表。`peerURLs` 每个成员增加一个列表项。该 `etcdctl member add` 命令将在添加每个成员时输出必须在 `etcd.conf` 文件中设置的值 ，如以下说明中所述
		
			# etcdctl -C https://${CURRENT_ETCD_HOST}:2379 \
			  --ca-file=/etc/etcd/ca.crt     \
			  --cert-file=/etc/etcd/peer.crt     \
			  --key-file=/etc/etcd/peer.key member add ${NEW_ETCD_HOSTNAME} https://${NEW_ETCD_IP}:2380 #在这一行中，10.3.9.222是etcd成员的标签。可以指定主机名，IP地址或简单名称
			
			Added member named 10.3.9.222 with ID 4e1db163a21d7651 to cluster
			
			ETCD_NAME="<NEW_ETCD_HOSTNAME>"
			ETCD_INITIAL_CLUSTER="<NEW_ETCD_HOSTNAME>=https://<NEW_HOST_IP>:2380,<CLUSTERMEMBER1_NAME>=https:/<CLUSTERMEMBER2_IP>:2380,<CLUSTERMEMBER2_NAME>=https:/<CLUSTERMEMBER2_IP>:2380,<CLUSTERMEMBER3_NAME>=https:/<CLUSTERMEMBER3_IP>:2380"
			ETCD_INITIAL_CLUSTER_STATE="existing"
	- 更新示例 `${PREFIX}/etcd.conf` 文件
		- 将以下值替换为上一步中生成的值
			- ETCD_NAME
			- ETCD_INITIAL_CLUSTER
			- ETCD_INITIAL_CLUSTER_STATE
		- 使用上一步输出中的新主机IP修改以下变量。可以将其 `${NEW_ETCD_IP}` 用作值
		
				ETCD_LISTEN_PEER_URLS
				ETCD_LISTEN_CLIENT_URLS
				ETCD_INITIAL_ADVERTISE_PEER_URLS
				ETCD_ADVERTISE_CLIENT_URLS
		- 如果以前使用成员系统作为 etcd 节点，则必须覆盖 `/etc/etcd/etcd.conf` 文件中的当前值
		- 需检查文件是否存在语法错误或缺少IP地址

				# vi ${PREFIX}/etcd.conf
	- 在承载安装节点上，更新 ` /etc/ansible/hosts` 清单文件中的 `[etcd]` 主机组。删除旧的 etcd 主机并添加新的主机。
	- 创建一个 `tgz` 包含证书，示例配置文件，`ca` 将其复制到新主机

			# tar -czvf /etc/etcd/generated_certs/${CN}.tgz -C ${PREFIX} .
			# scp /etc/etcd/generated_certs/${CN}.tgz ${CN}:/tmp/

### 修改新的 etcd 主机
1. 安装 `iptables-services` 以提供iptables实用程序以打开 etcd 所需的端口

		# yum install -y iptables-services
- 创建 `OS_FIREWALL_ALLOW` 防火墙规则以允许 etcd 进行通信
	- 端口 2379/tcp 为客户端
	- 端口 2380/tcp 用于对等通信

			# systemctl enable iptables.service --now
			# iptables -N OS_FIREWALL_ALLOW
			# iptables -t filter -I INPUT -j OS_FIREWALL_ALLOW
			# iptables -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 2379 -j ACCEPT
			# iptables -A OS_FIREWALL_ALLOW -p tcp -m state --state NEW -m tcp --dport 2380 -j ACCEPT
			# iptables-save | tee /etc/sysconfig/iptables
		在此示例中，将 `OS_FIREWALL_ALLOW` 创建一个新链，这是OKD安装程序用于防火墙规则的标准命名。
		
		如果环境托管在 IaaS 环境中，请修改实例的安全组以允许传入流量到这些端口。
- 安装etcd,确保 etcd-2.3.7-4.el7.x86_64 安装版本或更高版本

		# yum install -y etcd
- 确保etcd服务未运行

		# systemctl disable etcd --now
- 删除所有etcd配置和数据

		# rm -Rf /etc/etcd/*
		# rm -Rf /var/lib/etcd/*
- 提取证书和配置文件

		# tar xzvf /tmp/etcd0.example.com.tgz -C /etc/etcd/
- 修改文件所有权权限

		# chown -R etcd/etcd /etc/etcd/*
		# chown -R etcd/etcd /var/lib/etcd/
- 在新主机上启动etcd

		# systemctl enable etcd --now
- 验证主机是群集的一部分以及当前群集运行状况
	- v2 etcd api，请运行以下命令

			# etcdctl --cert-file=/etc/etcd/peer.crt \
			          --key-file=/etc/etcd/peer.key \
			          --ca-file=/etc/etcd/ca.crt \
			          --peers="https://*master-0.example.com*:2379,\
			          https://*master-1.example.com*:2379,\
			          https://*master-2.example.com*:2379,\
			          https://*etcd0.example.com*:2379"\
			          cluster-health
			member 5ee217d19001 is healthy: got healthy result from https://192.168.55.12:2379
			member 2a529ba1840722c0 is healthy: got healthy result from https://192.168.55.8:2379
			member 8b8904727bf526a5 is healthy: got healthy result from https://192.168.55.21:2379
			member ed4f0efd277d7599 is healthy: got healthy result from https://192.168.55.13:2379
			cluster is healthy
	- v3 etcd api，请运行以下命令
	
			# ETCDCTL_API=3 etcdctl --cert="/etc/etcd/peer.crt" \
			          --key=/etc/etcd/peer.key \
			          --cacert="/etc/etcd/ca.crt" \
			          --endpoints="https://*master-0.example.com*:2379,\
			            https://*master-1.example.com*:2379,\
			            https://*master-2.example.com*:2379,\
			            https://*etcd0.example.com*:2379"\
			            endpoint health
			https://master-0.example.com:2379 is healthy: successfully committed proposal: took = 5.011358ms
			https://master-1.example.com:2379 is healthy: successfully committed proposal: took = 1.305173ms
			https://master-2.example.com:2379 is healthy: successfully committed proposal: took = 1.388772ms
			https://etcd0.example.com:2379 is healthy: successfully committed proposal: took = 1.498829ms

### 修改每个 master 节点
1. 修改每个 master 节点上 `/etc/origin/master/master-config.yaml` 文件中的`etcClientInfo` 部分。将新的 etcd 主机添加到用于存储数据的 etcd 服务器列表中，并删除任何失败的 etcd 主机

		etcdClientInfo:
		  ca: master.etcd-ca.crt
		  certFile: master.etcd-client.crt
		  keyFile: master.etcd-client.key
		  urls:
		    - https://master-0.example.com:2379
		    - https://master-1.example.com:2379
		    - https://master-2.example.com:2379
		    - https://etcd0.example.com:2379
- 重新启动主 API 服务：
	- 在每个 master 节点执行

			# systemctl restart atomic-openshift-master-api
	- 单个 master 节点群集上

			# systemctl restart atomic-openshift-master
- 如果使用 Flannel，请修改 `flanneld` 位于 `/etc/sysconfig/flanneld` 每个OKD主机上的服务配置 以包含新的 etcd 主机

		FLANNEL_ETCD_ENDPOINTS = HTTPS：//master-0.example.com：2379，HTTPS：//master-1.example.com：2379，HTTPS：//master-2.example.com：2379，https：//开头etcd0。 example.com:2379
- 重启 `flanneld` 服务

		# systemctl restart flanneld.service

### 删除etcd主机
如果 etcd master 无法恢复，请将其从群集中删除。

	要在所有 master 节点上执行的步骤
步骤

1. 从 etcd 集群中删除彼此的 etcd 主机。对每个 etcd 节点运行以下命令

		# etcdctl -C https://<surviving host IP address>:2379 \
		  --ca-file=/etc/etcd/ca.crt     \
		  --cert-file=/etc/etcd/peer.crt     \
		  --key-file=/etc/etcd/peer.key member remove <failed member ID>
- 在每个主服务器上重新启动主 API 服务

		# master-restart api restart-master controller

在当前的 etcd 集群中执行的步骤

步骤

1. 从群集中删除失败的主机，	该 remove 命令需要etcd ID，而不是主机名

		# etcdctl2 cluster-health
		member 5ee217d19001 is healthy: got healthy result from https://192.168.55.12:2379
		member 2a529ba1840722c0 is healthy: got healthy result from https://192.168.55.8:2379
		failed to check the health of member 8372784203e11288 on https://192.168.55.21:2379: Get https://192.168.55.21:2379/health: dial tcp 192.168.55.21:2379: getsockopt: connection refused
		member 8372784203e11288 is unreachable: [https://192.168.55.21:2379] are all unreachable
		member ed4f0efd277d7599 is healthy: got healthy result from https://192.168.55.13:2379
		cluster is healthy
		
		# etcdctl2 member remove 8372784203e11288 
		Removed member 8372784203e11288 from cluster
		
		# etcdctl2 cluster-health
		member 5ee217d19001 is healthy: got healthy result from https://192.168.55.12:2379
		member 2a529ba1840722c0 is healthy: got healthy result from https://192.168.55.8:2379
		member ed4f0efd277d7599 is healthy: got healthy result from https://192.168.55.13:2379
		cluster is healthy
- 要确保 etcd 配置在重新启动 etcd 服务时不使用失败的主机，请修改 `/etc/etcd/etcd.conf` 所有剩余的 etcd 主机上的文件，并删除 `ETCD_INITIAL_CLUSTER` 变量值中的失败主机

		# vi /etc/etcd/etcd.conf
	例
	
		ETCD_INITIAL_CLUSTER=master-0.example.com=https://192.168.55.8:2380,master-1.example.com=https://192.168.55.12:2380,master-2.example.com=https://192.168.55.13:2380
	变为
	
		ETCD_INITIAL_CLUSTER=master-0.example.com=https://192.168.55.8:2380,master-1.example.com=https://192.168.55.12:2380
	不需要重新启动etcd服务，因为使用 `etcdctl` 删除失败的主机
- 修改 Ansible 清单文件以反映群集的当前状态，并避免在重新运行 Playbook 时出现问题

		[OSEv3:children]
		masters
		nodes
		etcd
		
		... [OUTPUT ABBREVIATED] ...
		
		[etcd]
		master-0.example.com
		master-1.example.com
- 如果使用 `Flannel` ，请修改 flanneld 位于 `/etc/sysconfig/flanneld` 每个主机上的服务配置并删除etcd主机

		FLANNEL_ETCD_ENDPOINTS=https://master-0.example.com:2379,https://master-1.example.com:2379,https://master-2.example.com:2379
- 重启服务

		# systemctl restart flanneld.service